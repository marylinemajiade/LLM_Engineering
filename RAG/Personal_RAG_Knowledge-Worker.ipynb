{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfe37963-1af6-44fc-a841-8e462443f5e6",
   "metadata": {},
   "source": [
    "## Expert Knowledge Worker\n",
    "\n",
    "A question answering agent that is an expert knowledge worker to be used by employees of Insurellm, an Insurance Tech company. The agent needs to be accurate and the solution should be low cost. This project will use <b>RAG (Retrieval Augmented Generation)</b> to ensure our question/answering assistant has high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba2779af-84ef-4227-9e9e-6eaf0df87e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import glob\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "802137aa-8a74-45e0-a487-d1974927d7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for langchain\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader, PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objects as go\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58c85082-e417-4708-9efe-81a5d55d1424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# price is a factor for our company, so we're going to use a low cost model\n",
    "\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "db_name = \"vector_db\"\n",
    "db_name2 = \"vector_db_nlp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee78efcb-60fe-449e-a944-40bab26261af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables in a file called .env\n",
    "\n",
    "load_dotenv(override=True)\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "730711a9-6ffe-4eee-8f48-d6cfb7314905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in documents using LangChain's loaders\n",
    "# Take everything in all the sub-folders of our knowledgebase\n",
    "\n",
    "documents = []\n",
    "\n",
    "loader = DirectoryLoader(\"NLP-lecture\", glob=\"*.pdf\", loader_cls=PyPDFLoader, silent_errors=True)\n",
    "root_docs = loader.load()\n",
    "for doc in root_docs:\n",
    "    doc.metadata[\"doc_type\"] = \"root\"   # or None\n",
    "    documents.append(doc)\n",
    "\n",
    "\n",
    "# Load PDFs inside sub-directories\n",
    "subdirs = [p for p in glob.glob(\"NLP-lecture/*\") if os.path.isdir(p)]\n",
    "for folder in subdirs:\n",
    "    doc_type = os.path.basename(folder)\n",
    "    loader = DirectoryLoader(\n",
    "        folder,\n",
    "        glob=\"**/*.pdf\",\n",
    "        loader_cls=PyPDFLoader,\n",
    "        silent_errors=True\n",
    "    )\n",
    "    folder_docs = loader.load()\n",
    "    for doc in folder_docs:\n",
    "        doc.metadata[\"doc_type\"] = doc_type\n",
    "        documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7310c9c8-03c1-4efc-a104-5e89aec6db1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the documents into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd06e02f-6d9b-44cc-a43d-e1faa8acc7bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "844"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c54b4b6-06da-463d-bee7-4dd456c2b887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document types found: lectures2025, root\n"
     ]
    }
   ],
   "source": [
    "doc_types = set(chunk.metadata['doc_type'] for chunk in chunks)\n",
    "print(f\"Document types found: {', '.join(doc_types)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f7d2a6-ccfa-425b-a1c3-5e55b23bd013",
   "metadata": {},
   "source": [
    "## Chroma\n",
    "We will be mapping each chunk of text into a Vector that represents the meaning of the text (embedding).\n",
    "OpenAI offers a model to do this (Auto-Encoding LLM which generates an output given a complete input), which we will use by calling their API with some LangChain code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78998399-ac17-4e28-b15f-0b5f51e6ee23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the chunks of data into a Vector Store that associates a Vector Embedding with each chunk\n",
    "# Chroma is a popular open source Vector Database based on SQLLite\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Alternative: Free Vector Embeddings from HuggingFace sentence-transformers\n",
    "# Replace embeddings = OpenAIEmbeddings()\n",
    "# with:\n",
    "# from langchain.embeddings import HuggingFaceEmbeddings\n",
    "# embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Delete if already exists\n",
    "if os.path.exists(db_name2):\n",
    "    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fb570fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorstore created with 1688 documents\n"
     ]
    }
   ],
   "source": [
    "# Create vectorstore\n",
    "vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name2)\n",
    "print(f\"Vectorstore created with {vectorstore._collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "057868f6-51a6-4087-94d1-380145821550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vectors have 1,536 dimensions\n"
     ]
    }
   ],
   "source": [
    "# Get one vector and find how many dimensions it has\n",
    "collection = vectorstore._collection\n",
    "sample_embedding = collection.get(limit=1, include=[\"embeddings\"])[\"embeddings\"][0]\n",
    "dimensions = len(sample_embedding)\n",
    "print(f\"The vectors have {dimensions:,} dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d45462-a818-441c-b010-b85b32bcf618",
   "metadata": {},
   "source": [
    "## Visualizing the Vector Store\n",
    "\n",
    "We look at the documents and their embedding vectors to see what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b98adf5e-d464-4bd2-9bdf-bc5b6770263b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prework\n",
    "result = collection.get(include=['embeddings', 'documents', 'metadatas'])\n",
    "vectors = np.array(result['embeddings'])\n",
    "documents = result['documents']\n",
    "doc_types = [metadata['doc_type'] for metadata in result['metadatas']]\n",
    "colors = [['blue', 'red'][['lectures2025', 'root'].index(t)] for t in doc_types]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "427149d5-e5d8-4abd-bb6f-7ef0333cca21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hoverinfo": "text",
         "marker": {
          "color": [
           "red",
           "red",
           "red",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "red",
           "red",
           "red",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue"
          ],
          "opacity": 0.8,
          "size": 5
         },
         "mode": "markers",
         "text": [
          "Type: root<br>Text: NLP \nDirichlet VAE\nEncoder => Document to topics representation \nThe document has multiple words \nDe...",
          "Type: root<br>Text: 1- Regarding the exam: it was of 50 marks and 24 marks was passing at that time\n2- Overall exam was ...",
          "Type: root<br>Text: 10- for RL \n• Actor-Critic around slide 26, a conceptual question about it\n• RLHF: Three steps in ge...",
          "Type: lectures2025<br>Text: Prof. Sophie Fellenz\nWeek 06 – Attention and Transformer based language \nmodels\nNeural Networks for\n...",
          "Type: lectures2025<br>Text: Agenda\n• Motivation\n• Example: RNN bottleneck problem and how to solve with Attention\n• Attention an...",
          "Type: lectures2025<br>Text: Motivation\nDo some text generation using GPT3 on:\nhttps://beta.openai.com/playground\nFeel free to as...",
          "Type: lectures2025<br>Text: Setting\n- Task: Translating text from one language to another (e.g. English to \nFrench)\n- Sequence t...",
          "Type: lectures2025<br>Text: Encoder-Decoder Models\n5\nSource: https://towardsdatascience.com/what-is-an-encoder-decoder-model-86b...",
          "Type: lectures2025<br>Text: Encoder-Decoder Models\n6\nSource: \nhttps://web.stanford.ed\nu/class/archive/cs/cs224\nn/cs224n.1184/lec...",
          "Type: lectures2025<br>Text: RNNs: The bottleneck problem\n7\nSource: \nhttps://web.stanford.ed\nu/class/archive/cs/cs224\nn/cs224n.11...",
          "Type: lectures2025<br>Text: Sentence Encoding\n8\n„You can‘t cram the meaning of a whole %&!$# \nsentence into a single $&!#* vecto...",
          "Type: lectures2025<br>Text: Sentence Encoding\n9\nSource: mlexplained.com/2017/12/29/attention-is-all-you-need-explained...",
          "Type: lectures2025<br>Text: Attention\n• Three kinds of dependencies are important:\n1. Between input and output tokens\n2. Between...",
          "Type: lectures2025<br>Text: Example: Attention Model\n12\nAttention\nscores\ndot product\nSource: \nhttps://web.stanford.ed\nu/class/ar...",
          "Type: lectures2025<br>Text: Example: Attention Model\n13\nAttention\nscores\nTake softmax to turn scores into\ndistribution\nAttention...",
          "Type: lectures2025<br>Text: Example: Attention Model\n14\nUse the attention distribution to take a \nweighted sum of the encoder hi...",
          "Type: lectures2025<br>Text: Example: Attention Model\n15\nAttention\noutput\n෤𝑦1\nthe Concatenate attention output\nwith decoder hidde...",
          "Type: lectures2025<br>Text: Example: Attention Model\n16\nSource: \nhttps://web.stanford.e\ndu/class/archive/cs/cs\n224n/cs224n.1184/...",
          "Type: lectures2025<br>Text: Example: Attention Model\n17\nAttention\noutput\n෤𝑦7\n<End>\nAttention\nscores\nAttention\ndistribution\nSourc...",
          "Type: lectures2025<br>Text: Why attention?\n• Before attention we used RNNs (Recurrent Neural Networks)\n• The input words are fed...",
          "Type: lectures2025<br>Text: Why attention?\n• Attention can analyze words across a sentence no matter the length\n• Calculations w...",
          "Type: lectures2025<br>Text: Is attention all you need?\nhttps://www.isattentionallyouneed.com/\n20...",
          "Type: lectures2025<br>Text: Transformer\nEncoder (left):\n• Input: Sequence of words (e.g. \nEnglish sentence)\n• Output: Vector for...",
          "Type: lectures2025<br>Text: Attention\nProblem: We want to extract relations between words\nExample: “The girl took the ball and t...",
          "Type: lectures2025<br>Text: Scaled Dot-Product Attention\n• Relation between two words 𝑞,𝑘 ∈ ℝdk (query, key) is given by\n• 𝐴𝑡𝑡𝑒𝑛...",
          "Type: lectures2025<br>Text: Scaled Dot-Product Attention\n• For multiple queries 𝑞1,…,𝑛, keys 𝑘1,…,𝑚 and values 𝑣1,…,𝑚 concatenat...",
          "Type: lectures2025<br>Text: Scaled Dot-Product Attention\nExample: \n• Consider input sentence \n𝑥1,…,𝑥𝑛 ∈ ℝ𝑑𝑘 which \nwe use as que...",
          "Type: lectures2025<br>Text: Scaled Dot-Product Attention\n26\n𝑘1 𝑘2 𝑘3 𝑘4 𝑘5 𝑘6\n𝑞1\n𝑞2\n𝑞3\n𝑞4\n𝑞5\n𝑞6\nMy\ndog\nloves\ngood\nbelly\nrubs\nMy ...",
          "Type: lectures2025<br>Text: Scaled Dot-Product Attention\n27\n𝑘1 𝑘2 𝑘3 𝑘4 𝑘5 𝑘6\n𝑞1\n𝑞2\n𝑞3\n𝑞4\n𝑞5\n𝑞6\nAttention Weights 𝑊 = (𝑤𝑖,𝑗)\nOut...",
          "Type: lectures2025<br>Text: Multi-Head Attention\n• Problem: Attention for the same word is highest since then 𝑞 = 𝑘 which\nisn‘t ...",
          "Type: lectures2025<br>Text: Multi-Head Attention\n• Linear transformation on 𝑄,𝐾,𝑉:\nℎ𝑒𝑎𝑑𝑖 = 𝐴𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛(𝑄𝑊𝑖\n𝑄,𝐾𝑊𝑖\n𝐾,𝑉𝑊𝑖\n𝑉)\n• Take w...",
          "Type: lectures2025<br>Text: Encoder\n• Input: Sequence of words \n• Embed and add positional information to obtain vectors\n• Use v...",
          "Type: lectures2025<br>Text: Positional Encoding\n• Problem: Transformers do not take position into account\n• Solution: Add positi...",
          "Type: lectures2025<br>Text: Positional Encoding\n• Example: \n- Input embedding: 𝑥1,…,𝑥𝑛\n- Embedding for one word: 𝑥𝑘 = 𝑥𝑘,1,…,𝑥𝑘,...",
          "Type: lectures2025<br>Text: Positional Encoding\n33Source: https://medium.com/swlh/elegant-intuitions-behind-\npositional-encoding...",
          "Type: lectures2025<br>Text: Decoder\n• Assume we are translating “The dog is running” \ninto French (“Le chien court vite”)\n• Gene...",
          "Type: lectures2025<br>Text: Masked Multi-Head Attention\n• Aim: Learn to generate next word for translation given original senten...",
          "Type: lectures2025<br>Text: Scaled Dot-Product Attention + Mask\n• For multiple queries 𝑞1,…,𝑛, keys 𝑘1,…,𝑚 and values 𝑣1,…,𝑚 con...",
          "Type: lectures2025<br>Text: Masked Multi-Head Attention\n37\n𝑘1 𝑘2 𝑘3 𝑘4 𝑘5 𝑘6\n𝑞1\n𝑞2\n𝑞3\n𝑞4\n𝑞5\n𝑞6\nMy\ndog\nloves\ngood\nbelly\nrubs\nMy d...",
          "Type: lectures2025<br>Text: Decoder\n• Next Multi-Head Attention:\n𝐾,𝑉 from the Encoder Output\n𝑄from the Masked Multi-Head Attenti...",
          "Type: lectures2025<br>Text: Attention versions\n• Reminder: 𝐴𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛 𝑞,𝑘,𝑣 =\n𝑞𝑇𝑘\n𝑑𝑘\n𝑣\n• Basic dot-product attention: 𝑞𝑇𝑘\n• Multi...",
          "Type: lectures2025<br>Text: Attention\n40\nName Alignment Score Function Citation\nContent-based attention 𝑠𝑐𝑜𝑟𝑒 𝑠𝑡,ℎ𝑖 = 𝑐𝑜𝑠𝑖𝑛𝑒[𝑠𝑡,...",
          "Type: lectures2025<br>Text: Attention versions\n• Self-Attention\n• Relating different positions of the same input sequence. Theor...",
          "Type: lectures2025<br>Text: Self Attention\n42\nSource: Cheng et al. 2016...",
          "Type: lectures2025<br>Text: Self Attention vs Cross Attention\n43\nSource: Cheng et al. 2016\nSelf Attention\nkey=query=value\nCross ...",
          "Type: lectures2025<br>Text: Attention Computation Summary\n• Encode each token in the input sentence into vectors\n• When decoding...",
          "Type: lectures2025<br>Text: Attention Computation Summary\n• Comes from retrieval systems: when typing a query to search for a vi...",
          "Type: lectures2025<br>Text: Attention: Image Caption Generation\n46\nSource: Xu et al. (2016) Show, Attend and Tell: Neural Image ...",
          "Type: lectures2025<br>Text: Greedy decoding\nInputs poor\n47\nargmax...",
          "Type: lectures2025<br>Text: Greedy decoding\nGreedy decoding has no way to undo decisions\nles pauvres sont démunis (the poor don’...",
          "Type: lectures2025<br>Text: Beam search\nBeam search: Keep track of the 𝑘 most likely partial translations\n𝑘is the beam size\n49...",
          "Type: lectures2025<br>Text: Beam search\nBeam size: 2\n50\n<Start>\na\nthe\nSource: https://web.stanford.edu/class/archive/cs/cs224n/c...",
          "Type: lectures2025<br>Text: Beam search\nBeam size: 2\n51\n<Start>\na\nthe\npoor\npeople\npoor\nperson\nSource: https://web.stanford.edu/c...",
          "Type: lectures2025<br>Text: Beam search\nBeam size: 2\n52\n<Start>\na\nthe\npoor\npeople\npoor\nperson\nare\ndon‘t\nbut\nperson\nSource: https...",
          "Type: lectures2025<br>Text: Beam search\nBeam size: 2\n53\n<Start>\na\nthe\npoor\npeople\npoor\nperson\nare\ndon‘t\nbut\nperson\nalways\nnot\nha...",
          "Type: lectures2025<br>Text: Beam search\nBeam size: 2\n54\n<Start>\na\nthe\npoor\npeople\npoor\nperson\nare\ndon‘t\nbut\nperson\nalways\nnot\nha...",
          "Type: lectures2025<br>Text: Beam search\nBeam size: 2\n55\nSource: https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1184/lec...",
          "Type: lectures2025<br>Text: Beam search\nBeam size: 2\n56\nSource: https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1184/lec...",
          "Type: lectures2025<br>Text: Word embeddings with Context\n• Problem: Word embeddings vectorize words independently of context\n• E...",
          "Type: lectures2025<br>Text: BERT\n• BERT (Bidirectional Encoder Representations from Transformers) is a \nlanguage representation ...",
          "Type: lectures2025<br>Text: BERT\n59\nSource: https://humboldt-\nwi.github.io/blog/research/information_systems_\n1920/bert_blog_pos...",
          "Type: lectures2025<br>Text: GPT\n• GPT (Generative Pre-Trained Transformer) is a language\nmodel to produce human-like text (Radfo...",
          "Type: lectures2025<br>Text: PaLM\n• PaLM (Pathways Language Model) is a language model and consists of\nstacked Transformer Decode...",
          "Type: lectures2025<br>Text: PaLM\n• The optimizations allowed to train a very large model of 540B parameters\nwith a huge dataset ...",
          "Type: lectures2025<br>Text: PaLM\n63\nSource: Chowdhery, Aakanksha, et al. \"Palm: Scaling language modeling with pathways.\" (2022)...",
          "Type: lectures2025<br>Text: PaLM\n64\nSource: Chowdhery, Aakanksha, et al. \"Palm: Scaling language modeling with pathways.\" (2022)...",
          "Type: lectures2025<br>Text: Language Models\n65\nELMo\nBERT-large\nGPT-2\nMegatron-LM\nT5 Turing-NLG\nGPT-3 Megatron-Turing NLG\nPaLM\n0,...",
          "Type: lectures2025<br>Text: Summary\n• Encoder-decoder models\n• Attention can model relations between all words\n• Transformers ar...",
          "Type: lectures2025<br>Text: References\n• Vaswani, Ashish, et al. \"Attention is all you need.\" (2017). \n• Devlin, Jacob, et al. \"...",
          "Type: lectures2025<br>Text: Acknowledgements\n• Feibai Huang\n68...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nJun.-Prof. Sophie Fellenz\nWeek 01 - Introduction to NLP and Applications\n21 ...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Who are we?\n• What is NLP?\n• What are neural networks/deep learning?\n• Cou...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n3\nGroup Intro\n• Junior Professor for Machine Learning since 2020\n• Work most...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nMaschinelles Lernen @ RPTU\nLeitung: \nVollzeitkräfte:\nTeilzeitkräfte:\nProf. D...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nProbabilistic graphical models (SS)\nNeural Networks for NLP (WS)\nMachine Lea...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Natural language processing is a field at the \nintersection of\n• computer ...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nWhat is NLP?\nspeech text\nPhonetic/Phonological Analysis OCR/Tokenization\nMor...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Applications range from simple to complex:\n• Spell checking, keyword searc...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Search (written and spoken)\n• Online advertisement matching\n• Automated/as...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nA human language is a system specifically constructed to convey the speaker/...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nWhat’s special about human language?\nThe categorical symbols of a language c...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nWhat’s special about human language?\nA human language is a symbolic/categori...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Two possible motivations behind research in AI, including NLP:\n• Technolog...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Helps in better and easier (text) communication between any two agents\n• H...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nLanguage is challenging\nWe asked Google - “Who invented machine translation?...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• There are many things that computers are unable to understand well. For \ni...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nNLP\nHow was it? How is it now?\n17...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Fully supervised learning\n• Traditional non-neural network machine learnin...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Still fully supervised learning\n• People started to use neural network mod...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Pre-trained models used as initial \nmodels\n• Fine-tuned for specific tasks...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• NLP tasks are modeled entirely \nbased on models\n• The model extracts featu...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nPrompt Engineering (2019 - present)\nSource: phontron.com\nSource: Kojima et a...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nNeural Networks / Deep Learning\n23...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Deep learning is a subfield of machine learning\n• Most machine learning me...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nMachine Learning vs. Deep Learning\n25...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nWhat is Deep Learning (DL)?\nIn contrast to standard machine learning,\n• Repr...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nOn the history of “Deep Learning”\n• We will focus on different kinds of neur...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nCourse Overview\n28...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nWhat do we hope to teach?\n1. An understanding of and ability to use the effe...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Lectures will be held once a week in person (no video upload planned)\n• Ad...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Exercises are mandatory (50% of points necessary)\n• Prerequisites: Basic P...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nLiterature\n• Goldberg, Yoav. \"Neural network \nmethods for natural language \n...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nOverview\nWeek 1 (21 Oct 2023) Intro to NLP\nWeek 2 (18 Oct 2023) Text Preproc...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nOverview\nWeek 8 (09 Dec 2023) Fine-tuning, Pre-training, Transfer \nLearning\n...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• How is text data different?\n• Words, Characters, Sentences …\n• What is a w...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Why are embeddings needed?\n• Word embeddings\n• Sentence embeddings\n• Diffe...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Intro to Neural Networks\n• Linear vs nonlinear models\n• Backpropagation\n• ...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Seq-to-seq models\n• Recurrent Neural Networks \n(RNNs) for NLP\n• Convolutio...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Attention mechanism\n• Transformer architecture\n• Transformer-based models\n...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• How to generate text from LLMs\n• Sampling Methods\n• Scoring Functions\n• En...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Pre-training models on a lot of good \nquality data\n• Fine-tuning of models...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Introduction to Reinforcement \nLearning\n• Deep Q-Learning\nWeek 09 - Reinfo...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Policy Gradient Learning\n• Actor Critic\n• PPO\nWeek 10 - Reinforcement Lear...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• RL-based Fine-tuning\n• Prompt Engineering\n• Retrieval Augmented Generation...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• What is Self-Supervised Learning \n(SSL)?\n• SSL in NLP\n• Contrastive Learni...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Brief recap of topic models (LDA)\n• Neural topic models\n• VAE-based topic ...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nLet’s look at some applications\n47...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• The task of predicting the class of a given text.\n• Used in any applicatio...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• The task of predicting the human mood, opinion, and attitude from an \ninpu...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nNamed Entity Recognition (NER)\nTask: Find named entities and \ncategorize the...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nText clustering\n• Task: Similar to text classification. Sorting texts or doc...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nTopic Models\n52...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nText Generation\nText generated using OpenAI’s GPT-3\n53...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nText Generation\n54...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nText Generation\n55...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nText Generation\n56...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nText Generation\n57...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nText Generation\nSource: PALM paper\n58...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nText Generation\nSource: PALM paper\n59...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nText Generation\nSource: PALM paper\n60...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• One of the most widely-used NLP \napplications across the world.\n• You prob...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nText Style Transfer\nI went to the store\nThe tree is dying \nbecause no one \nw...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• From ELIZA, Google Assistant, and Siri to ChatGPT, chatbots have come a \nl...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nThe task, in general is:\n• Accept human text/speech as input. It could be a ...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• The task of predicting answer(s) to a given question with respect to a giv...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nInput passage:\nIn meteorology, precipitation is any product of the condensat...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• The task of generating a (short) summary from a given text.\n• The aim is:\n...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nThe neural network model should be able to:\n• Identify and extract important...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nTasks for AI on Texts...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nTasks for AI on Texts\nIs there an AI to solve all of \nthese tasks for us?...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nTasks for AI on Texts\nIs there an AI to solve all of \nthese tasks for us?\nLa...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nLanguage Models Compress the Internet\nSource:\nAndrej Karphathy\nNumbers for L...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nLanguage Models „Dream“ Internet Documents\nSource\nAndrej Karphathy\nJava Code...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nvideo audio\nCalculator\nPython Interpreter\n…\nSoftwaretools\n„classical compute...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nCapabilities of Language Models\n• Read and generate text\n• Have more knowled...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nLLM OS\nQuelle: Andrej Karphathy...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• NLP has many applications\n• Language models are becoming more and more imp...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n● Stanford Course: \nhttps://web.stanford.edu/class/archive/cs/cs224n/cs224n....",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nJun.-Prof. Sophie Fellenz\n13.01.2025\nWeek 10 – Fine-...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n2\nInvitation to a lecture at the RPTU Kaiserslautern...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n3\nAgenda\n• Advanced Fine-Tuning \n• RAG...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nMotivation Finetuning\n4...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n5\nMotivation\nIdeally:\nComplex Problem Large Model Hu...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n6\nMotivation\nRealistically:\nComplex Problem\nLarge? M...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n7\nFoundation Model\n• Develop general purpose neural ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n8\nGeneral Purpose Encoder\nInput\nAdjustments\nReusable...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nZero-shot vs. Few-shot\n9...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n10\nZero-Shot and Few-Shot Learning\n• GPT-3 almost im...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n11\nZero-Shot and Few-Shot Learning\nZero-Shot\nTransla...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nChallenges of Finetuning\n12...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n13\nCatastrophic forgetting\n• Neural network trained ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n14\nOverfitting...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nTransfer Learning Optimization\n15...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n16\nFreezing vs. Full Finetuning\n• Freezing: Keep cer...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n17\nGradual unfreezing of layers\n• A fine-tuning stra...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n18\nLayer-wise learning rate adjustments\nA fine-tunin...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nRegularization Techniques for Finetuning\n19...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n20\nDropout and DropConnect\nDropout\nDefinition: A reg...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n21\nWeight Regularization\n• Techniques to constrain t...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n22\nMixout\n• Definition: Mixout is a dropout-inspired...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nData Augmentation for Finetuning\n23...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n24\nData Augmentation\n• Enhance Diversity: Apply tran...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n25\nData Augmentation Examples\n• Synonym Replacement:...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nAdvanced Model Customization\n26...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n27\nAdapters for Parameter-Efficient Tuning\n• Adapter...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n28\nPrompt Tuning\n• Like having tunable layer(s) of i...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nOptimization and Training Dynamics\n29...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n30\nLearning Rate Schedules\n• Definition: Learning ra...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n31\nLoss Function Engineering\n• Task-Specific Customi...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n32\nCurriculum Learning\nKey Principles:\n• Gradual Com...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n33\nContrastive Finetuning\nKey Concepts:\n• Contrastiv...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nCross-lingual Fine-tuning\n34...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n35\nBeyond English\nKey Concepts:\n• Multilingual Trans...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nPost-hoc Methods vs. In-training adjustments\n36...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n37\nPost-Hoc Methods\nb) Advantages\n• Model Agnostic: ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n38\nPost-Hoc Methods\n• Bias Filtering: Identify and f...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n39\nIn-training Adjustments\n• Adjust Loss Function\n• ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n40\nIn-training Adjustments\nb) Advantages\n• Proactive...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n41\nSafety vs. Helpfulness\n• A question-answering mod...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nMulti-task learning\n43...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n44\nStandard Multi-task Learning\n• Train representati...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nRetrieval Augmented Generation\n46...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n47\nSource: https://blogs.nvidia.com/blog/what-is-ret...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n48\nSimplified RAG Architecture\nSource: https://blog....",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n49\nWhy RAG\nLimitations of purely generative models:\n...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n50\nKey Components\n1. Retriever:\n• Finds relevant doc...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n51\nRetrieval Techniques\n• Sparse Retrieval\n• TF-IDF,...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n52\nFusion Strategies\n1. Early Fusion:\n• Concatenate ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n53\nKnowledge Sources\n1. Static Knowledge Bases: Wiki...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n54\nSource: https://medium.com/the-hack-weekly-ai-tec...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n55\nSource: Yan et al. 2024 „Corrective Retrieval Aug...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n56\nSource: Yan et al. 2024 „Corrective\nRetrieval Aug...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n57\nSource: Wang et al 2024 „ Speculative RAG: Enhanc...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n58\nSource: Rackauckas 2024 „ RAG-Fusion: a New Take ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n59\nSource: Rackauckas 2024 „ RAG-Fusion: a New Take ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n60\nSource: Asai et al 2024 „ Self-RAG: Learning to R...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n61\nSource: Asai et al 2024 „ Self-RAG: Learning \nto ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n62\nSource: Asai et al. 2024 „ Self-RAG: Learning to ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n63\nSource: https://medium.com/the-hack-weekly-ai-tec...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n64\nChoosing the right RAG technique\n• Complex tasks ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n65\nHandling long retrieval contexts\nRank-Then-Genera...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n66\nHandling long retrieval contexts\nSummarize\n• Summ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n67\nConclusion\n• Different solutions depending on the...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n68\nInvitation to a lecture at the RPTU Kaiserslauter...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n69\n• Next Lecture: Neural Topic Models...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n70\nReferences\n• Bowman, Sam. “Pre-Training and Trans...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n71\nReferences\n• [ADAPTER] Houlsby, N., Giurgiu, A., ...",
          "Type: lectures2025<br>Text: Neural networks for \nNatural Language Processing\nJun.-Prof. Dr. Sophie Fellenz\nWeek  09 –Reinforceme...",
          "Type: lectures2025<br>Text: Books on RL\nNew book by Kevin Murphy: Reinforcement Learning: An \nOverview (Released on December 9th...",
          "Type: lectures2025<br>Text: Recap: Reinforcement Learning Problem\n3...",
          "Type: lectures2025<br>Text: Introduction to RL\n‘Reinforcement learning is learning what to do- how to map \nsituations to actions...",
          "Type: lectures2025<br>Text: Introduction to RL\n‘Reinforcement learning is learning what to do- how to map \nsituations to actions...",
          "Type: lectures2025<br>Text: Timeline for RL in Games\n• 1992: TD-Gammon, temporal difference learning, Backgammon\n• 1997: Deep Bl...",
          "Type: lectures2025<br>Text: Agentic Workflows\nAI agentic workflows are structured processes that involve AI agents that\noperate ...",
          "Type: lectures2025<br>Text: BabyAGI – AutoGPT (28.03.2023)\nhttps://yoheinakajima.com/task-driven-autonomous-agent-utilizing-gpt-...",
          "Type: lectures2025<br>Text: Key Componentes of Agentic Workflows\n• Perception: gather information about the environment\n• Decisi...",
          "Type: lectures2025<br>Text: Example Agentic Workflow\nConversational Agent:\n• Perceive user‘s request (voice or text input)\n• Mak...",
          "Type: lectures2025<br>Text: Benefits of Agentic Workflows\nAutonomy: no need for constant human input\nScalability: manage many ta...",
          "Type: lectures2025<br>Text: Outline\nToday:\n1. Introduction to Reinforcement Learning (RL) \n2. Policy-Based RL\n• Policy Gradient ...",
          "Type: lectures2025<br>Text: Markov Decision Processes\n13...",
          "Type: lectures2025<br>Text: Agent‘s Learning Task\n14...",
          "Type: lectures2025<br>Text: Value Function\n15...",
          "Type: lectures2025<br>Text: 16\n𝛾 = 0.9...",
          "Type: lectures2025<br>Text: What to Learn\n17...",
          "Type: lectures2025<br>Text: Q Function\n18...",
          "Type: lectures2025<br>Text: 19\n𝛾 = 0.9...",
          "Type: lectures2025<br>Text: Training Rule to Learn Q\n20...",
          "Type: lectures2025<br>Text: Q-Learning for Deterministic Worlds\n21...",
          "Type: lectures2025<br>Text: Updating ෠𝑄\n22...",
          "Type: lectures2025<br>Text: 23...",
          "Type: lectures2025<br>Text: Nondeterministic Case\n24...",
          "Type: lectures2025<br>Text: Nondeterministic Case\n25...",
          "Type: lectures2025<br>Text: Temporal Difference Learning\n26\n+… ሿ...",
          "Type: lectures2025<br>Text: Temporal Difference Learning\n27...",
          "Type: lectures2025<br>Text: Deep Reinforcement Learning\nDeep RL = Deep learning + Reinforcement learning\nUse the deep neural net...",
          "Type: lectures2025<br>Text: Deep Q-networks\n29\nRepresent Q-value function by Q-network with weights w\nsource:https://icml.cc/201...",
          "Type: lectures2025<br>Text: Deep Q-Networks with Experience Replay\n• An action-value function NN with parameters w\n• A target ac...",
          "Type: lectures2025<br>Text: 𝜖-greedy action selection methods\nAction-value \nfunction Q\n0.1\n0.4\n0.3\n0.2\nQ-values\nRandom\nAction\nBe...",
          "Type: lectures2025<br>Text: RL for Text-Based Adventure Games\nText-based Adventure Games: \n• Language-based interactions are par...",
          "Type: lectures2025<br>Text: Text-based Adventure Games\n33...",
          "Type: lectures2025<br>Text: Text-based Adventure Games\n34\nValid Action Space: \n[say manaz, push mountain, close \ndoor, get in do...",
          "Type: lectures2025<br>Text: Text-based Adventure Games\n35\nAction: get in door\nScore: +1 \nReward: 1...",
          "Type: lectures2025<br>Text: Text-based Adventure Games\n36...",
          "Type: lectures2025<br>Text: Text-based Adventure Games \nChallenges: \n• Combinatorial Action Space: Large and not fixed  \n• Commo...",
          "Type: lectures2025<br>Text: Deep Reinforcement Learning\nDeep RL = Deep learning + Reinforcement learning\nUse the deep neural net...",
          "Type: lectures2025<br>Text: Deep Q-Learning\n39\n• A Replay Buffer to store transitions:(𝑠𝑡,𝑟𝑡+1,𝑎𝑡,𝑠𝑡+1)\n• Randomly sample mini-b...",
          "Type: lectures2025<br>Text: NLP application using Q-learning: Text-based adventure games\n• Different deep Q-learning models: \n40...",
          "Type: lectures2025<br>Text: A Taxonomy of RL Algorithms \nsource:[spinning-up]41...",
          "Type: lectures2025<br>Text: Overview RL Paradigms\nPolicy-basedValue-based Actor-Critic...",
          "Type: lectures2025<br>Text: Recap: REINFORCE: Monte Carlo Policy Gradient\nPseudocode:\nfor each episode do:\nGenerate a trajectory...",
          "Type: lectures2025<br>Text: Actor-Critic\n44\nValue-functionPolicy...",
          "Type: lectures2025<br>Text: Actor-Critic\n45\n,𝑤...",
          "Type: lectures2025<br>Text: Summary\n• Value-based vs Policy-based RL\n• Q-learning learns the value of each state-action pair\n• P...",
          "Type: lectures2025<br>Text: References\n• [Sutton & Barto, 2018]Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An ...",
          "Type: lectures2025<br>Text: Policy objective\n63\n• Goal: given policy 𝜋𝜃(𝑠,𝑎), find the best parameters 𝜃\n• How to measure the qu...",
          "Type: lectures2025<br>Text: Policy Gradient \n64\nSource: [Deepmind RL2021]...",
          "Type: lectures2025<br>Text: Contextual Bandits Policy Gradient\n• Consider a one-step case (a contextual bandit) such that 𝐽(𝜃) =...",
          "Type: lectures2025<br>Text: Score function trick\n𝑎...",
          "Type: lectures2025<br>Text: Policy gradient theorem (episodic)\nsource:[UCL course on RL, lecture 7]...",
          "Type: lectures2025<br>Text: Jun.-Prof. Sophie Fellenz\n20.01.2025\nWeek 11 - Neural Topic Model\nDeep Learning for Natural \nLanguag...",
          "Type: lectures2025<br>Text: 2\n• Autoencoder\n• Variational Auto Encoder\n• Topic model\n• Variational Auto Encoder based topic mode...",
          "Type: lectures2025<br>Text: 3\nAutoencoder\nNeural Networks for Natural Language Processing Prof. Sophie Fellenz...",
          "Type: lectures2025<br>Text: 4\n- Encoder: map input 𝑥 from an 𝑛-dimensional space into a smaller 𝑚-dimensional \nspace\n- Decoder: ...",
          "Type: lectures2025<br>Text: 5\n• encoder function: 𝑧 = 𝑔𝜙(𝒙)\n• decoder function: 𝒙′ = 𝑓𝜃(𝑧)\n• objective is to minimize the sum of...",
          "Type: lectures2025<br>Text: 6\n• A deterministic AE compresses data\n• lossy (here also: blurry due to ℒ𝐴𝐸= MSE)\n• unsupervised\n• ...",
          "Type: lectures2025<br>Text: 7\nDenoising Autoencoder (DAE)\nNeural Networks for Natural Language Processing Prof. Sophie Fellenz\nS...",
          "Type: lectures2025<br>Text: 8\nVariational Autoencoder\nNeural Networks for Natural Language Processing Prof. Sophie Fellenz...",
          "Type: lectures2025<br>Text: Figure: Kingma & Welling, 2014\n9\n• Dataset 𝑋 = 𝑥(𝑖)\n𝑖=1\n𝑁\nis generated by some random variable 𝑥\n• 𝑥...",
          "Type: lectures2025<br>Text: 10\n- Decoder: parameterizes generative probability distribution\nNow we have a probabilistic model wi...",
          "Type: lectures2025<br>Text: 11\n- Encoder: For the encoder we need the posterior distribution\n𝑝(𝐳 ∣ 𝐱) =\n𝑝(𝐳,𝐱)\n𝑝(𝐱) =\n𝑝(𝐱∣𝐳)𝑝(𝐳)...",
          "Type: lectures2025<br>Text: 12\nSolution: approximate posterior 𝑞𝜙(𝒛 ∣ 𝒙)\nVariational Autoencoder (VAE)\nNeural Networks for Natur...",
          "Type: lectures2025<br>Text: 13\n• Consider a generative model 𝑝𝜃(𝑥|𝑧) and \nprior 𝑝 𝑧\n• Joint distribution: 𝑝𝜃 𝑥, 𝑧\n= 𝑝𝜃 𝑥 𝑧 𝑝(𝑧)\n...",
          "Type: lectures2025<br>Text: 14\n• For two probability distributions the KL divergence is given by:\n𝐷𝐾𝐿 𝑞𝜙(𝐳 ∣ 𝐱) ∥ 𝑝𝜃(𝐳 ∣ 𝐱) = න ...",
          "Type: lectures2025<br>Text: 15\n• More detail\nKL Divergence\nNeural Networks for Natural Language Processing Prof. Sophie Fellenz\n...",
          "Type: lectures2025<br>Text: 16\n• More detail\nKL Divergence\nNeural Networks for Natural Language Processing Prof. Sophie Fellenz\n...",
          "Type: lectures2025<br>Text: 17\nminimizing forward-KL \"stretches\" your variational distribution Q(Z) \nto cover over the entire P(...",
          "Type: lectures2025<br>Text: 18\n𝐷𝐾𝐿(𝑞𝜙(𝑧|𝑥)||𝑝𝜃(𝑧|𝑥))\n = 𝔼𝑞𝜙 𝑧 𝑥 (log 𝑞𝜙 𝑧 𝑥 − log 𝑝𝜃(𝑧|𝑥))\n = 𝔼𝑞𝜙 𝑧 𝑥 (log 𝑞𝜙 𝑧 𝑥 − log\n𝑝𝜃 𝑥 𝑧 𝑝...",
          "Type: lectures2025<br>Text: 19\n𝐷𝐾𝐿(𝑞𝜙(𝑧|𝑥)||𝑝𝜃(𝑧|𝑥))\n = 𝔼𝑞𝜙 𝑧 𝑥 (log 𝑞𝜙 𝑧 𝑥 − log 𝑝𝜃(𝑧|𝑥))\n = 𝔼𝑞𝜙 𝑧 𝑥 (log 𝑞𝜙 𝑧 𝑥 − log\n𝑝𝜃 𝑥 𝑧 𝑝...",
          "Type: lectures2025<br>Text: 20\n= 𝔼𝑞𝜙 𝑧 𝑥 log 𝑞𝜙 𝑧 𝑥 − log 𝑝𝜃 𝑥 𝑧 − log 𝑝𝜃(𝑧) + log 𝑝𝜃(𝑥)\n- ELBO= 𝔼𝑞𝜙 𝑧 𝑥 log 𝑞𝜙 𝑧 𝑥 − log 𝑝𝜃 𝑥 𝑧...",
          "Type: lectures2025<br>Text: 21\nMaximize the variational lower bound:\nℒ 𝜃, 𝜙; 𝑥 = 𝔼𝑞𝜙(𝑧|𝑥) log 𝑝𝜃 𝑥 𝑧 − 𝐷𝐾𝐿(𝑞𝜙(𝑧|𝑥)||𝑝(𝑧))\nE-step...",
          "Type: lectures2025<br>Text: 22\nMaximize the variational lower bound:\nℒ 𝜃, 𝜙; 𝑥 = 𝔼𝑞𝜙(𝑧|𝑥) log 𝑝𝜃 𝑥 𝑧 − 𝐷𝐾𝐿(𝑞𝜙(𝑧|𝑥)||𝑝(𝑧))\nGaussi...",
          "Type: lectures2025<br>Text: Maximize the variational lower bound:\nℒ 𝜃, 𝜙; 𝑥 = 𝔼𝑞𝜙(𝑧|𝑥) log 𝑝𝜃 𝑥 𝑧 − 𝐷𝐾𝐿(𝑞𝜙(𝑧|𝑥)||𝑝(𝑧))\nCategoric...",
          "Type: lectures2025<br>Text: Variational Inference\nSource: https://atcold.github.io/NYU-DLSP20/en/week08/08-3/...",
          "Type: lectures2025<br>Text: Maximize the variational lower bound:\nℒ 𝜃, 𝜙; 𝑥 = 𝔼𝑞𝜙(𝑧|𝑥) log 𝑝𝜃 𝑥 𝑧 − 𝐷𝐾𝐿(𝑞𝜙(𝑧|𝑥)||𝑝(𝑧))\nGaussian ...",
          "Type: lectures2025<br>Text: Variational Inference\nSource: https://atcold.github.io/NYU-DLSP20/en/week08/08-3/...",
          "Type: lectures2025<br>Text: Gaussian VAEs\nInput Output\nDecoderz~𝑁(𝜇, 𝜎)\nz\n𝜇\n𝜎\nVariational \nparameters 𝜙 Posterior parameters 𝜃\nG...",
          "Type: lectures2025<br>Text: 28\nPosterior distribution -> Inference model\n• Variational approximation\n• Recognition model\n• Infer...",
          "Type: lectures2025<br>Text: 29\n„The Model“ (prior + conditional, or joint) -> generative model\n• The (data) likelihood model\n• G...",
          "Type: lectures2025<br>Text: 30\n• Variational lower bound\n𝐿 𝜃, 𝜙; 𝑥 = 𝔼𝑞𝜙(𝑧|𝑥) log 𝑝𝜃(𝑥|𝑧) − 𝐾𝐿 𝑞𝜙 𝑧 𝑥 ||𝑝(𝑧)\n  = 𝔼𝑞𝜙(𝑧|𝑥) log 𝑝𝜃...",
          "Type: lectures2025<br>Text: 31\nReparameterization trick...",
          "Type: lectures2025<br>Text: 32\n• Optimize 𝐿(𝜃, 𝜙; 𝑥) wrt. 𝜙 of 𝑞𝜙 𝑧 𝑥\n• ELBO: 𝐿(𝜃, 𝜙; 𝑥) = 𝔼𝑞𝜙(𝑧∣𝑥) log 𝑝𝜃(𝑥, 𝑧) + 𝐻 𝑞𝜙(𝑧 ∣ 𝑥)\n•...",
          "Type: lectures2025<br>Text: 33\n• Gradient estimate with reparameterization trick\n    𝑧 ∼ 𝑞𝜙(𝑧 ∣ 𝑥) ⇔ 𝑧 = 𝑔𝜙(𝜖, 𝑥), 𝜖 ∼ 𝑝(𝜖)\n∇𝜙𝔼𝑞...",
          "Type: lectures2025<br>Text: 34\n• Score function gradient is broadly applicable to nearly any \nvariational distribution, regardle...",
          "Type: lectures2025<br>Text: VAEs: Algorithm\n[Kingma & Welling, 2014]...",
          "Type: lectures2025<br>Text: Projecting Means in Latent Space\nSource: https://atcold.github.io/pytorch-Deep-Learning/en/week08/08...",
          "Type: lectures2025<br>Text: Projecting Means in Latent Space\nSource: https://atcold.github.io/pytorch-Deep-Learning/en/week08/08...",
          "Type: lectures2025<br>Text: 38\nLatent code interpolation and sentences generation from VAEs [Bowman et \nal., 2015] [5]\nVAE: Exam...",
          "Type: lectures2025<br>Text: 39\nTopic Models\nNeural Networks for Natural Language Processing Prof. Sophie Fellenz...",
          "Type: lectures2025<br>Text: ▪ Word clouds (important words are bigger)\n▪ Probability distributions over words\n40\nTopics\nNeural N...",
          "Type: lectures2025<br>Text: 41\nTopic Models\n▪ Input: unstructured text data\n▪ Output: Topics\n▪ No annotations, labels, tags …\n▪ ...",
          "Type: lectures2025<br>Text: 0\n0,5\n1\n42\nSPATIAL ORGANIZATION OF NEURAL NETWORKS: A \nPROBABILISTIC MODELING APPROACH\nprobabilistic...",
          "Type: lectures2025<br>Text: 43\nSPATIAL ORGANIZATION OF NEURAL NETWORKS: A \nPROBABILISTIC MODELING APPROACH\nTopics Documents\nTopi...",
          "Type: lectures2025<br>Text: 44\nVAE-based Topic Models\nNeural Networks for Natural Language Processing Prof. Sophie Fellenz...",
          "Type: lectures2025<br>Text: 45\nDirichlet VAE\nInput (BoW) Output (BoW)\nz~𝐷𝑖𝑟(𝛼)𝛼\n𝑧 ∈ ℝ𝑚\nDistribution over topics\nFood\nPlace\nNice\n...",
          "Type: lectures2025<br>Text: 46\n• The Dirichlet distribution is a distribution over the (K-1)-\ndimensional simplex\n• It is parame...",
          "Type: lectures2025<br>Text: 47\nProbability Simplex\n1-D Simplex\n2-D Simplex\nTopic 1 Topic 2\n0.1/0.9\n0.5/0.5\nTopic 1 Topic 2\nTopic...",
          "Type: lectures2025<br>Text: 48\n• If 𝜋 ∼ 𝐷𝑖𝑟𝑖𝑐ℎ𝑙𝑒𝑡(𝛼1, … , 𝛼𝐾) then 𝜋𝑘 ≥ 0 for all k, and \n∑𝑘=1\n𝐾 𝜋𝑘 = 1\n• Expectation: 𝔼 𝜋1, … ,...",
          "Type: lectures2025<br>Text: 49\n•The concentration parameter 𝛼 determines the distribution \nover atom sizes\n•Small values of 𝛼 gi...",
          "Type: lectures2025<br>Text: 50\n• One document should be assigned to as few topics as possible\n• Why?\n• If one document is assign...",
          "Type: lectures2025<br>Text: 51\n• Basic neural topic modelling (NTM) architecture, was proposed by Miao et al. \n(2015) (Gaussian)...",
          "Type: lectures2025<br>Text: 52\n• menu minutes service ordered new order came went table way\n• wait try minutes going time good v...",
          "Type: lectures2025<br>Text: 53\n• nice service went wait great [UNK] think want food time\n• ordered [UNK] nice going like people ...",
          "Type: lectures2025<br>Text: 54\nℒ 𝜃, 𝜙; 𝑥 = 𝔼𝑞𝜙(𝑧|𝑥) log 𝑝𝜃 𝑥 𝑧 − 𝐷𝐾𝐿(𝑞𝜙(𝑧|𝑥)||𝑝(𝑧))\nℒ 𝜃, 𝜙; 𝑥 = 𝔼𝑞𝜙(𝑧|𝑥) log 𝑝𝜃 𝑥 𝑧 − 𝜷𝐷𝐾𝐿(𝑞𝜙(𝑧|...",
          "Type: lectures2025<br>Text: 55\n• is are they your do buy always enjoy go favorite\n• to look 's really so have looking pretty bur...",
          "Type: lectures2025<br>Text: 56\n• Thai, soup, rice, tuna, roll\n• Bartender, tables, drinks, server, restaurant\n• Burger, fries, e...",
          "Type: lectures2025<br>Text: 61\n• VAEs are generative models that learn a generative distribution \nfor the data\n• Reparameterizat...",
          "Type: lectures2025<br>Text: 62\n• Kingma, Diederik, and Max Welling. “Auto-encoding variational \nBayes” https://arxiv.org/abs/131...",
          "Type: lectures2025<br>Text: Neural networks for \nNatural Language Processing\nJun.-Prof. Dr. Sophie Fellenz\nWeek  08 – Fine-tunin...",
          "Type: lectures2025<br>Text: Agenda\n•BERT Fine-Tuning\n•GPT Fine-Tuning\n•RLHF Fine-tuning...",
          "Type: lectures2025<br>Text: BERT\n3...",
          "Type: lectures2025<br>Text: 4\nBERT\nSource: https://humboldt-\nwi.github.io/blog/research/information_systems_1\n920/bert_blog_post...",
          "Type: lectures2025<br>Text: 5\nBERT Pre-Training\nMasked Language model (MLM): Take sentence, mask single words\nand let BERT predi...",
          "Type: lectures2025<br>Text: 6\nBERT Pre-Training\nNext sentence prediction (NSP): Take two sentences and ask\nBERT if the second se...",
          "Type: lectures2025<br>Text: 7\nBERT Pre-Training\n• Take two sentences\n• Mask some words and embed into 𝐸𝑖\n• BERT outputs vectors ...",
          "Type: lectures2025<br>Text: 8\nBERT Pre-Training (MLM)\n• For 𝐸𝑖 masked :\n• Transform 𝑇𝑖 into probabilities over vocabulary\n• Chec...",
          "Type: lectures2025<br>Text: 9\nBERT Pre-Training (NSP)\n• Transform 𝐶 into 2-dim. probability \n• Sentences belong (don‘t belong) t...",
          "Type: lectures2025<br>Text: 10\nBERT Fine-Tuning\n• For application adjust model by adding layers on input and \noutput to fit the ...",
          "Type: lectures2025<br>Text: 11\nBERT Fine-Tuning...",
          "Type: lectures2025<br>Text: GPT\n12...",
          "Type: lectures2025<br>Text: 13\nGPT\n• GPT (Generative Pre-Trained Transformer) is a \nlanguage model to produce human-like text\n• ...",
          "Type: lectures2025<br>Text: 14\nGPT Unsupervised Pre-Training\n• Input context 𝑥𝑖−1,…,𝑥𝑖−𝑘\n• Output probability distribution over ...",
          "Type: lectures2025<br>Text: 15\nGPT Supervised Fine-Tuning\n• Given labelled dataset of sentences 𝐶\n• For sentence 𝑥 = (𝑥1,…,𝑥𝑛) w...",
          "Type: lectures2025<br>Text: 16\nGPT Supervised Fine-Tuning\n• The objective function is\n𝐿2 𝐶 = ෍\n(𝑥,𝑦)\nlog(𝑃 𝑦|𝑥,Θ )\n• To improve ...",
          "Type: lectures2025<br>Text: 17\nGPT Supervised Fine-Tuning\n• The exact training procedure depends on the task\n• Example (Entailme...",
          "Type: lectures2025<br>Text: 18\nGPT Supervised Fine-Tuning\nSource: Radford, \nAlec, et al. \n\"Improving language \nunderstanding by ...",
          "Type: lectures2025<br>Text: Fine-tuning based on Reinforcement Learning\n19...",
          "Type: lectures2025<br>Text: ChatGPT:Optimizing Language models for dialogue...",
          "Type: lectures2025<br>Text: Why RLHF?\nHow to create a loss function for\n• What is funny?\n• What is ethical?\n• What is safe?\n• Wh...",
          "Type: lectures2025<br>Text: Core technique: Learning from Human Feedback (RLHF)...",
          "Type: lectures2025<br>Text: Core technique: Learning from Human Feedback (RLHF)\n• Using human preferences as a reward signal to ...",
          "Type: lectures2025<br>Text: Reinforcement learning on human feedback (RLHF)\n• Three steps in general:\n1. Pretraining a language ...",
          "Type: lectures2025<br>Text: RLHF: Step 1, Pretraining the language model\nSource: \nhttps://huggingface.co/blog/rlhf...",
          "Type: lectures2025<br>Text: RLHF: Step 2, reward model training\nSource: \nhttps://huggingface.co/blog/rlhf...",
          "Type: lectures2025<br>Text: RLHF: Step 3, fine-tuning with RL\nSource: \nhttps://huggingface.co/blog/rlhf...",
          "Type: lectures2025<br>Text: What is ChatGPT? \n• It is a sibling model of InstructGPT (Ouyang et al. 2022) which is trained (thro...",
          "Type: lectures2025<br>Text: InstructGPT\n30\nSource: https://openai.com/blog/instruction-\nfollowing/...",
          "Type: lectures2025<br>Text: InstructGPT: High-level methodology\n31...",
          "Type: lectures2025<br>Text: Step 1: Supervised Fine-Tuning (SFT)\nLabelers were asked to write three kinds of prompts:\n• Plain: W...",
          "Type: lectures2025<br>Text: Step 1: Supervised Fine-Tuning (SFT)\n• 40 contract workers were hired and screened\n• Labelers were a...",
          "Type: lectures2025<br>Text: Step 2: Reward model (RM)\n• Labelers were asked to rank between K=4 and K=9 model outputs according ...",
          "Type: lectures2025<br>Text: Step 3: RLHF with PPO\n• Fine-tune SFT model using PPO\n• PPO dataset contains 31k prompts from the AP...",
          "Type: lectures2025<br>Text: Reinforcement Learning and PPO\n36...",
          "Type: lectures2025<br>Text: Timeline for RL in Games\n• 1992: TD-Gammon, temporal difference learning, Backgammon\n• 1997: Deep Bl...",
          "Type: lectures2025<br>Text: Applying RL in NLP with Robotics\n38\nSource: https://say-can.github.io/...",
          "Type: lectures2025<br>Text: Outline\nToday:\n1. Introduction to Reinforcement Learning (RL) \n2. Policy-Based RL\n• Policy Gradient ...",
          "Type: lectures2025<br>Text: Policy-based RL\nAdvantages: \n• True objective\n• Can learn stochastic policies\n• Effective in high-di...",
          "Type: lectures2025<br>Text: Introduction to RL\n‘Reinforcement learning is learning what to do- how to map \nsituations to actions...",
          "Type: lectures2025<br>Text: Introduction to RL\n‘Reinforcement learning is learning what to do- how to map \nsituations to actions...",
          "Type: lectures2025<br>Text: Introduction to RL for NLP\n„The window is open!“ (is this a statement of fact?)\n„Can you close the w...",
          "Type: lectures2025<br>Text: Reinforcement Learning Problem\n44...",
          "Type: lectures2025<br>Text: Markov Decision Processes\n45...",
          "Type: lectures2025<br>Text: Agent‘s Learning Task\n46...",
          "Type: lectures2025<br>Text: Example: Aliased Grid World\nsource:[UCL course on RL, lecture 7]...",
          "Type: lectures2025<br>Text: Example: Aliased Grid World\nsource:[UCL course on RL, lecture 7]...",
          "Type: lectures2025<br>Text: Example: Aliased Grid World\nsource:[UCL course on RL, lecture 7]...",
          "Type: lectures2025<br>Text: Policy objective\n50\n• Goal: given policy 𝜋𝜃(𝑠,𝑎), find the best parameters 𝜃\n• How to measure the qu...",
          "Type: lectures2025<br>Text: Policy Gradient \n51\nSource: [Deepmind RL2021]...",
          "Type: lectures2025<br>Text: Contextual Bandits Policy Gradient\n• Consider a one-step case (a contextual bandit) such that 𝐽(𝜃) =...",
          "Type: lectures2025<br>Text: Score function trick\n𝑎...",
          "Type: lectures2025<br>Text: Policy gradient theorem (episodic)\nsource:[UCL course on RL, lecture 7]...",
          "Type: lectures2025<br>Text: Theorem Proof\n55source:[UCL course on RL, lecture 7]...",
          "Type: lectures2025<br>Text: Proof continued\n56\nsource:[UCL course on RL, lecture 7]...",
          "Type: lectures2025<br>Text: Policy gradient training\n57source:[UCL course on RL, lecture 7]...",
          "Type: lectures2025<br>Text: REINFORCE: Monte Carlo Policy Gradient\nPseudocode:\nfor each episode do:\nGenerate a trajectory Rollou...",
          "Type: lectures2025<br>Text: Actor-Critic\n59\nValue-functionPolicy...",
          "Type: lectures2025<br>Text: Overview RL Paradigms\nPolicy-basedValue-based Actor-Critic...",
          "Type: lectures2025<br>Text: Proximal Policy Optimization (PPO)\n• Motivation: Avoid too large policy updates -> improve the train...",
          "Type: lectures2025<br>Text: Proximal Policy Optimization (PPO)\n62\nsource:[UCL course on RL, lecture 7]...",
          "Type: lectures2025<br>Text: Step 3: RLHF with PPO\n𝑜𝑏𝑗𝑒𝑐𝑡𝑖𝑣𝑒 𝜙\n= 𝐸 𝑥,𝑦 ∼𝐷𝜋𝜙\n𝑅𝐿 𝑟𝜃 𝑥,𝑦 −𝛽log(𝜋𝜙\n𝑅𝐿(𝑦|𝑥)/𝜋𝑆𝐹𝑇(𝑦|𝑥)) +𝛾𝐸𝑥∼𝐷𝑝𝑟𝑒𝑡𝑟𝑎𝑖𝑛 ...",
          "Type: lectures2025<br>Text: Variations on the methodology\nAlmost all papers to date have tweaks:\nAnthropic\n• Initial policy help...",
          "Type: lectures2025<br>Text: InstructGPT vs Anthropic\nSource: https://youtu.be/2MBJOuVq380\nSource: Bai et al. “Training a Helpful...",
          "Type: lectures2025<br>Text: InstructGPT vs Anthropic\nSource: https://youtu.be/2MBJOuVq380\nSource: Bai et al. “Training a Helpful...",
          "Type: lectures2025<br>Text: InstructGPT vs Anthropic\nSource: https://youtu.be/2MBJOuVq380\nSource: Bai et al. “Training a Helpful...",
          "Type: lectures2025<br>Text: Summary\n• Policy-gradient methods directly optimize policy (in our case language model)\n• PPO tries ...",
          "Type: lectures2025<br>Text: References\n• [Sutton & Barto, 2018]Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An ...",
          "Type: lectures2025<br>Text: Jun.-Prof. Sophie Fellenz\nWeek 4 – Language Modeling and Neural Networks\n11 Nov 2024\nNeural Networks...",
          "Type: lectures2025<br>Text: • Is Skipgram based on neural networks?\n• What is the difference between a neural network word\nembed...",
          "Type: lectures2025<br>Text: • Difference:\n• Skipgram embeddings are used inside neural\nnetworks (first layer)\n• NN embeddings ar...",
          "Type: lectures2025<br>Text: Language Models\n4...",
          "Type: lectures2025<br>Text: • Jane went to the store.\n• store to Jane went the.\n• Jane went store.\n• Jane goed to the store.\n• T...",
          "Type: lectures2025<br>Text: • Jane went to the store.\n• store to Jane went the.\n• Jane went store.\n• Jane goed to the store.\n• T...",
          "Type: lectures2025<br>Text: 𝑃 𝑋 = ෑ\n𝑖=1\n𝐼\n𝑃(𝑥𝑖|𝑥1,…,𝑥𝑖−1)\nThe big problem: How do we predict\n𝑃 𝑥𝑖 𝑥1,…,𝑥𝑖−1\n??\nProbabilistic lan...",
          "Type: lectures2025<br>Text: Score sentences:\n• Jane went to the store . -> high\n• Store to Jane went the . -> low\n• (same as cal...",
          "Type: lectures2025<br>Text: Count-based Language Models\n9...",
          "Type: lectures2025<br>Text: Independence assumption: 𝑃 𝑥𝑖 𝑥1,…,𝑥𝑖−1 ≈ 𝑃(𝑥𝑖)\nCount-based maximum-likelihood estimation:\n𝑃𝑀𝐿𝐸 𝑥𝑖 =...",
          "Type: lectures2025<br>Text: Limit context length to 𝑛, count, and divide\n𝑃𝑀𝐿 𝑥𝑖 𝑥𝑖−𝑛+1,…,𝑥𝑖−1 ≔ 𝑐(𝑥𝑖−𝑛+1,…,𝑥𝑖)\n𝑐(𝑥𝑖−𝑛+1,…,𝑥𝑖−1)\n...",
          "Type: lectures2025<br>Text: Additive/Dirichlet:\n𝑃 𝑥𝑖 𝑥𝑖−𝑛+1,…,𝑥𝑖−1 ≔ 𝑐 𝑥𝑖−𝑛+1,…,𝑥𝑖 +𝛼𝑃(𝑥𝑖|𝑥𝑖−𝑛+2,…,𝑥𝑖−1)\n𝑐 𝑥𝑖−𝑛+1,…,𝑥𝑖−1 +𝛼\nDisc...",
          "Type: lectures2025<br>Text: Cannot share strength among similar words\nsolution: class based language models\nCannot condition on ...",
          "Type: lectures2025<br>Text: Cannot handle long-distance dependencies\nsolution: cache, trigger, topic, syntactic models, etc.\n14\n...",
          "Type: lectures2025<br>Text: • Neural language models (next) achieve better \nperformance, but\n• n-gram models are extremely fast ...",
          "Type: lectures2025<br>Text: LM Evaluation\n16...",
          "Type: lectures2025<br>Text: Log-likelihood:\n𝐿𝐿 ℇ𝑡𝑒𝑠𝑡 = ෍\n𝐸∈ℇ𝑡𝑒𝑠𝑡\nlog𝑃(𝐸)\nPer-word Log Likelihood:\n𝑊𝐿𝐿 ℇ𝑡𝑒𝑠𝑡 = 1\nσ𝐸∈ℇ𝑡𝑒𝑠𝑡 𝐸 ෍\n𝐸∈ℇ...",
          "Type: lectures2025<br>Text: • Important: the vocabulary must be the same over models \nyou compare\n• Or more accurately, all mode...",
          "Type: lectures2025<br>Text: Log-linear models\n19...",
          "Type: lectures2025<br>Text: • Calculate features of the context\n• Based on the features, calculate probabilities\n• Optimize feat...",
          "Type: lectures2025<br>Text: Calculate features of the context, calculate probabilities\nFeature weights optimized by SGD, etc 21\n...",
          "Type: lectures2025<br>Text: Previous words: “giving a\"\n22\nExample:\nWords we‘re\npredicting\nHow likely\nare they?\nHow likely\nare th...",
          "Type: lectures2025<br>Text: • Calculate the gradient of the loss function with respect to \nthe parameters\n• How? Use the chain r...",
          "Type: lectures2025<br>Text: 24\nWhat Problems are Handled?\nCannot share strength among similar words\nnot solved yet \nCannot condi...",
          "Type: lectures2025<br>Text: Cannot handle long-distance dependencies\nNot solved yet\n25\nProblems and solutions?\nFor tennis class ...",
          "Type: lectures2025<br>Text: Beyond linear models\n26...",
          "Type: lectures2025<br>Text: Students take tests → high Teachers take tests → low\nStudents write tests → low Teachers write tests...",
          "Type: lectures2025<br>Text: Original Motivation: Neurons in the Brain\nCurrent Conception: Computation Graphs\n28\n“Neural” Nets...",
          "Type: lectures2025<br>Text: 29\n𝑥1\n𝑥2\n𝑥3\nInput neurons:\n𝑤1,1\n𝑤2,1\n𝑤3,1\n𝑏1\n𝑥1𝑤1,1 +𝑥2𝑤2,1 +𝑥3𝑤3,1 +𝑏1\nOutput neurons:\n𝑥1𝑤1,2 +𝑥2𝑤2...",
          "Type: lectures2025<br>Text: 30\n𝑥1\n𝑥2\n𝑥3\nInput neurons: 𝑏\n𝑥1𝑤1,1 +𝑥2𝑤2,1 +𝑥3𝑤3,1 +𝑏1\nOutput neurons:\n𝑥1𝑤1,2 +𝑥2𝑤2,2 +𝑥3𝑤3,2 +𝑏2\n𝑥...",
          "Type: lectures2025<br>Text: 31\n𝑥1\n𝑥2\n𝑥3\nInput neurons: 𝑏\n𝑥1𝑤1,1 +𝑥2𝑤2,1 +𝑥3𝑤3,1 +𝑏1\nOutput neurons:\n𝑥1𝑤1,2 +𝑥2𝑤2,2 +𝑥3𝑤3,2 +𝑏2\n𝑥...",
          "Type: lectures2025<br>Text: 32\nInput layer Output\n𝑶𝒖𝒕𝒑𝒖𝒕 = 𝝈(…𝝈 𝒙𝑻𝑾𝟏 +𝒃𝟏\n𝑻 …𝑾𝒏 +𝒃𝒏𝑻)\nUsually we use multiple layers\nHidden layer...",
          "Type: lectures2025<br>Text: 33\nInput layer Output\nHow does this work specifically?\nExample: Given cat or dog image.\nTask: Find o...",
          "Type: lectures2025<br>Text: 34\nInput layer Output\nWhy does this work?\n• One can prove that any function, i.e. the cat-dog recogn...",
          "Type: lectures2025<br>Text: 35\nInput layer Output\nHow do we find out how large it needs to be?\n• Experimentation!\nHow do we find...",
          "Type: lectures2025<br>Text: • Given: \nTraining data, i.e. input data 𝑥 where desired output 𝑦is\nknown.\nNeural Network 𝑛𝑊,𝑏\n• Out...",
          "Type: lectures2025<br>Text: expression:\n𝑥\ngraph:\nA node is a {tensor, matrix, vector, scalar} value\n37\n𝑥...",
          "Type: lectures2025<br>Text: • An edge represents a function argument (and also a data dependency). \nThey are just pointers to no...",
          "Type: lectures2025<br>Text: expression:\n𝑥𝑇𝑊+𝑏\ngraph:\nFunctions can be nullary, unary, binary, ... n-ary. Often they are unary or...",
          "Type: lectures2025<br>Text: expression:\n𝜎(𝑥𝑇𝑊+𝑏𝑇)\ngraph:\n40\n𝑥\n𝑓1 𝑥,𝑊 = 𝑥𝑇𝑊\n𝑊\n∗\n𝑓2 𝑢,𝑏 = 𝑢+𝑏𝑇\n𝑏\n+\n𝑓3 𝑢 = 𝜎(𝑢)\n𝜎...",
          "Type: lectures2025<br>Text: expression:\n𝜎(𝑥𝑇𝑊+𝑏𝑇)−𝑦𝑇\ngraph:\n41\n𝑥\n𝑓1 𝑥,𝑊 = 𝑥𝑇𝑊\n𝑊\n∗\n𝑓2 𝑢,𝑏 = 𝑢+𝑏𝑇\n𝑏\n+\n𝑓3 𝑢 = 𝜎(𝑢)\n𝜎\n𝑓4 𝑢,𝑦 = 𝑢−𝑦\n−...",
          "Type: lectures2025<br>Text: expression:\nLoss = 𝜎(𝑥𝑇𝑊+𝑏𝑇 −𝑦𝑇ԡ2 = 𝑓5(𝑓4(𝑓3(𝑓2(𝑓1(𝑥,𝑊),𝑏)),𝑦))\ngraph:\n42\n𝑥\n𝑓1 𝑥,𝑊 = 𝑥𝑇𝑊\n𝑊\n∗\n𝑓2 𝑢,𝑏 ...",
          "Type: lectures2025<br>Text: • Graph construction\n• Forward propagation\nIn topological order, compute the value of the node \ngive...",
          "Type: lectures2025<br>Text: 44\nForward Propagation\n𝑥\n𝑓1 𝑥,𝑊 = 𝑥𝑇𝑊\n𝑊\n∗\n𝑓2 𝑢,𝑏 = 𝑢+𝑏𝑇\n𝑏\n+\n𝑓3 𝑢 = 𝜎(𝑢)\n𝜎\n𝑓4 𝑢,𝑦 = 𝑢−𝑦\n−\n𝑦\n𝑓5 𝑢 = 𝑢 ...",
          "Type: lectures2025<br>Text: 45\nForward Propagation\n𝑥\n𝑓1 𝑥,𝑊 = 𝑥𝑇𝑊\n𝑊\n𝑢1\n𝑓2 𝑢,𝑏 = 𝑢+𝑏𝑇\n𝑏\n+\n𝑓3 𝑢 = 𝜎(𝑢)\n𝜎\n𝑓4 𝑢,𝑦 = 𝑢−𝑦\n−\n𝑦\n𝑓5 𝑢 = 𝑢...",
          "Type: lectures2025<br>Text: 46\nForward Propagation\n𝑥\n𝑓1 𝑥,𝑊 = 𝑥𝑇𝑊\n𝑊\n𝑢1\n𝑓2 𝑢,𝑏 = 𝑢+𝑏𝑇\n𝑏\n𝑢2\n𝑓3 𝑢 = 𝜎(𝑢)\n𝜎\n𝑓4 𝑢,𝑦 = 𝑢−𝑦\n−\n𝑦\n𝑓5 𝑢 = ...",
          "Type: lectures2025<br>Text: 47\nForward Propagation\n𝑥\n𝑓1 𝑥,𝑊 = 𝑥𝑇𝑊\n𝑊\n𝑢1\n𝑓2 𝑢,𝑏 = 𝑢+𝑏𝑇\n𝑏\n𝑢2\n𝑓3 𝑢 = 𝜎(𝑢)\n𝑢3\n𝑓4 𝑢,𝑦 = 𝑢−𝑦\n−\n𝑦\n𝑓5 𝑢 =...",
          "Type: lectures2025<br>Text: 48\nForward Propagation\n𝑥\n𝑓1 𝑥,𝑊 = 𝑥𝑇𝑊\n𝑊\n𝑢1\n𝑓2 𝑢,𝑏 = 𝑢+𝑏𝑇\n𝑏\n𝑢2\n𝑓3 𝑢 = 𝜎(𝑢)\n𝑢3\n𝑓4 𝑢,𝑦 = 𝑢−𝑦\n𝑢4\n𝑦\n𝑓5 𝑢 ...",
          "Type: lectures2025<br>Text: 49\nForward Propagation\n𝑥\n𝑓1 𝑥,𝑊 = 𝑥𝑇𝑊\n𝑊\n𝑢1\n𝑓2 𝑢,𝑏 = 𝑢+𝑏𝑇\n𝑏\n𝑢2\n𝑓3 𝑢 = 𝜎(𝑢)\n𝑢3\n𝑓4 𝑢,𝑦 = 𝑢−𝑦\n𝑢4\n𝑦\n𝑓5 𝑢 ...",
          "Type: lectures2025<br>Text: • Aim: Minimize loss 𝑓 𝑥,𝑊,𝑏 = ԡ𝜎(𝑥𝑇𝑊 +𝑏𝑇)−𝑦𝑇ԡ2\nw.r.t. weights 𝑊,𝑏\n• Idea: Gradient = Direction of h...",
          "Type: lectures2025<br>Text: Back-propagation:\n• Process examples in reverse topological order\n• Calculate the derivatives of the...",
          "Type: lectures2025<br>Text: Expression:\nLoss = ԡ𝜎(𝑥𝑇𝑊 +𝑏𝑇)−𝑦𝑇ԡ2\nAim: Minimize loss by optimizing weights 𝑊,𝑏\n52\nBack Propagation...",
          "Type: lectures2025<br>Text: Step 1: Compute all derivatives at every node wrt. the inputs\n(at relevant edges)\nExample: \n𝜕𝑓2\n𝜕𝑏 =...",
          "Type: lectures2025<br>Text: Step 1: Compute all derivatives at every node wrt. the inputs.\n54\nBack Propagation\n𝑥\n𝑓1 𝑥,𝑊 = 𝑥𝑇𝑊\n𝑊\n...",
          "Type: lectures2025<br>Text: Step 2: Use the chain rule.\nExample:𝑓 𝑥,𝑊,𝑏 = 𝜎(𝑥𝑇𝑊 +𝑏𝑇 −𝑦𝑇ԡ2\n= 𝑓5(𝑓4(𝑓3(𝑓2(𝑓1(𝑥,𝑊),𝑏)),𝑦))\n𝜕𝑓(𝑥,𝑊,𝑏...",
          "Type: lectures2025<br>Text: Step 2: Use the chain rule.\nExample:𝑓 𝑥,𝑊,𝑏 = 𝜎(𝑥𝑇𝑊 +𝑏𝑇 −𝑦𝑇ԡ2\n= 𝑓5(𝑓4(𝑓3(𝑓2(𝑓1(𝑥,𝑊),𝑏)),𝑦))\n𝜕𝑓(𝑥,𝑊,𝑏...",
          "Type: lectures2025<br>Text: Step 2: Use the chain rule.\nExample:𝑓 𝑥,𝑊,𝑏 = 𝜎(𝑥𝑇𝑊 +𝑏𝑇 −𝑦𝑇ԡ2 = 𝑓5(𝑓4(𝑓3(𝑓2(𝑓1(𝑥,𝑊),𝑏)),𝑦))\n𝜕𝑓(𝑥,𝑊,𝑏...",
          "Type: lectures2025<br>Text: Step 2: Use the chain rule.\nSimilarly, we calculate\n𝜕𝑓\n𝜕𝑊 .Note that we already have calculated\n𝜕𝑓\n𝜕...",
          "Type: lectures2025<br>Text: Step 3: Apply Gradient descent\nUpdate: 𝛼 > 0learning rate\n𝑊𝑛𝑒𝑤 = 𝑊𝑜𝑙𝑑 −𝛼 𝜕𝑓\n𝜕𝑊\n𝑏𝑛𝑒𝑤 = 𝑏𝑜𝑙𝑑 −𝛼𝜕𝑓\n𝜕𝑏\n5...",
          "Type: lectures2025<br>Text: Back to language modeling\n60...",
          "Type: lectures2025<br>Text: • See Bengio et al. 2003\n61\nFeed-forward Neural Language Models\nDeep Learning for Natural Language P...",
          "Type: lectures2025<br>Text: • Word embeddings capture features of words\n• e.g. feature 1 indicates verbs, feature 2 indicates de...",
          "Type: lectures2025<br>Text: 63\nWhere is Strength Shared?\nlookup\ngiving\nlookup\n𝑡𝑎𝑛ℎ(𝑊1ℎ+𝑏) softmax=+\nBias scores probs\nW\nWord emb...",
          "Type: lectures2025<br>Text: 65\nWhat Problems are Handled?\nCannot share strength among similar words\nsolved, and similar contexts...",
          "Type: lectures2025<br>Text: Cannot handle long-distance dependencies\nNot solved yet\n66\nProblems and solutions?\nFor tennis class ...",
          "Type: lectures2025<br>Text: • Neural networks allow design of arbitrarily complex\nfunctions!\n• In future classes:\n▪ Recurrent ne...",
          "Type: lectures2025<br>Text: Next lecture\nRecurrent Neural Networks...",
          "Type: lectures2025<br>Text: ● CMU Advanced NLP Course:\n● https://phontron.com/class/anlp2022/schedule.html\n● Sören Laue\n● Feibai...",
          "Type: lectures2025<br>Text: ● Video on Backprop by Andrej Karpathy:\n• https://youtu.be/VMj-3S1tku0\n• Video on Language modeling ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nLecture 7 – Language Models – Generating Text \nfrom ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 2\nImage Source: https://www.tensorflow.org/text/tut...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n8\nI have eaten an apple\nI have eaten an apple\nI have...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n9\nI\nI\nI\nI\nI\nI\neaten\neaten\neaten\neaten\nan\nan\nan\napple...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n10...",
          "Type: lectures2025<br>Text: Beam search\nBeam search: Keep track of the 𝑘 most likely partial translations\n𝑘 is the beam size\n11...",
          "Type: lectures2025<br>Text: Beam search\nBeam size: 2\n12\n<Start>\na\nthe\nSource: https://web.stanford.edu/class/archive/cs/cs224n/c...",
          "Type: lectures2025<br>Text: Beam search\nBeam size: 2\n13\n<Start>\na\nthe\npoor\npeople\npoor\nperson\nSource: https://web.stanford.edu/c...",
          "Type: lectures2025<br>Text: Beam search\nBeam size: 2\n14\n<Start>\na\nthe\npoor\npeople\npoor\nperson\nare\ndon‘t\nbut\nperson\nSource: https...",
          "Type: lectures2025<br>Text: Beam search\nBeam size: 2\n15\n<Start>\na\nthe\npoor\npeople\npoor\nperson\nare\ndon‘t\nbut\nperson\nalways\nnot\nha...",
          "Type: lectures2025<br>Text: Beam search\nBeam size: 2\n16\n<Start>\na\nthe\npoor\npeople\npoor\nperson\nare\ndon‘t\nbut\nperson\nalways\nnot\nha...",
          "Type: lectures2025<br>Text: Beam search\nBeam size: 2\n17\nSource: https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1184/lec...",
          "Type: lectures2025<br>Text: Beam search\nBeam size: 2\n18\nSource: https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1184/lec...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Recap: language models\n• Generating text: Intro\n• ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• A language model (LM) 𝑝𝜃 is a probability distribu...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Intuitively, are Language Models naturally suited ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Language models might not be perfectly-suited for ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Language models might not be perfectly-suited for ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Language models might not be perfectly-suited for ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nFundamentals of discrete distributions\n25...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• A discrete probability distribution p(x) is define...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n“Small” discrete sets\n0.25 0.50 0.25 0.15\ne.g., a vo...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n“Large” discrete sets\ne.g., sequences of up to lengt...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• A discrete probability distribution p(x) is define...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Entropy is arguably the single most useful propert...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nIntuition of Entropy –low entropy and high entropy\nI...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nExtreme Example of\nHigh-entropy and Low-entropy toke...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nIn natural language generation, we often describe ta...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nWe have a discrete set 𝜒and a distribution 𝑝 𝑥\nThe e...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nSometimes, the true next token distribution has many...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nSometimes, the true next token distribution has many...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nSometimes, the true next token distribution has few ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nContext lowers entropy\nMy Car\nis\ndrives\nwas\nhas\nruns...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nAdding Context Reduces Entropy (Important for the co...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nWe have a discrete set 𝜒and a distribution 𝑝 𝑥\nThe m...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nWe have a discrete set 𝜒and a distribution 𝑝 𝑥 , and...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nHere are some examples of semantically similar examp...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nFrom parsing to machine translation, there’s a long ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nIn a high entropy distribution, most probability ten...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Often, researchers/developers are not working with...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nDecoding as a choice of Algorithm + Scoring  \nFuncti...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Given a distribution 𝑝𝜃, how can we characterize d...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Given a distribution 𝑝𝜃, how can we characterize d...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Given a distribution 𝑝𝜃, how can we characterize d...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Given a distribution 𝑝𝜃, how can we characterize d...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nCommon Choices of Scoring Functions\n51...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nIf current language generators were already flawless...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nWhy do we need alternative scoring functions?\n53\nUnf...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nThe mean-seeking nature of\nthe MLE training objectiv...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• In this section we’ll choose multinomial sampling ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Ancestral sampling is obtained when the scoring fu...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nConsequence of temperature sampling\nOne day a cat de...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nTemperature Sampling code examples –\nHigher Temperat...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nTemperature Sampling code examples –\nLower Temperatu...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• The presence of an unreliable tail suggests the us...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nTop-k sampling\n• Simply truncate the tail by selecti...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nTwo failure modes of top-k sampling\nSuppose we fix k...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nPerplexity and generation quality\n• Some scoring fun...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nPerplexity and generation quality\n• High-quality tex...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nMirostat\nAt each generation step\n• Choose the top-k ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nMirostat\nTune the size of the top-k set to sample a ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nNucleus sampling\n• Select tail size dynamically by o...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nDecreasing the pi in Nucleus Sampling decreases the ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nA failure mode of nucleus sampling\nSuppose we fix 𝜋=...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• The absolute probability principle — that words ou...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nEpsilon Sampling vs Top-K Sampling vs  Nucleus Sampl...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nPrompting: The Precursor to Controlled \nGeneration\n7...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nWhat is prompting?\n• So far, we’ve looked at how to ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nPrompting in our framework\n• Prompting can be viewed...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nPrompting in our framework\n• Prompting can be viewed...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nWhy does prompting work?\nRecall: Context (often) low...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nWhy does prompting work?\nIntuition: providing more c...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nPrompting to solve lower entropy tasks\n81\nEnglish:\nT...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nTypes of Prompting: Demonstrations\nFor a task with i...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nTypes of Prompting: Learning Prompts\nFor a task with...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nTypes of Prompting: Chain-of-Thought\nFor a task with...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nIs prompting the definite solution?\n85...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nCurrent Trends in Language Generation: Larger Models...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nImplications of Larger Models\n• Many of the methods ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nFine-Tuning Approaches\n• If we’re willing to retrain...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nInstruction-Tuned and RLHF Language Models\nBase pret...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nInstruction-Tuned and RLHF Language Models\nInstructi...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nInstruction-Tuned and RLHF Language Models\nRLHF like...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nConclusion\n• High-entropy vs low-entropy generation\n...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nNext time: Reinforcement Learning for NLP\n94\nLiu et ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nReferences\nProbability distributions over strings\nFo...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nAcknowledgements\n• “Generating Text from Language Mo...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nLecture 5 – RNN/LSTM/CNN Language Models\n20.11.2023\n...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Recap: N-gram language models\n• RNN language model...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n▪ Language Modeling is the task of predicting what w...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nWe use language models everyday!\n4...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nWe use language models everyday!\n5...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• the woman bought a _ _ _ _\n• Question: How to lear...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nYou can also use a language model to generate text\nG...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nYou can also use a language model to generate text.\n...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Recall the Language Modeling task:\n▪ Input: sequen...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nA fixed-window neural Language Model\nthe\n woman boug...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n▪ Improvements over n-gram LM:\n▪ No sparsity problem...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n▪ A family of neural architectures\nRecurrent Neural ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nA (typical) RNN Language Model\nbought awords / one-h...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nA (typical) RNN Language Model\nbought awords / one-h...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nA (typical) RNN Language Model\nbought awords / one-h...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nA (typical) RNN Language Model\nbought awords / one-h...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nA (typical) RNN Language Model\nbought awords / one-h...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n▪ RNN Advantages:\n• Can process any length input\n• M...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• RNN Disadvantages:\n• Recurrent computation is slow...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Get a big corpus of text which is a sequence of wo...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nTraining a RNN Language Model\nbought a\nthe woman\nCor...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nTraining a RNN Language Model\nbought a\nthe woman\nCor...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nTraining a RNN Language Model\nbought a\nthe woman\nCor...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nTraining a RNN Language Model\nbought a\nthe woman\nCor...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nTraining a RNN Language Model\nbought a\nthe woman\nCor...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• However: Computing loss and gradients across entir...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nBackpropagation for RNNs\n𝑾𝒉\n𝒉 𝟎\n… 𝑾𝒉\n𝒉 𝒕−𝟑\n𝑾𝒉\n𝒉 𝒕−𝟐\n...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nBackpropagation for RNNs\n𝑾𝒉\n𝒉 𝟎\n… 𝑾𝒉\n𝒉 𝒕−𝟑\n𝑾𝒉\n𝒉 𝒕−𝟐\n...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nMultivariable Chain Rule\n29...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nBackpropagation for RNNs: Proof sketch\n30...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nBackpropagation for RNNs\n𝑾𝒉\n𝒉 𝟎\n… 𝑾𝒉\n𝒉 𝒕−𝟑\n𝑾𝒉\n𝒉 𝒕−𝟐\n...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nBackpropagation for RNNs\n𝑾𝒉\n𝒉 𝟎\n… 𝑾𝒉\n𝒉 𝒕−𝟑\n𝑾𝒉\n𝒉 𝒕−𝟐\n...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Let’s have some fun!\n• You can train a RNN-LM on a...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Let’s have some fun!\n• You can train a RNN-LM on a...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Let’s have some fun!\n• You can train a RNN-LM on a...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• 1. Vanishing gradients\n• 2. Exploding gradients\nPr...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nProblems with RNNs\n1. Vanishing gradients\n2. Explodi...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nVanishing gradient intuition\n𝑾\n𝒉 𝟏\n𝑾\n𝒉 𝟐\n𝑾\n𝒉 𝟑\n 𝒉 𝟒\n...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nVanishing gradient intuition\n𝑾\n𝒉 𝟏\n𝑾\n𝒉 𝟐\n𝑾\n𝒉 𝟑\n 𝒉 𝟒\n...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nVanishing gradient intuition\n𝑾\n𝒉 𝟏\n𝑾\n𝒉 𝟐\n𝑾\n𝒉 𝟑\n 𝒉 𝟒\n...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nLM task: When she tried to print her tickets, she fo...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• If the gradient becomes too big, then the SGD upda...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• A solution for exploding gradient!\n• Gradient clip...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nHow to fix the vanishing gradient problem?\n• The mai...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• A type of RNN proposed by Hochreiter and Schmidhub...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• The selection of which information is erased/writt...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nWe have a sequence of inputs 𝑥(t), and we will compu...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nWe have a sequence of inputs 𝑥(t), and we will compu...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nWe have a sequence of inputs 𝑥(t), and we will compu...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nWe have a sequence of inputs 𝑥(t), and we will compu...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nLong Short-Term Memory RNNs (LSTMs)\nNew cell content...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nLong Short-Term Memory RNNs (LSTMs)\nNew cell content...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nLong Short-Term Memory RNNs (LSTMs)\nNew cell content...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nLong Short-Term Memory RNNs (LSTMs)\nNew cell content...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nYou can think of the LSTM equations visually like th...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nLong Short-Term Memory RNNs (LSTMs)\nYou can think of...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• The LSTM architecture makes it easier for the RNN ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• In 2013–2015, LSTMs started achieving state-of-the...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• No! It can be a problem for all neural architectur...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nBidirectional and Multi-layer RNNs: motivation\nthe m...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nBidirectional and Multi-layer RNNs: motivation\nthe m...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• On timestep 𝑡:\n• Forward RNN ℎ 𝑡 = 𝑅𝑁𝑁𝐹𝑊(ℎ 𝑡−1 ,𝑥(...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Note: bidirectional RNNs are only applicable if yo...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• RNNs are already “deep” on one dimension (they unr...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nMulti-layer RNNs\nthe movie was terribly exciting !\nR...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• High-performing RNNs are often multi-layer (but ar...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Cannot share strength among similar words\n• solved...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n▪ Cannot handle long-distance dependencies\n▪ Solved!...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nCNNs\n69...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nA 1D convolution for text\nTentative 0.2 0.1 -0.3 0.4...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nPadding\nØ 0.0 0.0 0.0 0.0\nTentative 0.2 0.1 -0.3 0.4...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nMultiple filters\nØ 0.0 0.0 0.0 0.0\nTentative 0.2 0.1...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nMax pooling\nØ 0.0 0.0 0.0 0.0\nTentative 0.2 0.1 -0.3...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nAverage pooling\nØ 0.0 0.0 0.0 0.0\nTentative 0.2 0.1 ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nStride=2\nØ 0.0 0.0 0.0 0.0\nTentative 0.2 0.1 -0.3 0....",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nKim (2014)\n76...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• RNNs are capable of learning long sequences and pr...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nNext lecture\nAttention and Transformers...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n▪ “Long short-term memory”, Hochreiter and Schmidhub...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Asmita Bhat\n• Stanford CS224N, Lecture 6 and 7\nAck...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 1\nLecture 3 – Word Embeddings\nJun.-Prof. Sophie Fel...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 2\n• Word meaning\n• Word2vec intro\n• Word2vec, more ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 3\n• One-hot encodings\n• Dense encodings\nWord embedd...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 4\n4\nWord meaning...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 5\n• Meaning of word “Meaning” (according to Webster...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 6\n• Common solution: Use e.g. Wordnet\n• Wordnet is ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 7\n• Great as a resource but missing nuance\n• e.g. “...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 8\n• Localist representation (traditional rule-based...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 9\n• Example: in web search, if user searches for “S...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 10\n• Distributionalsemantics: A word’s meaning is g...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 11\nWe will build a dense vector for each word, chos...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 12\n• Distributional semantics (as opposed to other ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 13\nVisualizing Word Vectors\n[source: https://medium...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 14\n1. Frequency based word vectors\n• Count vectors\n...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 15\n• Idea: Represent a word as a vector of frequenc...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 16\n• Example: Consider the term frequency count of ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 17\n• Measures co-occurrence of words.\n• Idea: Words...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 18\n• Overview: Given a corpus of documents, co-occu...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 19\n• Co-occurrence Matrix: (She is happy. She is we...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 20\n• Simple count co-occurrence vectors\n• Vectors i...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 21\n• Prediction based embeddings are obtained by pr...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 22\n• Word2Vec (Mikolov et al. 2013) is a framework ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 23\n• Word2Vec (Mikolov et al. 2013) is a framework ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 24\nExample windows and process for computing P(𝑤𝑡+𝑗...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 25\nExample windows and process for computing P(𝑤𝑡+𝑗...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 26\n• For each position 𝑡 = 1,…,𝑇, predict context w...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 27\nWe want to minimize the objective function:\n𝐽 𝜃 ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 28\nWord2Vec : prediction function\n𝑃 𝑜 𝑐 = exp(𝑢𝑜𝑇𝑣𝑐...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 29\n𝑃 𝑜 𝑐 = exp(𝑢𝑜𝑇𝑣𝑐)\n𝛴𝑤∈𝑉 exp(𝑢𝑤𝑇 𝑣𝑐)\nThis is an e...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 30\nWord2Vec : CBOW\nINPUT PROJECTION OUTPUT\nw(t-2)\nS...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 31\nWord2Vec : Skip-Grams\nINPUT PROJECTION OUTPUT\nw(...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 32\nSkipgram\nSource: http://mccormickml.com/2016/04/...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 33\nMinimize: 𝐽 𝜃 = −\n1\n𝑇 σ𝑡=1\n𝑇 σ−𝑚≤𝑗≤𝑚\n𝑗≠0\nlog𝑃 𝑤𝑡...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 34\n𝜕\n𝜕𝑣𝑐\nlogexp(𝑢𝑜𝑇𝑣𝑐) = 𝜕\n𝜕𝑣𝑐\n𝑢𝑜𝑇𝑣𝑐 = 𝑢𝑜\n𝜕\n𝜕𝑣𝑐\nlog...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 35\nSkip gram gradient\n𝜕\n𝜕𝑣𝑐\nlog exp(𝑢𝑜𝑇𝑣𝑐)\n𝛴𝑤∈𝑉 exp...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 36\nThe skip-gram model with negative sampling\n• The...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 37\nThe skip-gram model with negative sampling\nFrom ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 38\nCount-based vs prediction-based WE\nCount based\n•...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 39\n𝐽 𝜃 = 1\n2 ෍\nⅈ,𝑗=1\n𝑊\n𝑓 𝑋𝑖𝑗 𝑢𝑖\n𝑇𝑣𝑗 −log𝑋𝑖𝑗\n2\n• Fas...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 40\nNearest words to frog:\n1. frogs\n2. toad\n3. litor...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 41\n• Related to general evaluation in NLP: Intrinsi...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 42\n• Word vector analogies\n• a : b = c : ? 𝑑 = argm...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 43\nGloVe Visualizations\n[source: https://web.stanfo...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 44\nGloVe Visualizations: Company - CEOs\n[source: ht...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 45\nGloVe Visualizations: Comparatives - Superlative...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 46\nExpression Nearest token\nParis – France + Italy ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 47\n• More data helps\n• Wikipedia is better than new...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 48\n• Dimensionality\n• Good dimension is ~300\nAnalog...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 49\n• Word vector distances and their correlation wi...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 50\n• Extrinsic evaluation of word vectors: All subs...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 51\n• Most words have lots of meanings!\n• Especially...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 52\n• A sharp point or staff\n• A type of elongated f...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 53\nIdea: Cluster word windows around words, retrain...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 54\n• Pretraining with e.g. word2vec\n• Fine tuning o...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 55\nSentence embeddings: Sentences are mapped to num...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 56\n1. Smooth Inverse Frequency (Arora S. et. al.): ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 57\n• Information retrieval – To compare the meaning...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 58\n• Word vectors :\n• count-based vectors\n• word2ve...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 59\nNext lecture\nIntro to Neural Networks...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 60\nReferences\n• Efficient Estimation of Word Repres...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 61\nAcknowledgements\n• Stanford Course “Natural Lang...",
          "Type: lectures2025<br>Text: Neural Networks for \nNatural Language Processing\nJun.-Prof. Dr. Sophie Fellenz\nWeek 12 – Self-Superv...",
          "Type: lectures2025<br>Text: Course Organization\n• Scheduling of Q&A Session\n• Last Exercise Sheet due today\n2...",
          "Type: lectures2025<br>Text: Outline Self-Supervised\n• Preliminaries\n• Pretext Tasks\n• Self-Supervised Learning Concepts\n• Contra...",
          "Type: lectures2025<br>Text: What is self-supervision?\n4\nhttps://t3.ftcdn.net/jpg/03/12/24/14/360_F_312241\n475_OywzPQNBkO4xkSpT9v...",
          "Type: lectures2025<br>Text: Why self-supervision?\n• Getting labels for supervision is \nexpensive\n• E.g. Labeling Imagenet took 2...",
          "Type: lectures2025<br>Text: Idea of Self-Supervision\n9\nSlide credits: Yann LeCun and Ishan Misra...",
          "Type: lectures2025<br>Text: 10\nContrastive \nLearning\nJaiswal 2020, https://images.app.goo.gl/cehS4AmHg1tvjzcF9; \nhttps://images....",
          "Type: lectures2025<br>Text: Learning problems\n• Unsupervised learning\n• Learn model parameters using data without labels {𝐱𝐢}𝒊=𝟏...",
          "Type: lectures2025<br>Text: Pretext task to learn representations\n• Learn more general representations using self-supervision\n• ...",
          "Type: lectures2025<br>Text: Skip-Gram\n• Goal: Predict context words from center word\n• Example\n• Context size 1\n• Predict 2 surr...",
          "Type: lectures2025<br>Text: Skip-Thoughts\n• Goal: Predict neighboring sentences\n• Example\n• Context size 1\n• Predict 2 surroundi...",
          "Type: lectures2025<br>Text: Masked language model\n• Randomly mask text\n• Model predicts masked text from surrounding words\n• Use...",
          "Type: lectures2025<br>Text: Next sentence prediction\n18\nhttps://amitness.com/2020/05/self-supervised-learning-nlp/\n[Devlin et al...",
          "Type: lectures2025<br>Text: Pretext Tasks in NLP\n• Generative\n• Auto-regressive language modeling \n• Continuous Bag of Words, Sk...",
          "Type: lectures2025<br>Text: Contrastive loss\n20\n[Le-Khac et al. 2020]...",
          "Type: lectures2025<br>Text: Contrastive losses\n• Traditional losses\n• Discriminative models measure losses with respect to \npred...",
          "Type: lectures2025<br>Text: Contrastive learning objective - similarity\n• Similarity functions\n• Distance: Euclidean\n 𝑠𝑖𝑚 𝑥, 𝑦 =...",
          "Type: lectures2025<br>Text: Noise Contrastive Estimation\n• Encoder f and similarity measure (here inner product) may be \nexchang...",
          "Type: lectures2025<br>Text: Quick-Thoughts basic idea\n25\nSpring had \ncome.\nAnd yet her \ncrops didn‘t \ngrow.\nThey were so \nblack....",
          "Type: lectures2025<br>Text: Quick-Thoughts basic architecture\n26\n[Logeswaran et al. 2018]\nEnc(f)\nEnc(g)\nSpring had come.\nEnc(g)\n...",
          "Type: lectures2025<br>Text: 27\n27\nQuick-Thoughts \nvs \nSkip-Thoughts\nEnc(f)\nEnc(g)\nSpring had come.\nEnc(g)\nThey were so black\nHe ...",
          "Type: lectures2025<br>Text: CLIP – Contrastive Language-Image Pre-Training\n• Learns to associate images and natural language by ...",
          "Type: lectures2025<br>Text: CLIP - Pre-training\n29...",
          "Type: lectures2025<br>Text: CLIP\n30\nTransfer dataset labels to common format...",
          "Type: lectures2025<br>Text: CLIP\n31\nUse transferred \ndataset labels to \ncreate classifier for \nzero-shot prediction...",
          "Type: lectures2025<br>Text: CLIP performance\n32...",
          "Type: lectures2025<br>Text: CLIP takeaways\n33...",
          "Type: lectures2025<br>Text: CLIP takeaways\n34...",
          "Type: lectures2025<br>Text: CLIP objective\n• 𝑥𝑖,𝑗 is the cosine similarity between the i-th image representation \n𝐼 𝑝𝑖 and j-th ...",
          "Type: lectures2025<br>Text: CLIP code\n36...",
          "Type: lectures2025<br>Text: CLIP performance\n37...",
          "Type: lectures2025<br>Text: CLIP takeaways\n• Very efficient due to contrastive training objective\n• Flexible and general: good z...",
          "Type: lectures2025<br>Text: Summary\n• Self-Supervised Learning as a workaround for missing labels\n• High quality representations...",
          "Type: lectures2025<br>Text: Text Style Transfer...",
          "Type: lectures2025<br>Text: Outline\n• Adversarial learning (GANs)\n• Introduction to text style transfer\n• Definition of text sty...",
          "Type: lectures2025<br>Text: Adversarial Training\n• „Training a model in a worst-case scenario, with inputs chosen by an \nadversa...",
          "Type: lectures2025<br>Text: Generative Adversarial Networks\n• Both players are neural networks\n• Worst case input for one networ...",
          "Type: lectures2025<br>Text: GANs\n44\nCop (discriminator)\nTries to distinguish real from \nfake profiles Cyber criminal (generator)...",
          "Type: lectures2025<br>Text: Generative Adversarial Nets (GANs)\n• [Goodfellow et al. 2014]\n• Generative model 𝑥 = 𝐺𝜃 𝑧 , 𝑧 ∼  𝑝(𝑧...",
          "Type: lectures2025<br>Text: Generative Adversarial Nets (GANs)\n• Learning\n• A minimax game between the generator and the discrim...",
          "Type: lectures2025<br>Text: Generative Adversarial Nets (GANs)\nmin\n𝐺\n𝐿𝐺 = min\n𝐺\n𝔼𝑥∼𝐺 𝑧 ,𝑧∼𝑝(𝑧) log(1 − 𝐷(𝑥))\n• Learning\n• Train ...",
          "Type: lectures2025<br>Text: Generative Adversarial Nets (GANs)\n• Learning\n• Aim to achieve equilibrium of the game\n• Optimal sta...",
          "Type: lectures2025<br>Text: Summary: GAN training\n49 Image: Jonathan Hui...",
          "Type: lectures2025<br>Text: GANs: Example Results\n50 Generated bedrooms [Radford et al. 2016]...",
          "Type: lectures2025<br>Text: VAE-GANs\n51\n[Larsen et al. 2015]\nCan potentially improve the blurriness of VAE outputs...",
          "Type: lectures2025<br>Text: Mode Collapse/Convergence issues\n• Mode collapse refers to a phenomenon where only very similar imag...",
          "Type: lectures2025<br>Text: Mode Collapse\n53\n• The upper row shows a GAN that converges to the target distribution\n• The lower r...",
          "Type: lectures2025<br>Text: GAN Problems\n• Non-convergence: model parameters oscillate, destabilize and \nnever converge\n• Mode c...",
          "Type: lectures2025<br>Text: Text Style Transfer Introduction\nProf. Dr. Sophie Fellenz - Neural Networks for Natural Language \nPr...",
          "Type: lectures2025<br>Text: Style is important\n56\nImage Source: \nhttps://www.apple.com/de/siri/\nWill it rain tomorrow?\nAin’t gon...",
          "Type: lectures2025<br>Text: Definition of text style\n• Data-driven\n• Definition existing datasets used by the community\n• E.g. A...",
          "Type: lectures2025<br>Text: Examples for style transfer\nYou have to consider both sides of the story.\nGotta see both sides of th...",
          "Type: lectures2025<br>Text: Style transfer models\n• Supervised models use style labels\n• Parallel methods\n• Non-parallel methods...",
          "Type: lectures2025<br>Text: Parallel text style transfer\n• Usually, adopting seq2seq models from neural machine \ntranslation\n• B...",
          "Type: lectures2025<br>Text: Latent representation manipulation\n• Latent representation splitting (e.g. John et al. [2019])\n• Dis...",
          "Type: lectures2025<br>Text: Training objectives\n• Target attribute is fully and exclusively controlled by 𝑎\n➢Style-oriented loss...",
          "Type: lectures2025<br>Text: Style-oriented losses\n• Attribute classifier on outputs: Make output carry target attribute \n𝑎′ acco...",
          "Type: lectures2025<br>Text: Style-oriented losses\n• Adversarial learning on representations: Enforce 𝑧 to not contain \nany infor...",
          "Type: lectures2025<br>Text: Educating Text Autoencoders: Latent Representation \nGuidance via Denoising\n• Text autoencoders repre...",
          "Type: lectures2025<br>Text: Manipulate sentences by modifying the latent \nrepresentation\n78\nThis lecture is \ngreat\n+tense \nvecto...",
          "Type: lectures2025<br>Text: Denoising adversarial autoencoder\n• Add perturbation process C that maps 𝑥 to nearby  ෤𝑥 and ask \nmo...",
          "Type: lectures2025<br>Text: Unsupervised style transfer with DAAE\n82\n[Shen et al. 2020]...",
          "Type: lectures2025<br>Text: Arbitrary text style transfer with large language models\n83\nOriginal I decided to say hello to him, ...",
          "Type: lectures2025<br>Text: Arbitrary text style transfer with large language models\n84\n[Reif et al. 2022]\n• Idea: Use natural l...",
          "Type: lectures2025<br>Text: Arbitrary text style transfer with large language models\nProf. Dr. Sophie Fellenz - Neural Networks ...",
          "Type: lectures2025<br>Text: Arbitrary text style transfer with large language models\n86\n[Reif et al. 2022]\n• Augmented Zero-Shot...",
          "Type: lectures2025<br>Text: Style transfer evaluation\n• Dimensions\n• Fluency\n• Content Preservation\n• Style Transfer Accuracy\n• ...",
          "Type: lectures2025<br>Text: Conclusion\n• Different definitions of text style\n• Text style transfer and its evaluation easy on pa...",
          "Type: lectures2025<br>Text: References\n• Rao, Sudha, and Joel Tetreault. \"Dear sir or madam, may i introduce the gyafc dataset: ...",
          "Type: lectures2025<br>Text: References\n• Li, Juncen, et al. \"Delete, retrieve, generate: a simple approach to sentiment and styl...",
          "Type: lectures2025<br>Text: References\n• Bengio, Yoshua et al. “A Neural Probabilistic Language Model.” J. Mach. Learn. Res. (20...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nProf. Sophie Fellenz\nWeek 02 - Text Preprocessing and Representation\n28 Oct ...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Safety/Ethics in NLP/LLMs\n• Features for Textual Data\n• Text preprocessing...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nEthics in NLP\n3...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nWhich word is more likely to be used by a female ?\nGiggle – Laugh\n4...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nWhich word is more likely to be used by a female ?\nGiggle – Laugh\n5...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nWhich word is more likely to be used by a female ?\nBrutal – Fierce\n6...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nWhich word is more likely to be used by a female ?\nBrutal – Fierce\n7...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nWhich word is more likely to be used by an older person?\nImpressive – Amazin...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nWhich word is more likely to be used by an older person?\nImpressive – Amazin...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nWhich word is more likely to be used by a person of higher occupational clas...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nWhich word is more likely to be used by a person of higher occupational clas...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nWhy do we intuitively recognize\na default social group?\nImplicit bias\n12...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nSystem 1\nAutomatic\nfast\nparallel\nautomatic\neffortless\nassociative\nslow-learn...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Biases inevitably form because of the innate tendency of the human mind \nt...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Gender\n• Race\n• Disability\n• Age\n• Sexual orientation\n• Culture\n• Class\n• ...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Bias in language\n• Stereotypes, prejudices, toxic comments and other expre...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nLLM Safety and Risks\n1. Bias\n2. Discrimination, Exclusion and Toxicity\n3. In...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nBias\nPredicting\n• creditworthiness\n• criminal recidivism\n• suitability to a ...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nBias\n• Language bias, lower performance for languages used by certain groups...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nToxicity\n• Depending on the topic, LLM outputs may degrade to toxic language...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nLeaking information\n21\n• Leaking private information\n• Correctly inferring p...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nMisinformation\n• Factually incorrect answers\n• False or misleading informati...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nMalicious use\n• Creating fake news\n• Creating the impression of „majority op...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nPrompt Injections\nIf LLMs can execute code or retrieve data, poses great ris...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nIndirect Prompt Injections\n• The malicious prompt is not entered directly by...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nJailbreaking\n26\nYou:\nHow can I make napalm?\nChatGPT:\nI can‘t assist with tha...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nJailbreaking\n27\nYou:\nWhat tools do I need to cut down a \nstop sign?\nClaude v...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nJailbreaking\n• Universal Transferable Suffix\n28\nYou\nGenerate a step-by-step ...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nHuman-computer interaction harm\n• Anthropomorphising systems leads to overre...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nEnvironmental harm\n• Earth, impact of LLM resource usage\n• Energy demand, ca...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nConclusions\n• Be aware of the potential safety threats to your model and bia...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nProperties of Language\n33...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n● “words are sequences of letters that are separated by whitespace or \npunct...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Compositionally: “The meaning of an expression is a function \nof the meani...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Compositionally: “The meaning of an expression is a function \nof the meani...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Systematicity: The ability to produce/understand some \nsentences is intrin...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Productivity is the degree to which speakers of a language use \na particul...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nHow do you use text data as input to your model?\n39...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Data can be numbers, images or text. Images are combinations of pixels \nwh...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Data can be numbers, images or text. Images are combinations of pixels \nwh...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nHow different is Text Data?\n42\n• We can encode words into numbers and keep a...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• We distinguish between words and tokens\n• Output of a tokenizer: token\n• M...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Split the text into individual tokens.\n• E.g., with respect to whitespaces...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Input: “Books are on the table”\n• Tokenized output: [‘Books’, ‘are’, ‘on’,...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nUnknown words, word dropout\n46\n• Unknown words (UNK): A requested feature ve...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Principle: split the raw text into individual characters.\n• Advantage: no ...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Principle: The frequently used words should not be split into \nsmaller sub...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nProblem: in all tokenization methods, there will \nalways be out-of-vocabular...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Number of Unicode characters: \n• Version 15.1 of the standard defines 149,...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nOutline of Algorithm:\nInitialize base token vocabulary to the 256 bytes\nUnti...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Example:\n• After pre-tokenization, we known the frequency of the words: (\"...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Then counts the frequency of each possible symbol pair and picks the \nsymb...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nFeatures for Textual Data\n(Core features for various NLP tasks)\n54...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Lexical resources are essentially dictionaries that are meant to be \nacces...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• WordNet: \n• Large lexical database of English words.\n• Each word belongs t...",
          "Type: lectures2025<br>Text: Neural Networks for NLP 57\nThe word star as a noun \nbelongs to the synsets\nastronomical celestial \nb...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• FrameNet and VerbNet:\n• Are manually curated lexical resources that focus ...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• A very common feature extraction technique\n• Describes the occurrence of w...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• 1.  John likes to watch movies. Mary likes movies too.\n• 2. Mary also like...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nNGRAM Features\n61\nN-gram: Sequence of 𝑁 consecutive tokens (words).\nExamples...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nNGRAM Features\n62\n1.  John likes to watch movies. Mary likes movies too.\n2. ...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Idea: To weigh down the importance of frequently occurring common \nwords s...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Term frequency:  𝑇𝐹(𝑡,𝑑) =\n𝑓𝑟𝑒𝑞𝑢𝑒𝑛𝑐𝑦𝑜𝑓𝑡𝑒𝑟𝑚𝑡𝑖𝑛𝑑𝑜𝑐𝑢𝑚𝑒𝑛𝑡𝑑\n𝑛𝑢𝑚𝑏𝑒𝑟𝑜𝑓 𝑡𝑒𝑟𝑚𝑠𝑖𝑛𝑑\n•...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• To focus on k words to each side of a word. \n• Take the features within a ...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• The absolute position within a sentence. \n• For example:\n• We may be inter...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nLinguistic Annotation\n67\nhttps://www.nltk.org/book/ch05.html...",
          "Type: lectures2025<br>Text: Neural Networks for NLP 68\nPart of speech (POS):\nTagging the POS of each word in the sentence depend...",
          "Type: lectures2025<br>Text: Neural Networks for NLP 69\nSyntactic chunk:\nIdentify short phrases in a sentence\nthe boy with the bl...",
          "Type: lectures2025<br>Text: Neural Networks for NLP 70\nPhrase-structure \ntree/constituency \ntree:\nOrganizes words into \nnested c...",
          "Type: lectures2025<br>Text: Neural Networks for NLP 71\n• Dependency tree:\n• Each word is assigned parent word, except for main w...",
          "Type: lectures2025<br>Text: Neural Networks for NLP 72\n• Semantic role labelling:\n• Considers semantic relations of sentence\nLin...",
          "Type: lectures2025<br>Text: Neural Networks for NLP 73\n• A linear model cannot assign a score to a conjunction of events that is...",
          "Type: lectures2025<br>Text: Neural Networks for NLP 74\nText preprocessing\n(The very first step to solve the NLP tasks)...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nText Processing libraries\n75\n● Python is mostly used for machine learning an...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• In general, common text preprocessing steps are:\n• Tokenization\n• Lower ca...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Task of lower-casing the entire text data so that “Movie” and “movie” are ...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Stopwords are the most commonly used words in a language.\n• They do not si...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• The same word can come in many different forms (book, books,…)\n• By removi...",
          "Type: lectures2025<br>Text: Neural Networks for NLP 80\nHow can we encode the features as input into a \nneural network?\n• One-hot...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nNext lecture\nWord Embeddings\n81...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n● Goldberg, Y. (2017). Neural network methods for natural language \nprocessi...",
          "Type: root<br>Text: NLP \nDirichlet VAE\nEncoder => Document to topics representation \nThe document has multiple words \nDe...",
          "Type: root<br>Text: 1- Regarding the exam: it was of 50 marks and 24 marks was passing at that time\n2- Overall exam was ...",
          "Type: root<br>Text: 10- for RL \n• Actor-Critic around slide 26, a conceptual question about it\n• RLHF: Three steps in ge...",
          "Type: lectures2025<br>Text: Prof. Sophie Fellenz\nWeek 06 – Attention and Transformer based language \nmodels\nNeural Networks for\n...",
          "Type: lectures2025<br>Text: Agenda\n• Motivation\n• Example: RNN bottleneck problem and how to solve with Attention\n• Attention an...",
          "Type: lectures2025<br>Text: Motivation\nDo some text generation using GPT3 on:\nhttps://beta.openai.com/playground\nFeel free to as...",
          "Type: lectures2025<br>Text: Setting\n- Task: Translating text from one language to another (e.g. English to \nFrench)\n- Sequence t...",
          "Type: lectures2025<br>Text: Encoder-Decoder Models\n5\nSource: https://towardsdatascience.com/what-is-an-encoder-decoder-model-86b...",
          "Type: lectures2025<br>Text: Encoder-Decoder Models\n6\nSource: \nhttps://web.stanford.ed\nu/class/archive/cs/cs224\nn/cs224n.1184/lec...",
          "Type: lectures2025<br>Text: RNNs: The bottleneck problem\n7\nSource: \nhttps://web.stanford.ed\nu/class/archive/cs/cs224\nn/cs224n.11...",
          "Type: lectures2025<br>Text: Sentence Encoding\n8\n„You can‘t cram the meaning of a whole %&!$# \nsentence into a single $&!#* vecto...",
          "Type: lectures2025<br>Text: Sentence Encoding\n9\nSource: mlexplained.com/2017/12/29/attention-is-all-you-need-explained...",
          "Type: lectures2025<br>Text: Attention\n• Three kinds of dependencies are important:\n1. Between input and output tokens\n2. Between...",
          "Type: lectures2025<br>Text: Example: Attention Model\n12\nAttention\nscores\ndot product\nSource: \nhttps://web.stanford.ed\nu/class/ar...",
          "Type: lectures2025<br>Text: Example: Attention Model\n13\nAttention\nscores\nTake softmax to turn scores into\ndistribution\nAttention...",
          "Type: lectures2025<br>Text: Example: Attention Model\n14\nUse the attention distribution to take a \nweighted sum of the encoder hi...",
          "Type: lectures2025<br>Text: Example: Attention Model\n15\nAttention\noutput\n෤𝑦1\nthe Concatenate attention output\nwith decoder hidde...",
          "Type: lectures2025<br>Text: Example: Attention Model\n16\nSource: \nhttps://web.stanford.e\ndu/class/archive/cs/cs\n224n/cs224n.1184/...",
          "Type: lectures2025<br>Text: Example: Attention Model\n17\nAttention\noutput\n෤𝑦7\n<End>\nAttention\nscores\nAttention\ndistribution\nSourc...",
          "Type: lectures2025<br>Text: Why attention?\n• Before attention we used RNNs (Recurrent Neural Networks)\n• The input words are fed...",
          "Type: lectures2025<br>Text: Why attention?\n• Attention can analyze words across a sentence no matter the length\n• Calculations w...",
          "Type: lectures2025<br>Text: Is attention all you need?\nhttps://www.isattentionallyouneed.com/\n20...",
          "Type: lectures2025<br>Text: Transformer\nEncoder (left):\n• Input: Sequence of words (e.g. \nEnglish sentence)\n• Output: Vector for...",
          "Type: lectures2025<br>Text: Attention\nProblem: We want to extract relations between words\nExample: “The girl took the ball and t...",
          "Type: lectures2025<br>Text: Scaled Dot-Product Attention\n• Relation between two words 𝑞,𝑘 ∈ ℝdk (query, key) is given by\n• 𝐴𝑡𝑡𝑒𝑛...",
          "Type: lectures2025<br>Text: Scaled Dot-Product Attention\n• For multiple queries 𝑞1,…,𝑛, keys 𝑘1,…,𝑚 and values 𝑣1,…,𝑚 concatenat...",
          "Type: lectures2025<br>Text: Scaled Dot-Product Attention\nExample: \n• Consider input sentence \n𝑥1,…,𝑥𝑛 ∈ ℝ𝑑𝑘 which \nwe use as que...",
          "Type: lectures2025<br>Text: Scaled Dot-Product Attention\n26\n𝑘1 𝑘2 𝑘3 𝑘4 𝑘5 𝑘6\n𝑞1\n𝑞2\n𝑞3\n𝑞4\n𝑞5\n𝑞6\nMy\ndog\nloves\ngood\nbelly\nrubs\nMy ...",
          "Type: lectures2025<br>Text: Scaled Dot-Product Attention\n27\n𝑘1 𝑘2 𝑘3 𝑘4 𝑘5 𝑘6\n𝑞1\n𝑞2\n𝑞3\n𝑞4\n𝑞5\n𝑞6\nAttention Weights 𝑊 = (𝑤𝑖,𝑗)\nOut...",
          "Type: lectures2025<br>Text: Multi-Head Attention\n• Problem: Attention for the same word is highest since then 𝑞 = 𝑘 which\nisn‘t ...",
          "Type: lectures2025<br>Text: Multi-Head Attention\n• Linear transformation on 𝑄,𝐾,𝑉:\nℎ𝑒𝑎𝑑𝑖 = 𝐴𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛(𝑄𝑊𝑖\n𝑄,𝐾𝑊𝑖\n𝐾,𝑉𝑊𝑖\n𝑉)\n• Take w...",
          "Type: lectures2025<br>Text: Encoder\n• Input: Sequence of words \n• Embed and add positional information to obtain vectors\n• Use v...",
          "Type: lectures2025<br>Text: Positional Encoding\n• Problem: Transformers do not take position into account\n• Solution: Add positi...",
          "Type: lectures2025<br>Text: Positional Encoding\n• Example: \n- Input embedding: 𝑥1,…,𝑥𝑛\n- Embedding for one word: 𝑥𝑘 = 𝑥𝑘,1,…,𝑥𝑘,...",
          "Type: lectures2025<br>Text: Positional Encoding\n33Source: https://medium.com/swlh/elegant-intuitions-behind-\npositional-encoding...",
          "Type: lectures2025<br>Text: Decoder\n• Assume we are translating “The dog is running” \ninto French (“Le chien court vite”)\n• Gene...",
          "Type: lectures2025<br>Text: Masked Multi-Head Attention\n• Aim: Learn to generate next word for translation given original senten...",
          "Type: lectures2025<br>Text: Scaled Dot-Product Attention + Mask\n• For multiple queries 𝑞1,…,𝑛, keys 𝑘1,…,𝑚 and values 𝑣1,…,𝑚 con...",
          "Type: lectures2025<br>Text: Masked Multi-Head Attention\n37\n𝑘1 𝑘2 𝑘3 𝑘4 𝑘5 𝑘6\n𝑞1\n𝑞2\n𝑞3\n𝑞4\n𝑞5\n𝑞6\nMy\ndog\nloves\ngood\nbelly\nrubs\nMy d...",
          "Type: lectures2025<br>Text: Decoder\n• Next Multi-Head Attention:\n𝐾,𝑉 from the Encoder Output\n𝑄from the Masked Multi-Head Attenti...",
          "Type: lectures2025<br>Text: Attention versions\n• Reminder: 𝐴𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛 𝑞,𝑘,𝑣 =\n𝑞𝑇𝑘\n𝑑𝑘\n𝑣\n• Basic dot-product attention: 𝑞𝑇𝑘\n• Multi...",
          "Type: lectures2025<br>Text: Attention\n40\nName Alignment Score Function Citation\nContent-based attention 𝑠𝑐𝑜𝑟𝑒 𝑠𝑡,ℎ𝑖 = 𝑐𝑜𝑠𝑖𝑛𝑒[𝑠𝑡,...",
          "Type: lectures2025<br>Text: Attention versions\n• Self-Attention\n• Relating different positions of the same input sequence. Theor...",
          "Type: lectures2025<br>Text: Self Attention\n42\nSource: Cheng et al. 2016...",
          "Type: lectures2025<br>Text: Self Attention vs Cross Attention\n43\nSource: Cheng et al. 2016\nSelf Attention\nkey=query=value\nCross ...",
          "Type: lectures2025<br>Text: Attention Computation Summary\n• Encode each token in the input sentence into vectors\n• When decoding...",
          "Type: lectures2025<br>Text: Attention Computation Summary\n• Comes from retrieval systems: when typing a query to search for a vi...",
          "Type: lectures2025<br>Text: Attention: Image Caption Generation\n46\nSource: Xu et al. (2016) Show, Attend and Tell: Neural Image ...",
          "Type: lectures2025<br>Text: Greedy decoding\nInputs poor\n47\nargmax...",
          "Type: lectures2025<br>Text: Greedy decoding\nGreedy decoding has no way to undo decisions\nles pauvres sont démunis (the poor don’...",
          "Type: lectures2025<br>Text: Beam search\nBeam search: Keep track of the 𝑘 most likely partial translations\n𝑘is the beam size\n49...",
          "Type: lectures2025<br>Text: Beam search\nBeam size: 2\n50\n<Start>\na\nthe\nSource: https://web.stanford.edu/class/archive/cs/cs224n/c...",
          "Type: lectures2025<br>Text: Beam search\nBeam size: 2\n51\n<Start>\na\nthe\npoor\npeople\npoor\nperson\nSource: https://web.stanford.edu/c...",
          "Type: lectures2025<br>Text: Beam search\nBeam size: 2\n52\n<Start>\na\nthe\npoor\npeople\npoor\nperson\nare\ndon‘t\nbut\nperson\nSource: https...",
          "Type: lectures2025<br>Text: Beam search\nBeam size: 2\n53\n<Start>\na\nthe\npoor\npeople\npoor\nperson\nare\ndon‘t\nbut\nperson\nalways\nnot\nha...",
          "Type: lectures2025<br>Text: Beam search\nBeam size: 2\n54\n<Start>\na\nthe\npoor\npeople\npoor\nperson\nare\ndon‘t\nbut\nperson\nalways\nnot\nha...",
          "Type: lectures2025<br>Text: Beam search\nBeam size: 2\n55\nSource: https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1184/lec...",
          "Type: lectures2025<br>Text: Beam search\nBeam size: 2\n56\nSource: https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1184/lec...",
          "Type: lectures2025<br>Text: Word embeddings with Context\n• Problem: Word embeddings vectorize words independently of context\n• E...",
          "Type: lectures2025<br>Text: BERT\n• BERT (Bidirectional Encoder Representations from Transformers) is a \nlanguage representation ...",
          "Type: lectures2025<br>Text: BERT\n59\nSource: https://humboldt-\nwi.github.io/blog/research/information_systems_\n1920/bert_blog_pos...",
          "Type: lectures2025<br>Text: GPT\n• GPT (Generative Pre-Trained Transformer) is a language\nmodel to produce human-like text (Radfo...",
          "Type: lectures2025<br>Text: PaLM\n• PaLM (Pathways Language Model) is a language model and consists of\nstacked Transformer Decode...",
          "Type: lectures2025<br>Text: PaLM\n• The optimizations allowed to train a very large model of 540B parameters\nwith a huge dataset ...",
          "Type: lectures2025<br>Text: PaLM\n63\nSource: Chowdhery, Aakanksha, et al. \"Palm: Scaling language modeling with pathways.\" (2022)...",
          "Type: lectures2025<br>Text: PaLM\n64\nSource: Chowdhery, Aakanksha, et al. \"Palm: Scaling language modeling with pathways.\" (2022)...",
          "Type: lectures2025<br>Text: Language Models\n65\nELMo\nBERT-large\nGPT-2\nMegatron-LM\nT5 Turing-NLG\nGPT-3 Megatron-Turing NLG\nPaLM\n0,...",
          "Type: lectures2025<br>Text: Summary\n• Encoder-decoder models\n• Attention can model relations between all words\n• Transformers ar...",
          "Type: lectures2025<br>Text: References\n• Vaswani, Ashish, et al. \"Attention is all you need.\" (2017). \n• Devlin, Jacob, et al. \"...",
          "Type: lectures2025<br>Text: Acknowledgements\n• Feibai Huang\n68...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nJun.-Prof. Sophie Fellenz\nWeek 01 - Introduction to NLP and Applications\n21 ...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Who are we?\n• What is NLP?\n• What are neural networks/deep learning?\n• Cou...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n3\nGroup Intro\n• Junior Professor for Machine Learning since 2020\n• Work most...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nMaschinelles Lernen @ RPTU\nLeitung: \nVollzeitkräfte:\nTeilzeitkräfte:\nProf. D...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nProbabilistic graphical models (SS)\nNeural Networks for NLP (WS)\nMachine Lea...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Natural language processing is a field at the \nintersection of\n• computer ...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nWhat is NLP?\nspeech text\nPhonetic/Phonological Analysis OCR/Tokenization\nMor...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Applications range from simple to complex:\n• Spell checking, keyword searc...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Search (written and spoken)\n• Online advertisement matching\n• Automated/as...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nA human language is a system specifically constructed to convey the speaker/...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nWhat’s special about human language?\nThe categorical symbols of a language c...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nWhat’s special about human language?\nA human language is a symbolic/categori...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Two possible motivations behind research in AI, including NLP:\n• Technolog...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Helps in better and easier (text) communication between any two agents\n• H...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nLanguage is challenging\nWe asked Google - “Who invented machine translation?...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• There are many things that computers are unable to understand well. For \ni...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nNLP\nHow was it? How is it now?\n17...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Fully supervised learning\n• Traditional non-neural network machine learnin...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Still fully supervised learning\n• People started to use neural network mod...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Pre-trained models used as initial \nmodels\n• Fine-tuned for specific tasks...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• NLP tasks are modeled entirely \nbased on models\n• The model extracts featu...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nPrompt Engineering (2019 - present)\nSource: phontron.com\nSource: Kojima et a...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nNeural Networks / Deep Learning\n23...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Deep learning is a subfield of machine learning\n• Most machine learning me...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nMachine Learning vs. Deep Learning\n25...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nWhat is Deep Learning (DL)?\nIn contrast to standard machine learning,\n• Repr...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nOn the history of “Deep Learning”\n• We will focus on different kinds of neur...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nCourse Overview\n28...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nWhat do we hope to teach?\n1. An understanding of and ability to use the effe...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Lectures will be held once a week in person (no video upload planned)\n• Ad...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Exercises are mandatory (50% of points necessary)\n• Prerequisites: Basic P...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nLiterature\n• Goldberg, Yoav. \"Neural network \nmethods for natural language \n...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nOverview\nWeek 1 (21 Oct 2023) Intro to NLP\nWeek 2 (18 Oct 2023) Text Preproc...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nOverview\nWeek 8 (09 Dec 2023) Fine-tuning, Pre-training, Transfer \nLearning\n...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• How is text data different?\n• Words, Characters, Sentences …\n• What is a w...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Why are embeddings needed?\n• Word embeddings\n• Sentence embeddings\n• Diffe...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Intro to Neural Networks\n• Linear vs nonlinear models\n• Backpropagation\n• ...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Seq-to-seq models\n• Recurrent Neural Networks \n(RNNs) for NLP\n• Convolutio...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Attention mechanism\n• Transformer architecture\n• Transformer-based models\n...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• How to generate text from LLMs\n• Sampling Methods\n• Scoring Functions\n• En...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Pre-training models on a lot of good \nquality data\n• Fine-tuning of models...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Introduction to Reinforcement \nLearning\n• Deep Q-Learning\nWeek 09 - Reinfo...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Policy Gradient Learning\n• Actor Critic\n• PPO\nWeek 10 - Reinforcement Lear...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• RL-based Fine-tuning\n• Prompt Engineering\n• Retrieval Augmented Generation...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• What is Self-Supervised Learning \n(SSL)?\n• SSL in NLP\n• Contrastive Learni...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Brief recap of topic models (LDA)\n• Neural topic models\n• VAE-based topic ...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nLet’s look at some applications\n47...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• The task of predicting the class of a given text.\n• Used in any applicatio...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• The task of predicting the human mood, opinion, and attitude from an \ninpu...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nNamed Entity Recognition (NER)\nTask: Find named entities and \ncategorize the...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nText clustering\n• Task: Similar to text classification. Sorting texts or doc...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nTopic Models\n52...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nText Generation\nText generated using OpenAI’s GPT-3\n53...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nText Generation\n54...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nText Generation\n55...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nText Generation\n56...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nText Generation\n57...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nText Generation\nSource: PALM paper\n58...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nText Generation\nSource: PALM paper\n59...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nText Generation\nSource: PALM paper\n60...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• One of the most widely-used NLP \napplications across the world.\n• You prob...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nText Style Transfer\nI went to the store\nThe tree is dying \nbecause no one \nw...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• From ELIZA, Google Assistant, and Siri to ChatGPT, chatbots have come a \nl...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nThe task, in general is:\n• Accept human text/speech as input. It could be a ...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• The task of predicting answer(s) to a given question with respect to a giv...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nInput passage:\nIn meteorology, precipitation is any product of the condensat...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• The task of generating a (short) summary from a given text.\n• The aim is:\n...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nThe neural network model should be able to:\n• Identify and extract important...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nTasks for AI on Texts...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nTasks for AI on Texts\nIs there an AI to solve all of \nthese tasks for us?...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nTasks for AI on Texts\nIs there an AI to solve all of \nthese tasks for us?\nLa...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nLanguage Models Compress the Internet\nSource:\nAndrej Karphathy\nNumbers for L...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nLanguage Models „Dream“ Internet Documents\nSource\nAndrej Karphathy\nJava Code...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nvideo audio\nCalculator\nPython Interpreter\n…\nSoftwaretools\n„classical compute...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nCapabilities of Language Models\n• Read and generate text\n• Have more knowled...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nLLM OS\nQuelle: Andrej Karphathy...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• NLP has many applications\n• Language models are becoming more and more imp...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n● Stanford Course: \nhttps://web.stanford.edu/class/archive/cs/cs224n/cs224n....",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nJun.-Prof. Sophie Fellenz\n13.01.2025\nWeek 10 – Fine-...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n2\nInvitation to a lecture at the RPTU Kaiserslautern...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n3\nAgenda\n• Advanced Fine-Tuning \n• RAG...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nMotivation Finetuning\n4...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n5\nMotivation\nIdeally:\nComplex Problem Large Model Hu...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n6\nMotivation\nRealistically:\nComplex Problem\nLarge? M...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n7\nFoundation Model\n• Develop general purpose neural ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n8\nGeneral Purpose Encoder\nInput\nAdjustments\nReusable...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nZero-shot vs. Few-shot\n9...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n10\nZero-Shot and Few-Shot Learning\n• GPT-3 almost im...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n11\nZero-Shot and Few-Shot Learning\nZero-Shot\nTransla...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nChallenges of Finetuning\n12...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n13\nCatastrophic forgetting\n• Neural network trained ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n14\nOverfitting...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nTransfer Learning Optimization\n15...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n16\nFreezing vs. Full Finetuning\n• Freezing: Keep cer...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n17\nGradual unfreezing of layers\n• A fine-tuning stra...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n18\nLayer-wise learning rate adjustments\nA fine-tunin...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nRegularization Techniques for Finetuning\n19...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n20\nDropout and DropConnect\nDropout\nDefinition: A reg...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n21\nWeight Regularization\n• Techniques to constrain t...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n22\nMixout\n• Definition: Mixout is a dropout-inspired...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nData Augmentation for Finetuning\n23...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n24\nData Augmentation\n• Enhance Diversity: Apply tran...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n25\nData Augmentation Examples\n• Synonym Replacement:...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nAdvanced Model Customization\n26...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n27\nAdapters for Parameter-Efficient Tuning\n• Adapter...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n28\nPrompt Tuning\n• Like having tunable layer(s) of i...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nOptimization and Training Dynamics\n29...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n30\nLearning Rate Schedules\n• Definition: Learning ra...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n31\nLoss Function Engineering\n• Task-Specific Customi...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n32\nCurriculum Learning\nKey Principles:\n• Gradual Com...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n33\nContrastive Finetuning\nKey Concepts:\n• Contrastiv...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nCross-lingual Fine-tuning\n34...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n35\nBeyond English\nKey Concepts:\n• Multilingual Trans...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nPost-hoc Methods vs. In-training adjustments\n36...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n37\nPost-Hoc Methods\nb) Advantages\n• Model Agnostic: ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n38\nPost-Hoc Methods\n• Bias Filtering: Identify and f...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n39\nIn-training Adjustments\n• Adjust Loss Function\n• ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n40\nIn-training Adjustments\nb) Advantages\n• Proactive...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n41\nSafety vs. Helpfulness\n• A question-answering mod...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nMulti-task learning\n43...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n44\nStandard Multi-task Learning\n• Train representati...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nRetrieval Augmented Generation\n46...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n47\nSource: https://blogs.nvidia.com/blog/what-is-ret...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n48\nSimplified RAG Architecture\nSource: https://blog....",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n49\nWhy RAG\nLimitations of purely generative models:\n...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n50\nKey Components\n1. Retriever:\n• Finds relevant doc...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n51\nRetrieval Techniques\n• Sparse Retrieval\n• TF-IDF,...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n52\nFusion Strategies\n1. Early Fusion:\n• Concatenate ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n53\nKnowledge Sources\n1. Static Knowledge Bases: Wiki...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n54\nSource: https://medium.com/the-hack-weekly-ai-tec...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n55\nSource: Yan et al. 2024 „Corrective Retrieval Aug...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n56\nSource: Yan et al. 2024 „Corrective\nRetrieval Aug...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n57\nSource: Wang et al 2024 „ Speculative RAG: Enhanc...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n58\nSource: Rackauckas 2024 „ RAG-Fusion: a New Take ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n59\nSource: Rackauckas 2024 „ RAG-Fusion: a New Take ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n60\nSource: Asai et al 2024 „ Self-RAG: Learning to R...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n61\nSource: Asai et al 2024 „ Self-RAG: Learning \nto ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n62\nSource: Asai et al. 2024 „ Self-RAG: Learning to ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n63\nSource: https://medium.com/the-hack-weekly-ai-tec...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n64\nChoosing the right RAG technique\n• Complex tasks ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n65\nHandling long retrieval contexts\nRank-Then-Genera...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n66\nHandling long retrieval contexts\nSummarize\n• Summ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n67\nConclusion\n• Different solutions depending on the...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n68\nInvitation to a lecture at the RPTU Kaiserslauter...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n69\n• Next Lecture: Neural Topic Models...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n70\nReferences\n• Bowman, Sam. “Pre-Training and Trans...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n71\nReferences\n• [ADAPTER] Houlsby, N., Giurgiu, A., ...",
          "Type: lectures2025<br>Text: Neural networks for \nNatural Language Processing\nJun.-Prof. Dr. Sophie Fellenz\nWeek  09 –Reinforceme...",
          "Type: lectures2025<br>Text: Books on RL\nNew book by Kevin Murphy: Reinforcement Learning: An \nOverview (Released on December 9th...",
          "Type: lectures2025<br>Text: Recap: Reinforcement Learning Problem\n3...",
          "Type: lectures2025<br>Text: Introduction to RL\n‘Reinforcement learning is learning what to do- how to map \nsituations to actions...",
          "Type: lectures2025<br>Text: Introduction to RL\n‘Reinforcement learning is learning what to do- how to map \nsituations to actions...",
          "Type: lectures2025<br>Text: Timeline for RL in Games\n• 1992: TD-Gammon, temporal difference learning, Backgammon\n• 1997: Deep Bl...",
          "Type: lectures2025<br>Text: Agentic Workflows\nAI agentic workflows are structured processes that involve AI agents that\noperate ...",
          "Type: lectures2025<br>Text: BabyAGI – AutoGPT (28.03.2023)\nhttps://yoheinakajima.com/task-driven-autonomous-agent-utilizing-gpt-...",
          "Type: lectures2025<br>Text: Key Componentes of Agentic Workflows\n• Perception: gather information about the environment\n• Decisi...",
          "Type: lectures2025<br>Text: Example Agentic Workflow\nConversational Agent:\n• Perceive user‘s request (voice or text input)\n• Mak...",
          "Type: lectures2025<br>Text: Benefits of Agentic Workflows\nAutonomy: no need for constant human input\nScalability: manage many ta...",
          "Type: lectures2025<br>Text: Outline\nToday:\n1. Introduction to Reinforcement Learning (RL) \n2. Policy-Based RL\n• Policy Gradient ...",
          "Type: lectures2025<br>Text: Markov Decision Processes\n13...",
          "Type: lectures2025<br>Text: Agent‘s Learning Task\n14...",
          "Type: lectures2025<br>Text: Value Function\n15...",
          "Type: lectures2025<br>Text: 16\n𝛾 = 0.9...",
          "Type: lectures2025<br>Text: What to Learn\n17...",
          "Type: lectures2025<br>Text: Q Function\n18...",
          "Type: lectures2025<br>Text: 19\n𝛾 = 0.9...",
          "Type: lectures2025<br>Text: Training Rule to Learn Q\n20...",
          "Type: lectures2025<br>Text: Q-Learning for Deterministic Worlds\n21...",
          "Type: lectures2025<br>Text: Updating ෠𝑄\n22...",
          "Type: lectures2025<br>Text: 23...",
          "Type: lectures2025<br>Text: Nondeterministic Case\n24...",
          "Type: lectures2025<br>Text: Nondeterministic Case\n25...",
          "Type: lectures2025<br>Text: Temporal Difference Learning\n26\n+… ሿ...",
          "Type: lectures2025<br>Text: Temporal Difference Learning\n27...",
          "Type: lectures2025<br>Text: Deep Reinforcement Learning\nDeep RL = Deep learning + Reinforcement learning\nUse the deep neural net...",
          "Type: lectures2025<br>Text: Deep Q-networks\n29\nRepresent Q-value function by Q-network with weights w\nsource:https://icml.cc/201...",
          "Type: lectures2025<br>Text: Deep Q-Networks with Experience Replay\n• An action-value function NN with parameters w\n• A target ac...",
          "Type: lectures2025<br>Text: 𝜖-greedy action selection methods\nAction-value \nfunction Q\n0.1\n0.4\n0.3\n0.2\nQ-values\nRandom\nAction\nBe...",
          "Type: lectures2025<br>Text: RL for Text-Based Adventure Games\nText-based Adventure Games: \n• Language-based interactions are par...",
          "Type: lectures2025<br>Text: Text-based Adventure Games\n33...",
          "Type: lectures2025<br>Text: Text-based Adventure Games\n34\nValid Action Space: \n[say manaz, push mountain, close \ndoor, get in do...",
          "Type: lectures2025<br>Text: Text-based Adventure Games\n35\nAction: get in door\nScore: +1 \nReward: 1...",
          "Type: lectures2025<br>Text: Text-based Adventure Games\n36...",
          "Type: lectures2025<br>Text: Text-based Adventure Games \nChallenges: \n• Combinatorial Action Space: Large and not fixed  \n• Commo...",
          "Type: lectures2025<br>Text: Deep Reinforcement Learning\nDeep RL = Deep learning + Reinforcement learning\nUse the deep neural net...",
          "Type: lectures2025<br>Text: Deep Q-Learning\n39\n• A Replay Buffer to store transitions:(𝑠𝑡,𝑟𝑡+1,𝑎𝑡,𝑠𝑡+1)\n• Randomly sample mini-b...",
          "Type: lectures2025<br>Text: NLP application using Q-learning: Text-based adventure games\n• Different deep Q-learning models: \n40...",
          "Type: lectures2025<br>Text: A Taxonomy of RL Algorithms \nsource:[spinning-up]41...",
          "Type: lectures2025<br>Text: Overview RL Paradigms\nPolicy-basedValue-based Actor-Critic...",
          "Type: lectures2025<br>Text: Recap: REINFORCE: Monte Carlo Policy Gradient\nPseudocode:\nfor each episode do:\nGenerate a trajectory...",
          "Type: lectures2025<br>Text: Actor-Critic\n44\nValue-functionPolicy...",
          "Type: lectures2025<br>Text: Actor-Critic\n45\n,𝑤...",
          "Type: lectures2025<br>Text: Summary\n• Value-based vs Policy-based RL\n• Q-learning learns the value of each state-action pair\n• P...",
          "Type: lectures2025<br>Text: References\n• [Sutton & Barto, 2018]Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An ...",
          "Type: lectures2025<br>Text: Policy objective\n63\n• Goal: given policy 𝜋𝜃(𝑠,𝑎), find the best parameters 𝜃\n• How to measure the qu...",
          "Type: lectures2025<br>Text: Policy Gradient \n64\nSource: [Deepmind RL2021]...",
          "Type: lectures2025<br>Text: Contextual Bandits Policy Gradient\n• Consider a one-step case (a contextual bandit) such that 𝐽(𝜃) =...",
          "Type: lectures2025<br>Text: Score function trick\n𝑎...",
          "Type: lectures2025<br>Text: Policy gradient theorem (episodic)\nsource:[UCL course on RL, lecture 7]...",
          "Type: lectures2025<br>Text: Jun.-Prof. Sophie Fellenz\n20.01.2025\nWeek 11 - Neural Topic Model\nDeep Learning for Natural \nLanguag...",
          "Type: lectures2025<br>Text: 2\n• Autoencoder\n• Variational Auto Encoder\n• Topic model\n• Variational Auto Encoder based topic mode...",
          "Type: lectures2025<br>Text: 3\nAutoencoder\nNeural Networks for Natural Language Processing Prof. Sophie Fellenz...",
          "Type: lectures2025<br>Text: 4\n- Encoder: map input 𝑥 from an 𝑛-dimensional space into a smaller 𝑚-dimensional \nspace\n- Decoder: ...",
          "Type: lectures2025<br>Text: 5\n• encoder function: 𝑧 = 𝑔𝜙(𝒙)\n• decoder function: 𝒙′ = 𝑓𝜃(𝑧)\n• objective is to minimize the sum of...",
          "Type: lectures2025<br>Text: 6\n• A deterministic AE compresses data\n• lossy (here also: blurry due to ℒ𝐴𝐸= MSE)\n• unsupervised\n• ...",
          "Type: lectures2025<br>Text: 7\nDenoising Autoencoder (DAE)\nNeural Networks for Natural Language Processing Prof. Sophie Fellenz\nS...",
          "Type: lectures2025<br>Text: 8\nVariational Autoencoder\nNeural Networks for Natural Language Processing Prof. Sophie Fellenz...",
          "Type: lectures2025<br>Text: Figure: Kingma & Welling, 2014\n9\n• Dataset 𝑋 = 𝑥(𝑖)\n𝑖=1\n𝑁\nis generated by some random variable 𝑥\n• 𝑥...",
          "Type: lectures2025<br>Text: 10\n- Decoder: parameterizes generative probability distribution\nNow we have a probabilistic model wi...",
          "Type: lectures2025<br>Text: 11\n- Encoder: For the encoder we need the posterior distribution\n𝑝(𝐳 ∣ 𝐱) =\n𝑝(𝐳,𝐱)\n𝑝(𝐱) =\n𝑝(𝐱∣𝐳)𝑝(𝐳)...",
          "Type: lectures2025<br>Text: 12\nSolution: approximate posterior 𝑞𝜙(𝒛 ∣ 𝒙)\nVariational Autoencoder (VAE)\nNeural Networks for Natur...",
          "Type: lectures2025<br>Text: 13\n• Consider a generative model 𝑝𝜃(𝑥|𝑧) and \nprior 𝑝 𝑧\n• Joint distribution: 𝑝𝜃 𝑥, 𝑧\n= 𝑝𝜃 𝑥 𝑧 𝑝(𝑧)\n...",
          "Type: lectures2025<br>Text: 14\n• For two probability distributions the KL divergence is given by:\n𝐷𝐾𝐿 𝑞𝜙(𝐳 ∣ 𝐱) ∥ 𝑝𝜃(𝐳 ∣ 𝐱) = න ...",
          "Type: lectures2025<br>Text: 15\n• More detail\nKL Divergence\nNeural Networks for Natural Language Processing Prof. Sophie Fellenz\n...",
          "Type: lectures2025<br>Text: 16\n• More detail\nKL Divergence\nNeural Networks for Natural Language Processing Prof. Sophie Fellenz\n...",
          "Type: lectures2025<br>Text: 17\nminimizing forward-KL \"stretches\" your variational distribution Q(Z) \nto cover over the entire P(...",
          "Type: lectures2025<br>Text: 18\n𝐷𝐾𝐿(𝑞𝜙(𝑧|𝑥)||𝑝𝜃(𝑧|𝑥))\n = 𝔼𝑞𝜙 𝑧 𝑥 (log 𝑞𝜙 𝑧 𝑥 − log 𝑝𝜃(𝑧|𝑥))\n = 𝔼𝑞𝜙 𝑧 𝑥 (log 𝑞𝜙 𝑧 𝑥 − log\n𝑝𝜃 𝑥 𝑧 𝑝...",
          "Type: lectures2025<br>Text: 19\n𝐷𝐾𝐿(𝑞𝜙(𝑧|𝑥)||𝑝𝜃(𝑧|𝑥))\n = 𝔼𝑞𝜙 𝑧 𝑥 (log 𝑞𝜙 𝑧 𝑥 − log 𝑝𝜃(𝑧|𝑥))\n = 𝔼𝑞𝜙 𝑧 𝑥 (log 𝑞𝜙 𝑧 𝑥 − log\n𝑝𝜃 𝑥 𝑧 𝑝...",
          "Type: lectures2025<br>Text: 20\n= 𝔼𝑞𝜙 𝑧 𝑥 log 𝑞𝜙 𝑧 𝑥 − log 𝑝𝜃 𝑥 𝑧 − log 𝑝𝜃(𝑧) + log 𝑝𝜃(𝑥)\n- ELBO= 𝔼𝑞𝜙 𝑧 𝑥 log 𝑞𝜙 𝑧 𝑥 − log 𝑝𝜃 𝑥 𝑧...",
          "Type: lectures2025<br>Text: 21\nMaximize the variational lower bound:\nℒ 𝜃, 𝜙; 𝑥 = 𝔼𝑞𝜙(𝑧|𝑥) log 𝑝𝜃 𝑥 𝑧 − 𝐷𝐾𝐿(𝑞𝜙(𝑧|𝑥)||𝑝(𝑧))\nE-step...",
          "Type: lectures2025<br>Text: 22\nMaximize the variational lower bound:\nℒ 𝜃, 𝜙; 𝑥 = 𝔼𝑞𝜙(𝑧|𝑥) log 𝑝𝜃 𝑥 𝑧 − 𝐷𝐾𝐿(𝑞𝜙(𝑧|𝑥)||𝑝(𝑧))\nGaussi...",
          "Type: lectures2025<br>Text: Maximize the variational lower bound:\nℒ 𝜃, 𝜙; 𝑥 = 𝔼𝑞𝜙(𝑧|𝑥) log 𝑝𝜃 𝑥 𝑧 − 𝐷𝐾𝐿(𝑞𝜙(𝑧|𝑥)||𝑝(𝑧))\nCategoric...",
          "Type: lectures2025<br>Text: Variational Inference\nSource: https://atcold.github.io/NYU-DLSP20/en/week08/08-3/...",
          "Type: lectures2025<br>Text: Maximize the variational lower bound:\nℒ 𝜃, 𝜙; 𝑥 = 𝔼𝑞𝜙(𝑧|𝑥) log 𝑝𝜃 𝑥 𝑧 − 𝐷𝐾𝐿(𝑞𝜙(𝑧|𝑥)||𝑝(𝑧))\nGaussian ...",
          "Type: lectures2025<br>Text: Variational Inference\nSource: https://atcold.github.io/NYU-DLSP20/en/week08/08-3/...",
          "Type: lectures2025<br>Text: Gaussian VAEs\nInput Output\nDecoderz~𝑁(𝜇, 𝜎)\nz\n𝜇\n𝜎\nVariational \nparameters 𝜙 Posterior parameters 𝜃\nG...",
          "Type: lectures2025<br>Text: 28\nPosterior distribution -> Inference model\n• Variational approximation\n• Recognition model\n• Infer...",
          "Type: lectures2025<br>Text: 29\n„The Model“ (prior + conditional, or joint) -> generative model\n• The (data) likelihood model\n• G...",
          "Type: lectures2025<br>Text: 30\n• Variational lower bound\n𝐿 𝜃, 𝜙; 𝑥 = 𝔼𝑞𝜙(𝑧|𝑥) log 𝑝𝜃(𝑥|𝑧) − 𝐾𝐿 𝑞𝜙 𝑧 𝑥 ||𝑝(𝑧)\n  = 𝔼𝑞𝜙(𝑧|𝑥) log 𝑝𝜃...",
          "Type: lectures2025<br>Text: 31\nReparameterization trick...",
          "Type: lectures2025<br>Text: 32\n• Optimize 𝐿(𝜃, 𝜙; 𝑥) wrt. 𝜙 of 𝑞𝜙 𝑧 𝑥\n• ELBO: 𝐿(𝜃, 𝜙; 𝑥) = 𝔼𝑞𝜙(𝑧∣𝑥) log 𝑝𝜃(𝑥, 𝑧) + 𝐻 𝑞𝜙(𝑧 ∣ 𝑥)\n•...",
          "Type: lectures2025<br>Text: 33\n• Gradient estimate with reparameterization trick\n    𝑧 ∼ 𝑞𝜙(𝑧 ∣ 𝑥) ⇔ 𝑧 = 𝑔𝜙(𝜖, 𝑥), 𝜖 ∼ 𝑝(𝜖)\n∇𝜙𝔼𝑞...",
          "Type: lectures2025<br>Text: 34\n• Score function gradient is broadly applicable to nearly any \nvariational distribution, regardle...",
          "Type: lectures2025<br>Text: VAEs: Algorithm\n[Kingma & Welling, 2014]...",
          "Type: lectures2025<br>Text: Projecting Means in Latent Space\nSource: https://atcold.github.io/pytorch-Deep-Learning/en/week08/08...",
          "Type: lectures2025<br>Text: Projecting Means in Latent Space\nSource: https://atcold.github.io/pytorch-Deep-Learning/en/week08/08...",
          "Type: lectures2025<br>Text: 38\nLatent code interpolation and sentences generation from VAEs [Bowman et \nal., 2015] [5]\nVAE: Exam...",
          "Type: lectures2025<br>Text: 39\nTopic Models\nNeural Networks for Natural Language Processing Prof. Sophie Fellenz...",
          "Type: lectures2025<br>Text: ▪ Word clouds (important words are bigger)\n▪ Probability distributions over words\n40\nTopics\nNeural N...",
          "Type: lectures2025<br>Text: 41\nTopic Models\n▪ Input: unstructured text data\n▪ Output: Topics\n▪ No annotations, labels, tags …\n▪ ...",
          "Type: lectures2025<br>Text: 0\n0,5\n1\n42\nSPATIAL ORGANIZATION OF NEURAL NETWORKS: A \nPROBABILISTIC MODELING APPROACH\nprobabilistic...",
          "Type: lectures2025<br>Text: 43\nSPATIAL ORGANIZATION OF NEURAL NETWORKS: A \nPROBABILISTIC MODELING APPROACH\nTopics Documents\nTopi...",
          "Type: lectures2025<br>Text: 44\nVAE-based Topic Models\nNeural Networks for Natural Language Processing Prof. Sophie Fellenz...",
          "Type: lectures2025<br>Text: 45\nDirichlet VAE\nInput (BoW) Output (BoW)\nz~𝐷𝑖𝑟(𝛼)𝛼\n𝑧 ∈ ℝ𝑚\nDistribution over topics\nFood\nPlace\nNice\n...",
          "Type: lectures2025<br>Text: 46\n• The Dirichlet distribution is a distribution over the (K-1)-\ndimensional simplex\n• It is parame...",
          "Type: lectures2025<br>Text: 47\nProbability Simplex\n1-D Simplex\n2-D Simplex\nTopic 1 Topic 2\n0.1/0.9\n0.5/0.5\nTopic 1 Topic 2\nTopic...",
          "Type: lectures2025<br>Text: 48\n• If 𝜋 ∼ 𝐷𝑖𝑟𝑖𝑐ℎ𝑙𝑒𝑡(𝛼1, … , 𝛼𝐾) then 𝜋𝑘 ≥ 0 for all k, and \n∑𝑘=1\n𝐾 𝜋𝑘 = 1\n• Expectation: 𝔼 𝜋1, … ,...",
          "Type: lectures2025<br>Text: 49\n•The concentration parameter 𝛼 determines the distribution \nover atom sizes\n•Small values of 𝛼 gi...",
          "Type: lectures2025<br>Text: 50\n• One document should be assigned to as few topics as possible\n• Why?\n• If one document is assign...",
          "Type: lectures2025<br>Text: 51\n• Basic neural topic modelling (NTM) architecture, was proposed by Miao et al. \n(2015) (Gaussian)...",
          "Type: lectures2025<br>Text: 52\n• menu minutes service ordered new order came went table way\n• wait try minutes going time good v...",
          "Type: lectures2025<br>Text: 53\n• nice service went wait great [UNK] think want food time\n• ordered [UNK] nice going like people ...",
          "Type: lectures2025<br>Text: 54\nℒ 𝜃, 𝜙; 𝑥 = 𝔼𝑞𝜙(𝑧|𝑥) log 𝑝𝜃 𝑥 𝑧 − 𝐷𝐾𝐿(𝑞𝜙(𝑧|𝑥)||𝑝(𝑧))\nℒ 𝜃, 𝜙; 𝑥 = 𝔼𝑞𝜙(𝑧|𝑥) log 𝑝𝜃 𝑥 𝑧 − 𝜷𝐷𝐾𝐿(𝑞𝜙(𝑧|...",
          "Type: lectures2025<br>Text: 55\n• is are they your do buy always enjoy go favorite\n• to look 's really so have looking pretty bur...",
          "Type: lectures2025<br>Text: 56\n• Thai, soup, rice, tuna, roll\n• Bartender, tables, drinks, server, restaurant\n• Burger, fries, e...",
          "Type: lectures2025<br>Text: 61\n• VAEs are generative models that learn a generative distribution \nfor the data\n• Reparameterizat...",
          "Type: lectures2025<br>Text: 62\n• Kingma, Diederik, and Max Welling. “Auto-encoding variational \nBayes” https://arxiv.org/abs/131...",
          "Type: lectures2025<br>Text: Neural networks for \nNatural Language Processing\nJun.-Prof. Dr. Sophie Fellenz\nWeek  08 – Fine-tunin...",
          "Type: lectures2025<br>Text: Agenda\n•BERT Fine-Tuning\n•GPT Fine-Tuning\n•RLHF Fine-tuning...",
          "Type: lectures2025<br>Text: BERT\n3...",
          "Type: lectures2025<br>Text: 4\nBERT\nSource: https://humboldt-\nwi.github.io/blog/research/information_systems_1\n920/bert_blog_post...",
          "Type: lectures2025<br>Text: 5\nBERT Pre-Training\nMasked Language model (MLM): Take sentence, mask single words\nand let BERT predi...",
          "Type: lectures2025<br>Text: 6\nBERT Pre-Training\nNext sentence prediction (NSP): Take two sentences and ask\nBERT if the second se...",
          "Type: lectures2025<br>Text: 7\nBERT Pre-Training\n• Take two sentences\n• Mask some words and embed into 𝐸𝑖\n• BERT outputs vectors ...",
          "Type: lectures2025<br>Text: 8\nBERT Pre-Training (MLM)\n• For 𝐸𝑖 masked :\n• Transform 𝑇𝑖 into probabilities over vocabulary\n• Chec...",
          "Type: lectures2025<br>Text: 9\nBERT Pre-Training (NSP)\n• Transform 𝐶 into 2-dim. probability \n• Sentences belong (don‘t belong) t...",
          "Type: lectures2025<br>Text: 10\nBERT Fine-Tuning\n• For application adjust model by adding layers on input and \noutput to fit the ...",
          "Type: lectures2025<br>Text: 11\nBERT Fine-Tuning...",
          "Type: lectures2025<br>Text: GPT\n12...",
          "Type: lectures2025<br>Text: 13\nGPT\n• GPT (Generative Pre-Trained Transformer) is a \nlanguage model to produce human-like text\n• ...",
          "Type: lectures2025<br>Text: 14\nGPT Unsupervised Pre-Training\n• Input context 𝑥𝑖−1,…,𝑥𝑖−𝑘\n• Output probability distribution over ...",
          "Type: lectures2025<br>Text: 15\nGPT Supervised Fine-Tuning\n• Given labelled dataset of sentences 𝐶\n• For sentence 𝑥 = (𝑥1,…,𝑥𝑛) w...",
          "Type: lectures2025<br>Text: 16\nGPT Supervised Fine-Tuning\n• The objective function is\n𝐿2 𝐶 = ෍\n(𝑥,𝑦)\nlog(𝑃 𝑦|𝑥,Θ )\n• To improve ...",
          "Type: lectures2025<br>Text: 17\nGPT Supervised Fine-Tuning\n• The exact training procedure depends on the task\n• Example (Entailme...",
          "Type: lectures2025<br>Text: 18\nGPT Supervised Fine-Tuning\nSource: Radford, \nAlec, et al. \n\"Improving language \nunderstanding by ...",
          "Type: lectures2025<br>Text: Fine-tuning based on Reinforcement Learning\n19...",
          "Type: lectures2025<br>Text: ChatGPT:Optimizing Language models for dialogue...",
          "Type: lectures2025<br>Text: Why RLHF?\nHow to create a loss function for\n• What is funny?\n• What is ethical?\n• What is safe?\n• Wh...",
          "Type: lectures2025<br>Text: Core technique: Learning from Human Feedback (RLHF)...",
          "Type: lectures2025<br>Text: Core technique: Learning from Human Feedback (RLHF)\n• Using human preferences as a reward signal to ...",
          "Type: lectures2025<br>Text: Reinforcement learning on human feedback (RLHF)\n• Three steps in general:\n1. Pretraining a language ...",
          "Type: lectures2025<br>Text: RLHF: Step 1, Pretraining the language model\nSource: \nhttps://huggingface.co/blog/rlhf...",
          "Type: lectures2025<br>Text: RLHF: Step 2, reward model training\nSource: \nhttps://huggingface.co/blog/rlhf...",
          "Type: lectures2025<br>Text: RLHF: Step 3, fine-tuning with RL\nSource: \nhttps://huggingface.co/blog/rlhf...",
          "Type: lectures2025<br>Text: What is ChatGPT? \n• It is a sibling model of InstructGPT (Ouyang et al. 2022) which is trained (thro...",
          "Type: lectures2025<br>Text: InstructGPT\n30\nSource: https://openai.com/blog/instruction-\nfollowing/...",
          "Type: lectures2025<br>Text: InstructGPT: High-level methodology\n31...",
          "Type: lectures2025<br>Text: Step 1: Supervised Fine-Tuning (SFT)\nLabelers were asked to write three kinds of prompts:\n• Plain: W...",
          "Type: lectures2025<br>Text: Step 1: Supervised Fine-Tuning (SFT)\n• 40 contract workers were hired and screened\n• Labelers were a...",
          "Type: lectures2025<br>Text: Step 2: Reward model (RM)\n• Labelers were asked to rank between K=4 and K=9 model outputs according ...",
          "Type: lectures2025<br>Text: Step 3: RLHF with PPO\n• Fine-tune SFT model using PPO\n• PPO dataset contains 31k prompts from the AP...",
          "Type: lectures2025<br>Text: Reinforcement Learning and PPO\n36...",
          "Type: lectures2025<br>Text: Timeline for RL in Games\n• 1992: TD-Gammon, temporal difference learning, Backgammon\n• 1997: Deep Bl...",
          "Type: lectures2025<br>Text: Applying RL in NLP with Robotics\n38\nSource: https://say-can.github.io/...",
          "Type: lectures2025<br>Text: Outline\nToday:\n1. Introduction to Reinforcement Learning (RL) \n2. Policy-Based RL\n• Policy Gradient ...",
          "Type: lectures2025<br>Text: Policy-based RL\nAdvantages: \n• True objective\n• Can learn stochastic policies\n• Effective in high-di...",
          "Type: lectures2025<br>Text: Introduction to RL\n‘Reinforcement learning is learning what to do- how to map \nsituations to actions...",
          "Type: lectures2025<br>Text: Introduction to RL\n‘Reinforcement learning is learning what to do- how to map \nsituations to actions...",
          "Type: lectures2025<br>Text: Introduction to RL for NLP\n„The window is open!“ (is this a statement of fact?)\n„Can you close the w...",
          "Type: lectures2025<br>Text: Reinforcement Learning Problem\n44...",
          "Type: lectures2025<br>Text: Markov Decision Processes\n45...",
          "Type: lectures2025<br>Text: Agent‘s Learning Task\n46...",
          "Type: lectures2025<br>Text: Example: Aliased Grid World\nsource:[UCL course on RL, lecture 7]...",
          "Type: lectures2025<br>Text: Example: Aliased Grid World\nsource:[UCL course on RL, lecture 7]...",
          "Type: lectures2025<br>Text: Example: Aliased Grid World\nsource:[UCL course on RL, lecture 7]...",
          "Type: lectures2025<br>Text: Policy objective\n50\n• Goal: given policy 𝜋𝜃(𝑠,𝑎), find the best parameters 𝜃\n• How to measure the qu...",
          "Type: lectures2025<br>Text: Policy Gradient \n51\nSource: [Deepmind RL2021]...",
          "Type: lectures2025<br>Text: Contextual Bandits Policy Gradient\n• Consider a one-step case (a contextual bandit) such that 𝐽(𝜃) =...",
          "Type: lectures2025<br>Text: Score function trick\n𝑎...",
          "Type: lectures2025<br>Text: Policy gradient theorem (episodic)\nsource:[UCL course on RL, lecture 7]...",
          "Type: lectures2025<br>Text: Theorem Proof\n55source:[UCL course on RL, lecture 7]...",
          "Type: lectures2025<br>Text: Proof continued\n56\nsource:[UCL course on RL, lecture 7]...",
          "Type: lectures2025<br>Text: Policy gradient training\n57source:[UCL course on RL, lecture 7]...",
          "Type: lectures2025<br>Text: REINFORCE: Monte Carlo Policy Gradient\nPseudocode:\nfor each episode do:\nGenerate a trajectory Rollou...",
          "Type: lectures2025<br>Text: Actor-Critic\n59\nValue-functionPolicy...",
          "Type: lectures2025<br>Text: Overview RL Paradigms\nPolicy-basedValue-based Actor-Critic...",
          "Type: lectures2025<br>Text: Proximal Policy Optimization (PPO)\n• Motivation: Avoid too large policy updates -> improve the train...",
          "Type: lectures2025<br>Text: Proximal Policy Optimization (PPO)\n62\nsource:[UCL course on RL, lecture 7]...",
          "Type: lectures2025<br>Text: Step 3: RLHF with PPO\n𝑜𝑏𝑗𝑒𝑐𝑡𝑖𝑣𝑒 𝜙\n= 𝐸 𝑥,𝑦 ∼𝐷𝜋𝜙\n𝑅𝐿 𝑟𝜃 𝑥,𝑦 −𝛽log(𝜋𝜙\n𝑅𝐿(𝑦|𝑥)/𝜋𝑆𝐹𝑇(𝑦|𝑥)) +𝛾𝐸𝑥∼𝐷𝑝𝑟𝑒𝑡𝑟𝑎𝑖𝑛 ...",
          "Type: lectures2025<br>Text: Variations on the methodology\nAlmost all papers to date have tweaks:\nAnthropic\n• Initial policy help...",
          "Type: lectures2025<br>Text: InstructGPT vs Anthropic\nSource: https://youtu.be/2MBJOuVq380\nSource: Bai et al. “Training a Helpful...",
          "Type: lectures2025<br>Text: InstructGPT vs Anthropic\nSource: https://youtu.be/2MBJOuVq380\nSource: Bai et al. “Training a Helpful...",
          "Type: lectures2025<br>Text: InstructGPT vs Anthropic\nSource: https://youtu.be/2MBJOuVq380\nSource: Bai et al. “Training a Helpful...",
          "Type: lectures2025<br>Text: Summary\n• Policy-gradient methods directly optimize policy (in our case language model)\n• PPO tries ...",
          "Type: lectures2025<br>Text: References\n• [Sutton & Barto, 2018]Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An ...",
          "Type: lectures2025<br>Text: Jun.-Prof. Sophie Fellenz\nWeek 4 – Language Modeling and Neural Networks\n11 Nov 2024\nNeural Networks...",
          "Type: lectures2025<br>Text: • Is Skipgram based on neural networks?\n• What is the difference between a neural network word\nembed...",
          "Type: lectures2025<br>Text: • Difference:\n• Skipgram embeddings are used inside neural\nnetworks (first layer)\n• NN embeddings ar...",
          "Type: lectures2025<br>Text: Language Models\n4...",
          "Type: lectures2025<br>Text: • Jane went to the store.\n• store to Jane went the.\n• Jane went store.\n• Jane goed to the store.\n• T...",
          "Type: lectures2025<br>Text: • Jane went to the store.\n• store to Jane went the.\n• Jane went store.\n• Jane goed to the store.\n• T...",
          "Type: lectures2025<br>Text: 𝑃 𝑋 = ෑ\n𝑖=1\n𝐼\n𝑃(𝑥𝑖|𝑥1,…,𝑥𝑖−1)\nThe big problem: How do we predict\n𝑃 𝑥𝑖 𝑥1,…,𝑥𝑖−1\n??\nProbabilistic lan...",
          "Type: lectures2025<br>Text: Score sentences:\n• Jane went to the store . -> high\n• Store to Jane went the . -> low\n• (same as cal...",
          "Type: lectures2025<br>Text: Count-based Language Models\n9...",
          "Type: lectures2025<br>Text: Independence assumption: 𝑃 𝑥𝑖 𝑥1,…,𝑥𝑖−1 ≈ 𝑃(𝑥𝑖)\nCount-based maximum-likelihood estimation:\n𝑃𝑀𝐿𝐸 𝑥𝑖 =...",
          "Type: lectures2025<br>Text: Limit context length to 𝑛, count, and divide\n𝑃𝑀𝐿 𝑥𝑖 𝑥𝑖−𝑛+1,…,𝑥𝑖−1 ≔ 𝑐(𝑥𝑖−𝑛+1,…,𝑥𝑖)\n𝑐(𝑥𝑖−𝑛+1,…,𝑥𝑖−1)\n...",
          "Type: lectures2025<br>Text: Additive/Dirichlet:\n𝑃 𝑥𝑖 𝑥𝑖−𝑛+1,…,𝑥𝑖−1 ≔ 𝑐 𝑥𝑖−𝑛+1,…,𝑥𝑖 +𝛼𝑃(𝑥𝑖|𝑥𝑖−𝑛+2,…,𝑥𝑖−1)\n𝑐 𝑥𝑖−𝑛+1,…,𝑥𝑖−1 +𝛼\nDisc...",
          "Type: lectures2025<br>Text: Cannot share strength among similar words\nsolution: class based language models\nCannot condition on ...",
          "Type: lectures2025<br>Text: Cannot handle long-distance dependencies\nsolution: cache, trigger, topic, syntactic models, etc.\n14\n...",
          "Type: lectures2025<br>Text: • Neural language models (next) achieve better \nperformance, but\n• n-gram models are extremely fast ...",
          "Type: lectures2025<br>Text: LM Evaluation\n16...",
          "Type: lectures2025<br>Text: Log-likelihood:\n𝐿𝐿 ℇ𝑡𝑒𝑠𝑡 = ෍\n𝐸∈ℇ𝑡𝑒𝑠𝑡\nlog𝑃(𝐸)\nPer-word Log Likelihood:\n𝑊𝐿𝐿 ℇ𝑡𝑒𝑠𝑡 = 1\nσ𝐸∈ℇ𝑡𝑒𝑠𝑡 𝐸 ෍\n𝐸∈ℇ...",
          "Type: lectures2025<br>Text: • Important: the vocabulary must be the same over models \nyou compare\n• Or more accurately, all mode...",
          "Type: lectures2025<br>Text: Log-linear models\n19...",
          "Type: lectures2025<br>Text: • Calculate features of the context\n• Based on the features, calculate probabilities\n• Optimize feat...",
          "Type: lectures2025<br>Text: Calculate features of the context, calculate probabilities\nFeature weights optimized by SGD, etc 21\n...",
          "Type: lectures2025<br>Text: Previous words: “giving a\"\n22\nExample:\nWords we‘re\npredicting\nHow likely\nare they?\nHow likely\nare th...",
          "Type: lectures2025<br>Text: • Calculate the gradient of the loss function with respect to \nthe parameters\n• How? Use the chain r...",
          "Type: lectures2025<br>Text: 24\nWhat Problems are Handled?\nCannot share strength among similar words\nnot solved yet \nCannot condi...",
          "Type: lectures2025<br>Text: Cannot handle long-distance dependencies\nNot solved yet\n25\nProblems and solutions?\nFor tennis class ...",
          "Type: lectures2025<br>Text: Beyond linear models\n26...",
          "Type: lectures2025<br>Text: Students take tests → high Teachers take tests → low\nStudents write tests → low Teachers write tests...",
          "Type: lectures2025<br>Text: Original Motivation: Neurons in the Brain\nCurrent Conception: Computation Graphs\n28\n“Neural” Nets...",
          "Type: lectures2025<br>Text: 29\n𝑥1\n𝑥2\n𝑥3\nInput neurons:\n𝑤1,1\n𝑤2,1\n𝑤3,1\n𝑏1\n𝑥1𝑤1,1 +𝑥2𝑤2,1 +𝑥3𝑤3,1 +𝑏1\nOutput neurons:\n𝑥1𝑤1,2 +𝑥2𝑤2...",
          "Type: lectures2025<br>Text: 30\n𝑥1\n𝑥2\n𝑥3\nInput neurons: 𝑏\n𝑥1𝑤1,1 +𝑥2𝑤2,1 +𝑥3𝑤3,1 +𝑏1\nOutput neurons:\n𝑥1𝑤1,2 +𝑥2𝑤2,2 +𝑥3𝑤3,2 +𝑏2\n𝑥...",
          "Type: lectures2025<br>Text: 31\n𝑥1\n𝑥2\n𝑥3\nInput neurons: 𝑏\n𝑥1𝑤1,1 +𝑥2𝑤2,1 +𝑥3𝑤3,1 +𝑏1\nOutput neurons:\n𝑥1𝑤1,2 +𝑥2𝑤2,2 +𝑥3𝑤3,2 +𝑏2\n𝑥...",
          "Type: lectures2025<br>Text: 32\nInput layer Output\n𝑶𝒖𝒕𝒑𝒖𝒕 = 𝝈(…𝝈 𝒙𝑻𝑾𝟏 +𝒃𝟏\n𝑻 …𝑾𝒏 +𝒃𝒏𝑻)\nUsually we use multiple layers\nHidden layer...",
          "Type: lectures2025<br>Text: 33\nInput layer Output\nHow does this work specifically?\nExample: Given cat or dog image.\nTask: Find o...",
          "Type: lectures2025<br>Text: 34\nInput layer Output\nWhy does this work?\n• One can prove that any function, i.e. the cat-dog recogn...",
          "Type: lectures2025<br>Text: 35\nInput layer Output\nHow do we find out how large it needs to be?\n• Experimentation!\nHow do we find...",
          "Type: lectures2025<br>Text: • Given: \nTraining data, i.e. input data 𝑥 where desired output 𝑦is\nknown.\nNeural Network 𝑛𝑊,𝑏\n• Out...",
          "Type: lectures2025<br>Text: expression:\n𝑥\ngraph:\nA node is a {tensor, matrix, vector, scalar} value\n37\n𝑥...",
          "Type: lectures2025<br>Text: • An edge represents a function argument (and also a data dependency). \nThey are just pointers to no...",
          "Type: lectures2025<br>Text: expression:\n𝑥𝑇𝑊+𝑏\ngraph:\nFunctions can be nullary, unary, binary, ... n-ary. Often they are unary or...",
          "Type: lectures2025<br>Text: expression:\n𝜎(𝑥𝑇𝑊+𝑏𝑇)\ngraph:\n40\n𝑥\n𝑓1 𝑥,𝑊 = 𝑥𝑇𝑊\n𝑊\n∗\n𝑓2 𝑢,𝑏 = 𝑢+𝑏𝑇\n𝑏\n+\n𝑓3 𝑢 = 𝜎(𝑢)\n𝜎...",
          "Type: lectures2025<br>Text: expression:\n𝜎(𝑥𝑇𝑊+𝑏𝑇)−𝑦𝑇\ngraph:\n41\n𝑥\n𝑓1 𝑥,𝑊 = 𝑥𝑇𝑊\n𝑊\n∗\n𝑓2 𝑢,𝑏 = 𝑢+𝑏𝑇\n𝑏\n+\n𝑓3 𝑢 = 𝜎(𝑢)\n𝜎\n𝑓4 𝑢,𝑦 = 𝑢−𝑦\n−...",
          "Type: lectures2025<br>Text: expression:\nLoss = 𝜎(𝑥𝑇𝑊+𝑏𝑇 −𝑦𝑇ԡ2 = 𝑓5(𝑓4(𝑓3(𝑓2(𝑓1(𝑥,𝑊),𝑏)),𝑦))\ngraph:\n42\n𝑥\n𝑓1 𝑥,𝑊 = 𝑥𝑇𝑊\n𝑊\n∗\n𝑓2 𝑢,𝑏 ...",
          "Type: lectures2025<br>Text: • Graph construction\n• Forward propagation\nIn topological order, compute the value of the node \ngive...",
          "Type: lectures2025<br>Text: 44\nForward Propagation\n𝑥\n𝑓1 𝑥,𝑊 = 𝑥𝑇𝑊\n𝑊\n∗\n𝑓2 𝑢,𝑏 = 𝑢+𝑏𝑇\n𝑏\n+\n𝑓3 𝑢 = 𝜎(𝑢)\n𝜎\n𝑓4 𝑢,𝑦 = 𝑢−𝑦\n−\n𝑦\n𝑓5 𝑢 = 𝑢 ...",
          "Type: lectures2025<br>Text: 45\nForward Propagation\n𝑥\n𝑓1 𝑥,𝑊 = 𝑥𝑇𝑊\n𝑊\n𝑢1\n𝑓2 𝑢,𝑏 = 𝑢+𝑏𝑇\n𝑏\n+\n𝑓3 𝑢 = 𝜎(𝑢)\n𝜎\n𝑓4 𝑢,𝑦 = 𝑢−𝑦\n−\n𝑦\n𝑓5 𝑢 = 𝑢...",
          "Type: lectures2025<br>Text: 46\nForward Propagation\n𝑥\n𝑓1 𝑥,𝑊 = 𝑥𝑇𝑊\n𝑊\n𝑢1\n𝑓2 𝑢,𝑏 = 𝑢+𝑏𝑇\n𝑏\n𝑢2\n𝑓3 𝑢 = 𝜎(𝑢)\n𝜎\n𝑓4 𝑢,𝑦 = 𝑢−𝑦\n−\n𝑦\n𝑓5 𝑢 = ...",
          "Type: lectures2025<br>Text: 47\nForward Propagation\n𝑥\n𝑓1 𝑥,𝑊 = 𝑥𝑇𝑊\n𝑊\n𝑢1\n𝑓2 𝑢,𝑏 = 𝑢+𝑏𝑇\n𝑏\n𝑢2\n𝑓3 𝑢 = 𝜎(𝑢)\n𝑢3\n𝑓4 𝑢,𝑦 = 𝑢−𝑦\n−\n𝑦\n𝑓5 𝑢 =...",
          "Type: lectures2025<br>Text: 48\nForward Propagation\n𝑥\n𝑓1 𝑥,𝑊 = 𝑥𝑇𝑊\n𝑊\n𝑢1\n𝑓2 𝑢,𝑏 = 𝑢+𝑏𝑇\n𝑏\n𝑢2\n𝑓3 𝑢 = 𝜎(𝑢)\n𝑢3\n𝑓4 𝑢,𝑦 = 𝑢−𝑦\n𝑢4\n𝑦\n𝑓5 𝑢 ...",
          "Type: lectures2025<br>Text: 49\nForward Propagation\n𝑥\n𝑓1 𝑥,𝑊 = 𝑥𝑇𝑊\n𝑊\n𝑢1\n𝑓2 𝑢,𝑏 = 𝑢+𝑏𝑇\n𝑏\n𝑢2\n𝑓3 𝑢 = 𝜎(𝑢)\n𝑢3\n𝑓4 𝑢,𝑦 = 𝑢−𝑦\n𝑢4\n𝑦\n𝑓5 𝑢 ...",
          "Type: lectures2025<br>Text: • Aim: Minimize loss 𝑓 𝑥,𝑊,𝑏 = ԡ𝜎(𝑥𝑇𝑊 +𝑏𝑇)−𝑦𝑇ԡ2\nw.r.t. weights 𝑊,𝑏\n• Idea: Gradient = Direction of h...",
          "Type: lectures2025<br>Text: Back-propagation:\n• Process examples in reverse topological order\n• Calculate the derivatives of the...",
          "Type: lectures2025<br>Text: Expression:\nLoss = ԡ𝜎(𝑥𝑇𝑊 +𝑏𝑇)−𝑦𝑇ԡ2\nAim: Minimize loss by optimizing weights 𝑊,𝑏\n52\nBack Propagation...",
          "Type: lectures2025<br>Text: Step 1: Compute all derivatives at every node wrt. the inputs\n(at relevant edges)\nExample: \n𝜕𝑓2\n𝜕𝑏 =...",
          "Type: lectures2025<br>Text: Step 1: Compute all derivatives at every node wrt. the inputs.\n54\nBack Propagation\n𝑥\n𝑓1 𝑥,𝑊 = 𝑥𝑇𝑊\n𝑊\n...",
          "Type: lectures2025<br>Text: Step 2: Use the chain rule.\nExample:𝑓 𝑥,𝑊,𝑏 = 𝜎(𝑥𝑇𝑊 +𝑏𝑇 −𝑦𝑇ԡ2\n= 𝑓5(𝑓4(𝑓3(𝑓2(𝑓1(𝑥,𝑊),𝑏)),𝑦))\n𝜕𝑓(𝑥,𝑊,𝑏...",
          "Type: lectures2025<br>Text: Step 2: Use the chain rule.\nExample:𝑓 𝑥,𝑊,𝑏 = 𝜎(𝑥𝑇𝑊 +𝑏𝑇 −𝑦𝑇ԡ2\n= 𝑓5(𝑓4(𝑓3(𝑓2(𝑓1(𝑥,𝑊),𝑏)),𝑦))\n𝜕𝑓(𝑥,𝑊,𝑏...",
          "Type: lectures2025<br>Text: Step 2: Use the chain rule.\nExample:𝑓 𝑥,𝑊,𝑏 = 𝜎(𝑥𝑇𝑊 +𝑏𝑇 −𝑦𝑇ԡ2 = 𝑓5(𝑓4(𝑓3(𝑓2(𝑓1(𝑥,𝑊),𝑏)),𝑦))\n𝜕𝑓(𝑥,𝑊,𝑏...",
          "Type: lectures2025<br>Text: Step 2: Use the chain rule.\nSimilarly, we calculate\n𝜕𝑓\n𝜕𝑊 .Note that we already have calculated\n𝜕𝑓\n𝜕...",
          "Type: lectures2025<br>Text: Step 3: Apply Gradient descent\nUpdate: 𝛼 > 0learning rate\n𝑊𝑛𝑒𝑤 = 𝑊𝑜𝑙𝑑 −𝛼 𝜕𝑓\n𝜕𝑊\n𝑏𝑛𝑒𝑤 = 𝑏𝑜𝑙𝑑 −𝛼𝜕𝑓\n𝜕𝑏\n5...",
          "Type: lectures2025<br>Text: Back to language modeling\n60...",
          "Type: lectures2025<br>Text: • See Bengio et al. 2003\n61\nFeed-forward Neural Language Models\nDeep Learning for Natural Language P...",
          "Type: lectures2025<br>Text: • Word embeddings capture features of words\n• e.g. feature 1 indicates verbs, feature 2 indicates de...",
          "Type: lectures2025<br>Text: 63\nWhere is Strength Shared?\nlookup\ngiving\nlookup\n𝑡𝑎𝑛ℎ(𝑊1ℎ+𝑏) softmax=+\nBias scores probs\nW\nWord emb...",
          "Type: lectures2025<br>Text: 65\nWhat Problems are Handled?\nCannot share strength among similar words\nsolved, and similar contexts...",
          "Type: lectures2025<br>Text: Cannot handle long-distance dependencies\nNot solved yet\n66\nProblems and solutions?\nFor tennis class ...",
          "Type: lectures2025<br>Text: • Neural networks allow design of arbitrarily complex\nfunctions!\n• In future classes:\n▪ Recurrent ne...",
          "Type: lectures2025<br>Text: Next lecture\nRecurrent Neural Networks...",
          "Type: lectures2025<br>Text: ● CMU Advanced NLP Course:\n● https://phontron.com/class/anlp2022/schedule.html\n● Sören Laue\n● Feibai...",
          "Type: lectures2025<br>Text: ● Video on Backprop by Andrej Karpathy:\n• https://youtu.be/VMj-3S1tku0\n• Video on Language modeling ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nLecture 7 – Language Models – Generating Text \nfrom ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 2\nImage Source: https://www.tensorflow.org/text/tut...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n8\nI have eaten an apple\nI have eaten an apple\nI have...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n9\nI\nI\nI\nI\nI\nI\neaten\neaten\neaten\neaten\nan\nan\nan\napple...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n10...",
          "Type: lectures2025<br>Text: Beam search\nBeam search: Keep track of the 𝑘 most likely partial translations\n𝑘 is the beam size\n11...",
          "Type: lectures2025<br>Text: Beam search\nBeam size: 2\n12\n<Start>\na\nthe\nSource: https://web.stanford.edu/class/archive/cs/cs224n/c...",
          "Type: lectures2025<br>Text: Beam search\nBeam size: 2\n13\n<Start>\na\nthe\npoor\npeople\npoor\nperson\nSource: https://web.stanford.edu/c...",
          "Type: lectures2025<br>Text: Beam search\nBeam size: 2\n14\n<Start>\na\nthe\npoor\npeople\npoor\nperson\nare\ndon‘t\nbut\nperson\nSource: https...",
          "Type: lectures2025<br>Text: Beam search\nBeam size: 2\n15\n<Start>\na\nthe\npoor\npeople\npoor\nperson\nare\ndon‘t\nbut\nperson\nalways\nnot\nha...",
          "Type: lectures2025<br>Text: Beam search\nBeam size: 2\n16\n<Start>\na\nthe\npoor\npeople\npoor\nperson\nare\ndon‘t\nbut\nperson\nalways\nnot\nha...",
          "Type: lectures2025<br>Text: Beam search\nBeam size: 2\n17\nSource: https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1184/lec...",
          "Type: lectures2025<br>Text: Beam search\nBeam size: 2\n18\nSource: https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1184/lec...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Recap: language models\n• Generating text: Intro\n• ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• A language model (LM) 𝑝𝜃 is a probability distribu...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Intuitively, are Language Models naturally suited ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Language models might not be perfectly-suited for ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Language models might not be perfectly-suited for ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Language models might not be perfectly-suited for ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nFundamentals of discrete distributions\n25...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• A discrete probability distribution p(x) is define...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n“Small” discrete sets\n0.25 0.50 0.25 0.15\ne.g., a vo...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n“Large” discrete sets\ne.g., sequences of up to lengt...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• A discrete probability distribution p(x) is define...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Entropy is arguably the single most useful propert...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nIntuition of Entropy –low entropy and high entropy\nI...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nExtreme Example of\nHigh-entropy and Low-entropy toke...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nIn natural language generation, we often describe ta...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nWe have a discrete set 𝜒and a distribution 𝑝 𝑥\nThe e...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nSometimes, the true next token distribution has many...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nSometimes, the true next token distribution has many...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nSometimes, the true next token distribution has few ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nContext lowers entropy\nMy Car\nis\ndrives\nwas\nhas\nruns...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nAdding Context Reduces Entropy (Important for the co...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nWe have a discrete set 𝜒and a distribution 𝑝 𝑥\nThe m...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nWe have a discrete set 𝜒and a distribution 𝑝 𝑥 , and...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nHere are some examples of semantically similar examp...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nFrom parsing to machine translation, there’s a long ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nIn a high entropy distribution, most probability ten...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Often, researchers/developers are not working with...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nDecoding as a choice of Algorithm + Scoring  \nFuncti...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Given a distribution 𝑝𝜃, how can we characterize d...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Given a distribution 𝑝𝜃, how can we characterize d...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Given a distribution 𝑝𝜃, how can we characterize d...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Given a distribution 𝑝𝜃, how can we characterize d...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nCommon Choices of Scoring Functions\n51...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nIf current language generators were already flawless...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nWhy do we need alternative scoring functions?\n53\nUnf...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nThe mean-seeking nature of\nthe MLE training objectiv...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• In this section we’ll choose multinomial sampling ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Ancestral sampling is obtained when the scoring fu...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nConsequence of temperature sampling\nOne day a cat de...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nTemperature Sampling code examples –\nHigher Temperat...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nTemperature Sampling code examples –\nLower Temperatu...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• The presence of an unreliable tail suggests the us...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nTop-k sampling\n• Simply truncate the tail by selecti...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nTwo failure modes of top-k sampling\nSuppose we fix k...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nPerplexity and generation quality\n• Some scoring fun...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nPerplexity and generation quality\n• High-quality tex...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nMirostat\nAt each generation step\n• Choose the top-k ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nMirostat\nTune the size of the top-k set to sample a ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nNucleus sampling\n• Select tail size dynamically by o...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nDecreasing the pi in Nucleus Sampling decreases the ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nA failure mode of nucleus sampling\nSuppose we fix 𝜋=...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• The absolute probability principle — that words ou...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nEpsilon Sampling vs Top-K Sampling vs  Nucleus Sampl...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nPrompting: The Precursor to Controlled \nGeneration\n7...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nWhat is prompting?\n• So far, we’ve looked at how to ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nPrompting in our framework\n• Prompting can be viewed...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nPrompting in our framework\n• Prompting can be viewed...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nWhy does prompting work?\nRecall: Context (often) low...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nWhy does prompting work?\nIntuition: providing more c...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nPrompting to solve lower entropy tasks\n81\nEnglish:\nT...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nTypes of Prompting: Demonstrations\nFor a task with i...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nTypes of Prompting: Learning Prompts\nFor a task with...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nTypes of Prompting: Chain-of-Thought\nFor a task with...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nIs prompting the definite solution?\n85...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nCurrent Trends in Language Generation: Larger Models...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nImplications of Larger Models\n• Many of the methods ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nFine-Tuning Approaches\n• If we’re willing to retrain...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nInstruction-Tuned and RLHF Language Models\nBase pret...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nInstruction-Tuned and RLHF Language Models\nInstructi...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nInstruction-Tuned and RLHF Language Models\nRLHF like...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nConclusion\n• High-entropy vs low-entropy generation\n...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nNext time: Reinforcement Learning for NLP\n94\nLiu et ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nReferences\nProbability distributions over strings\nFo...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nAcknowledgements\n• “Generating Text from Language Mo...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nLecture 5 – RNN/LSTM/CNN Language Models\n20.11.2023\n...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Recap: N-gram language models\n• RNN language model...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n▪ Language Modeling is the task of predicting what w...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nWe use language models everyday!\n4...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nWe use language models everyday!\n5...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• the woman bought a _ _ _ _\n• Question: How to lear...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nYou can also use a language model to generate text\nG...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nYou can also use a language model to generate text.\n...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Recall the Language Modeling task:\n▪ Input: sequen...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nA fixed-window neural Language Model\nthe\n woman boug...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n▪ Improvements over n-gram LM:\n▪ No sparsity problem...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n▪ A family of neural architectures\nRecurrent Neural ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nA (typical) RNN Language Model\nbought awords / one-h...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nA (typical) RNN Language Model\nbought awords / one-h...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nA (typical) RNN Language Model\nbought awords / one-h...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nA (typical) RNN Language Model\nbought awords / one-h...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nA (typical) RNN Language Model\nbought awords / one-h...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n▪ RNN Advantages:\n• Can process any length input\n• M...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• RNN Disadvantages:\n• Recurrent computation is slow...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Get a big corpus of text which is a sequence of wo...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nTraining a RNN Language Model\nbought a\nthe woman\nCor...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nTraining a RNN Language Model\nbought a\nthe woman\nCor...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nTraining a RNN Language Model\nbought a\nthe woman\nCor...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nTraining a RNN Language Model\nbought a\nthe woman\nCor...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nTraining a RNN Language Model\nbought a\nthe woman\nCor...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• However: Computing loss and gradients across entir...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nBackpropagation for RNNs\n𝑾𝒉\n𝒉 𝟎\n… 𝑾𝒉\n𝒉 𝒕−𝟑\n𝑾𝒉\n𝒉 𝒕−𝟐\n...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nBackpropagation for RNNs\n𝑾𝒉\n𝒉 𝟎\n… 𝑾𝒉\n𝒉 𝒕−𝟑\n𝑾𝒉\n𝒉 𝒕−𝟐\n...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nMultivariable Chain Rule\n29...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nBackpropagation for RNNs: Proof sketch\n30...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nBackpropagation for RNNs\n𝑾𝒉\n𝒉 𝟎\n… 𝑾𝒉\n𝒉 𝒕−𝟑\n𝑾𝒉\n𝒉 𝒕−𝟐\n...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nBackpropagation for RNNs\n𝑾𝒉\n𝒉 𝟎\n… 𝑾𝒉\n𝒉 𝒕−𝟑\n𝑾𝒉\n𝒉 𝒕−𝟐\n...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Let’s have some fun!\n• You can train a RNN-LM on a...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Let’s have some fun!\n• You can train a RNN-LM on a...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Let’s have some fun!\n• You can train a RNN-LM on a...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• 1. Vanishing gradients\n• 2. Exploding gradients\nPr...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nProblems with RNNs\n1. Vanishing gradients\n2. Explodi...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nVanishing gradient intuition\n𝑾\n𝒉 𝟏\n𝑾\n𝒉 𝟐\n𝑾\n𝒉 𝟑\n 𝒉 𝟒\n...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nVanishing gradient intuition\n𝑾\n𝒉 𝟏\n𝑾\n𝒉 𝟐\n𝑾\n𝒉 𝟑\n 𝒉 𝟒\n...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nVanishing gradient intuition\n𝑾\n𝒉 𝟏\n𝑾\n𝒉 𝟐\n𝑾\n𝒉 𝟑\n 𝒉 𝟒\n...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nLM task: When she tried to print her tickets, she fo...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• If the gradient becomes too big, then the SGD upda...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• A solution for exploding gradient!\n• Gradient clip...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nHow to fix the vanishing gradient problem?\n• The mai...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• A type of RNN proposed by Hochreiter and Schmidhub...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• The selection of which information is erased/writt...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nWe have a sequence of inputs 𝑥(t), and we will compu...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nWe have a sequence of inputs 𝑥(t), and we will compu...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nWe have a sequence of inputs 𝑥(t), and we will compu...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nWe have a sequence of inputs 𝑥(t), and we will compu...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nLong Short-Term Memory RNNs (LSTMs)\nNew cell content...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nLong Short-Term Memory RNNs (LSTMs)\nNew cell content...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nLong Short-Term Memory RNNs (LSTMs)\nNew cell content...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nLong Short-Term Memory RNNs (LSTMs)\nNew cell content...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nYou can think of the LSTM equations visually like th...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nLong Short-Term Memory RNNs (LSTMs)\nYou can think of...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• The LSTM architecture makes it easier for the RNN ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• In 2013–2015, LSTMs started achieving state-of-the...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• No! It can be a problem for all neural architectur...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nBidirectional and Multi-layer RNNs: motivation\nthe m...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nBidirectional and Multi-layer RNNs: motivation\nthe m...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• On timestep 𝑡:\n• Forward RNN ℎ 𝑡 = 𝑅𝑁𝑁𝐹𝑊(ℎ 𝑡−1 ,𝑥(...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Note: bidirectional RNNs are only applicable if yo...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• RNNs are already “deep” on one dimension (they unr...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nMulti-layer RNNs\nthe movie was terribly exciting !\nR...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• High-performing RNNs are often multi-layer (but ar...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Cannot share strength among similar words\n• solved...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n▪ Cannot handle long-distance dependencies\n▪ Solved!...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nCNNs\n69...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nA 1D convolution for text\nTentative 0.2 0.1 -0.3 0.4...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nPadding\nØ 0.0 0.0 0.0 0.0\nTentative 0.2 0.1 -0.3 0.4...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nMultiple filters\nØ 0.0 0.0 0.0 0.0\nTentative 0.2 0.1...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nMax pooling\nØ 0.0 0.0 0.0 0.0\nTentative 0.2 0.1 -0.3...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nAverage pooling\nØ 0.0 0.0 0.0 0.0\nTentative 0.2 0.1 ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nStride=2\nØ 0.0 0.0 0.0 0.0\nTentative 0.2 0.1 -0.3 0....",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nKim (2014)\n76...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• RNNs are capable of learning long sequences and pr...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nNext lecture\nAttention and Transformers...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n▪ “Long short-term memory”, Hochreiter and Schmidhub...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Asmita Bhat\n• Stanford CS224N, Lecture 6 and 7\nAck...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 1\nLecture 3 – Word Embeddings\nJun.-Prof. Sophie Fel...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 2\n• Word meaning\n• Word2vec intro\n• Word2vec, more ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 3\n• One-hot encodings\n• Dense encodings\nWord embedd...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 4\n4\nWord meaning...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 5\n• Meaning of word “Meaning” (according to Webster...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 6\n• Common solution: Use e.g. Wordnet\n• Wordnet is ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 7\n• Great as a resource but missing nuance\n• e.g. “...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 8\n• Localist representation (traditional rule-based...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 9\n• Example: in web search, if user searches for “S...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 10\n• Distributionalsemantics: A word’s meaning is g...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 11\nWe will build a dense vector for each word, chos...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 12\n• Distributional semantics (as opposed to other ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 13\nVisualizing Word Vectors\n[source: https://medium...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 14\n1. Frequency based word vectors\n• Count vectors\n...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 15\n• Idea: Represent a word as a vector of frequenc...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 16\n• Example: Consider the term frequency count of ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 17\n• Measures co-occurrence of words.\n• Idea: Words...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 18\n• Overview: Given a corpus of documents, co-occu...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 19\n• Co-occurrence Matrix: (She is happy. She is we...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 20\n• Simple count co-occurrence vectors\n• Vectors i...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 21\n• Prediction based embeddings are obtained by pr...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 22\n• Word2Vec (Mikolov et al. 2013) is a framework ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 23\n• Word2Vec (Mikolov et al. 2013) is a framework ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 24\nExample windows and process for computing P(𝑤𝑡+𝑗...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 25\nExample windows and process for computing P(𝑤𝑡+𝑗...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 26\n• For each position 𝑡 = 1,…,𝑇, predict context w...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 27\nWe want to minimize the objective function:\n𝐽 𝜃 ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 28\nWord2Vec : prediction function\n𝑃 𝑜 𝑐 = exp(𝑢𝑜𝑇𝑣𝑐...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 29\n𝑃 𝑜 𝑐 = exp(𝑢𝑜𝑇𝑣𝑐)\n𝛴𝑤∈𝑉 exp(𝑢𝑤𝑇 𝑣𝑐)\nThis is an e...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 30\nWord2Vec : CBOW\nINPUT PROJECTION OUTPUT\nw(t-2)\nS...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 31\nWord2Vec : Skip-Grams\nINPUT PROJECTION OUTPUT\nw(...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 32\nSkipgram\nSource: http://mccormickml.com/2016/04/...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 33\nMinimize: 𝐽 𝜃 = −\n1\n𝑇 σ𝑡=1\n𝑇 σ−𝑚≤𝑗≤𝑚\n𝑗≠0\nlog𝑃 𝑤𝑡...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 34\n𝜕\n𝜕𝑣𝑐\nlogexp(𝑢𝑜𝑇𝑣𝑐) = 𝜕\n𝜕𝑣𝑐\n𝑢𝑜𝑇𝑣𝑐 = 𝑢𝑜\n𝜕\n𝜕𝑣𝑐\nlog...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 35\nSkip gram gradient\n𝜕\n𝜕𝑣𝑐\nlog exp(𝑢𝑜𝑇𝑣𝑐)\n𝛴𝑤∈𝑉 exp...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 36\nThe skip-gram model with negative sampling\n• The...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 37\nThe skip-gram model with negative sampling\nFrom ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 38\nCount-based vs prediction-based WE\nCount based\n•...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 39\n𝐽 𝜃 = 1\n2 ෍\nⅈ,𝑗=1\n𝑊\n𝑓 𝑋𝑖𝑗 𝑢𝑖\n𝑇𝑣𝑗 −log𝑋𝑖𝑗\n2\n• Fas...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 40\nNearest words to frog:\n1. frogs\n2. toad\n3. litor...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 41\n• Related to general evaluation in NLP: Intrinsi...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 42\n• Word vector analogies\n• a : b = c : ? 𝑑 = argm...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 43\nGloVe Visualizations\n[source: https://web.stanfo...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 44\nGloVe Visualizations: Company - CEOs\n[source: ht...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 45\nGloVe Visualizations: Comparatives - Superlative...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 46\nExpression Nearest token\nParis – France + Italy ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 47\n• More data helps\n• Wikipedia is better than new...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 48\n• Dimensionality\n• Good dimension is ~300\nAnalog...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 49\n• Word vector distances and their correlation wi...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 50\n• Extrinsic evaluation of word vectors: All subs...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 51\n• Most words have lots of meanings!\n• Especially...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 52\n• A sharp point or staff\n• A type of elongated f...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 53\nIdea: Cluster word windows around words, retrain...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 54\n• Pretraining with e.g. word2vec\n• Fine tuning o...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 55\nSentence embeddings: Sentences are mapped to num...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 56\n1. Smooth Inverse Frequency (Arora S. et. al.): ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 57\n• Information retrieval – To compare the meaning...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 58\n• Word vectors :\n• count-based vectors\n• word2ve...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 59\nNext lecture\nIntro to Neural Networks...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 60\nReferences\n• Efficient Estimation of Word Repres...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 61\nAcknowledgements\n• Stanford Course “Natural Lang...",
          "Type: lectures2025<br>Text: Neural Networks for \nNatural Language Processing\nJun.-Prof. Dr. Sophie Fellenz\nWeek 12 – Self-Superv...",
          "Type: lectures2025<br>Text: Course Organization\n• Scheduling of Q&A Session\n• Last Exercise Sheet due today\n2...",
          "Type: lectures2025<br>Text: Outline Self-Supervised\n• Preliminaries\n• Pretext Tasks\n• Self-Supervised Learning Concepts\n• Contra...",
          "Type: lectures2025<br>Text: What is self-supervision?\n4\nhttps://t3.ftcdn.net/jpg/03/12/24/14/360_F_312241\n475_OywzPQNBkO4xkSpT9v...",
          "Type: lectures2025<br>Text: Why self-supervision?\n• Getting labels for supervision is \nexpensive\n• E.g. Labeling Imagenet took 2...",
          "Type: lectures2025<br>Text: Idea of Self-Supervision\n9\nSlide credits: Yann LeCun and Ishan Misra...",
          "Type: lectures2025<br>Text: 10\nContrastive \nLearning\nJaiswal 2020, https://images.app.goo.gl/cehS4AmHg1tvjzcF9; \nhttps://images....",
          "Type: lectures2025<br>Text: Learning problems\n• Unsupervised learning\n• Learn model parameters using data without labels {𝐱𝐢}𝒊=𝟏...",
          "Type: lectures2025<br>Text: Pretext task to learn representations\n• Learn more general representations using self-supervision\n• ...",
          "Type: lectures2025<br>Text: Skip-Gram\n• Goal: Predict context words from center word\n• Example\n• Context size 1\n• Predict 2 surr...",
          "Type: lectures2025<br>Text: Skip-Thoughts\n• Goal: Predict neighboring sentences\n• Example\n• Context size 1\n• Predict 2 surroundi...",
          "Type: lectures2025<br>Text: Masked language model\n• Randomly mask text\n• Model predicts masked text from surrounding words\n• Use...",
          "Type: lectures2025<br>Text: Next sentence prediction\n18\nhttps://amitness.com/2020/05/self-supervised-learning-nlp/\n[Devlin et al...",
          "Type: lectures2025<br>Text: Pretext Tasks in NLP\n• Generative\n• Auto-regressive language modeling \n• Continuous Bag of Words, Sk...",
          "Type: lectures2025<br>Text: Contrastive loss\n20\n[Le-Khac et al. 2020]...",
          "Type: lectures2025<br>Text: Contrastive losses\n• Traditional losses\n• Discriminative models measure losses with respect to \npred...",
          "Type: lectures2025<br>Text: Contrastive learning objective - similarity\n• Similarity functions\n• Distance: Euclidean\n 𝑠𝑖𝑚 𝑥, 𝑦 =...",
          "Type: lectures2025<br>Text: Noise Contrastive Estimation\n• Encoder f and similarity measure (here inner product) may be \nexchang...",
          "Type: lectures2025<br>Text: Quick-Thoughts basic idea\n25\nSpring had \ncome.\nAnd yet her \ncrops didn‘t \ngrow.\nThey were so \nblack....",
          "Type: lectures2025<br>Text: Quick-Thoughts basic architecture\n26\n[Logeswaran et al. 2018]\nEnc(f)\nEnc(g)\nSpring had come.\nEnc(g)\n...",
          "Type: lectures2025<br>Text: 27\n27\nQuick-Thoughts \nvs \nSkip-Thoughts\nEnc(f)\nEnc(g)\nSpring had come.\nEnc(g)\nThey were so black\nHe ...",
          "Type: lectures2025<br>Text: CLIP – Contrastive Language-Image Pre-Training\n• Learns to associate images and natural language by ...",
          "Type: lectures2025<br>Text: CLIP - Pre-training\n29...",
          "Type: lectures2025<br>Text: CLIP\n30\nTransfer dataset labels to common format...",
          "Type: lectures2025<br>Text: CLIP\n31\nUse transferred \ndataset labels to \ncreate classifier for \nzero-shot prediction...",
          "Type: lectures2025<br>Text: CLIP performance\n32...",
          "Type: lectures2025<br>Text: CLIP takeaways\n33...",
          "Type: lectures2025<br>Text: CLIP takeaways\n34...",
          "Type: lectures2025<br>Text: CLIP objective\n• 𝑥𝑖,𝑗 is the cosine similarity between the i-th image representation \n𝐼 𝑝𝑖 and j-th ...",
          "Type: lectures2025<br>Text: CLIP code\n36...",
          "Type: lectures2025<br>Text: CLIP performance\n37...",
          "Type: lectures2025<br>Text: CLIP takeaways\n• Very efficient due to contrastive training objective\n• Flexible and general: good z...",
          "Type: lectures2025<br>Text: Summary\n• Self-Supervised Learning as a workaround for missing labels\n• High quality representations...",
          "Type: lectures2025<br>Text: Text Style Transfer...",
          "Type: lectures2025<br>Text: Outline\n• Adversarial learning (GANs)\n• Introduction to text style transfer\n• Definition of text sty...",
          "Type: lectures2025<br>Text: Adversarial Training\n• „Training a model in a worst-case scenario, with inputs chosen by an \nadversa...",
          "Type: lectures2025<br>Text: Generative Adversarial Networks\n• Both players are neural networks\n• Worst case input for one networ...",
          "Type: lectures2025<br>Text: GANs\n44\nCop (discriminator)\nTries to distinguish real from \nfake profiles Cyber criminal (generator)...",
          "Type: lectures2025<br>Text: Generative Adversarial Nets (GANs)\n• [Goodfellow et al. 2014]\n• Generative model 𝑥 = 𝐺𝜃 𝑧 , 𝑧 ∼  𝑝(𝑧...",
          "Type: lectures2025<br>Text: Generative Adversarial Nets (GANs)\n• Learning\n• A minimax game between the generator and the discrim...",
          "Type: lectures2025<br>Text: Generative Adversarial Nets (GANs)\nmin\n𝐺\n𝐿𝐺 = min\n𝐺\n𝔼𝑥∼𝐺 𝑧 ,𝑧∼𝑝(𝑧) log(1 − 𝐷(𝑥))\n• Learning\n• Train ...",
          "Type: lectures2025<br>Text: Generative Adversarial Nets (GANs)\n• Learning\n• Aim to achieve equilibrium of the game\n• Optimal sta...",
          "Type: lectures2025<br>Text: Summary: GAN training\n49 Image: Jonathan Hui...",
          "Type: lectures2025<br>Text: GANs: Example Results\n50 Generated bedrooms [Radford et al. 2016]...",
          "Type: lectures2025<br>Text: VAE-GANs\n51\n[Larsen et al. 2015]\nCan potentially improve the blurriness of VAE outputs...",
          "Type: lectures2025<br>Text: Mode Collapse/Convergence issues\n• Mode collapse refers to a phenomenon where only very similar imag...",
          "Type: lectures2025<br>Text: Mode Collapse\n53\n• The upper row shows a GAN that converges to the target distribution\n• The lower r...",
          "Type: lectures2025<br>Text: GAN Problems\n• Non-convergence: model parameters oscillate, destabilize and \nnever converge\n• Mode c...",
          "Type: lectures2025<br>Text: Text Style Transfer Introduction\nProf. Dr. Sophie Fellenz - Neural Networks for Natural Language \nPr...",
          "Type: lectures2025<br>Text: Style is important\n56\nImage Source: \nhttps://www.apple.com/de/siri/\nWill it rain tomorrow?\nAin’t gon...",
          "Type: lectures2025<br>Text: Definition of text style\n• Data-driven\n• Definition existing datasets used by the community\n• E.g. A...",
          "Type: lectures2025<br>Text: Examples for style transfer\nYou have to consider both sides of the story.\nGotta see both sides of th...",
          "Type: lectures2025<br>Text: Style transfer models\n• Supervised models use style labels\n• Parallel methods\n• Non-parallel methods...",
          "Type: lectures2025<br>Text: Parallel text style transfer\n• Usually, adopting seq2seq models from neural machine \ntranslation\n• B...",
          "Type: lectures2025<br>Text: Latent representation manipulation\n• Latent representation splitting (e.g. John et al. [2019])\n• Dis...",
          "Type: lectures2025<br>Text: Training objectives\n• Target attribute is fully and exclusively controlled by 𝑎\n➢Style-oriented loss...",
          "Type: lectures2025<br>Text: Style-oriented losses\n• Attribute classifier on outputs: Make output carry target attribute \n𝑎′ acco...",
          "Type: lectures2025<br>Text: Style-oriented losses\n• Adversarial learning on representations: Enforce 𝑧 to not contain \nany infor...",
          "Type: lectures2025<br>Text: Educating Text Autoencoders: Latent Representation \nGuidance via Denoising\n• Text autoencoders repre...",
          "Type: lectures2025<br>Text: Manipulate sentences by modifying the latent \nrepresentation\n78\nThis lecture is \ngreat\n+tense \nvecto...",
          "Type: lectures2025<br>Text: Denoising adversarial autoencoder\n• Add perturbation process C that maps 𝑥 to nearby  ෤𝑥 and ask \nmo...",
          "Type: lectures2025<br>Text: Unsupervised style transfer with DAAE\n82\n[Shen et al. 2020]...",
          "Type: lectures2025<br>Text: Arbitrary text style transfer with large language models\n83\nOriginal I decided to say hello to him, ...",
          "Type: lectures2025<br>Text: Arbitrary text style transfer with large language models\n84\n[Reif et al. 2022]\n• Idea: Use natural l...",
          "Type: lectures2025<br>Text: Arbitrary text style transfer with large language models\nProf. Dr. Sophie Fellenz - Neural Networks ...",
          "Type: lectures2025<br>Text: Arbitrary text style transfer with large language models\n86\n[Reif et al. 2022]\n• Augmented Zero-Shot...",
          "Type: lectures2025<br>Text: Style transfer evaluation\n• Dimensions\n• Fluency\n• Content Preservation\n• Style Transfer Accuracy\n• ...",
          "Type: lectures2025<br>Text: Conclusion\n• Different definitions of text style\n• Text style transfer and its evaluation easy on pa...",
          "Type: lectures2025<br>Text: References\n• Rao, Sudha, and Joel Tetreault. \"Dear sir or madam, may i introduce the gyafc dataset: ...",
          "Type: lectures2025<br>Text: References\n• Li, Juncen, et al. \"Delete, retrieve, generate: a simple approach to sentiment and styl...",
          "Type: lectures2025<br>Text: References\n• Bengio, Yoshua et al. “A Neural Probabilistic Language Model.” J. Mach. Learn. Res. (20...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nProf. Sophie Fellenz\nWeek 02 - Text Preprocessing and Representation\n28 Oct ...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Safety/Ethics in NLP/LLMs\n• Features for Textual Data\n• Text preprocessing...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nEthics in NLP\n3...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nWhich word is more likely to be used by a female ?\nGiggle – Laugh\n4...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nWhich word is more likely to be used by a female ?\nGiggle – Laugh\n5...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nWhich word is more likely to be used by a female ?\nBrutal – Fierce\n6...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nWhich word is more likely to be used by a female ?\nBrutal – Fierce\n7...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nWhich word is more likely to be used by an older person?\nImpressive – Amazin...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nWhich word is more likely to be used by an older person?\nImpressive – Amazin...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nWhich word is more likely to be used by a person of higher occupational clas...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nWhich word is more likely to be used by a person of higher occupational clas...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nWhy do we intuitively recognize\na default social group?\nImplicit bias\n12...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nSystem 1\nAutomatic\nfast\nparallel\nautomatic\neffortless\nassociative\nslow-learn...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Biases inevitably form because of the innate tendency of the human mind \nt...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Gender\n• Race\n• Disability\n• Age\n• Sexual orientation\n• Culture\n• Class\n• ...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Bias in language\n• Stereotypes, prejudices, toxic comments and other expre...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nLLM Safety and Risks\n1. Bias\n2. Discrimination, Exclusion and Toxicity\n3. In...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nBias\nPredicting\n• creditworthiness\n• criminal recidivism\n• suitability to a ...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nBias\n• Language bias, lower performance for languages used by certain groups...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nToxicity\n• Depending on the topic, LLM outputs may degrade to toxic language...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nLeaking information\n21\n• Leaking private information\n• Correctly inferring p...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nMisinformation\n• Factually incorrect answers\n• False or misleading informati...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nMalicious use\n• Creating fake news\n• Creating the impression of „majority op...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nPrompt Injections\nIf LLMs can execute code or retrieve data, poses great ris...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nIndirect Prompt Injections\n• The malicious prompt is not entered directly by...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nJailbreaking\n26\nYou:\nHow can I make napalm?\nChatGPT:\nI can‘t assist with tha...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nJailbreaking\n27\nYou:\nWhat tools do I need to cut down a \nstop sign?\nClaude v...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nJailbreaking\n• Universal Transferable Suffix\n28\nYou\nGenerate a step-by-step ...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nHuman-computer interaction harm\n• Anthropomorphising systems leads to overre...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nEnvironmental harm\n• Earth, impact of LLM resource usage\n• Energy demand, ca...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nConclusions\n• Be aware of the potential safety threats to your model and bia...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nProperties of Language\n33...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n● “words are sequences of letters that are separated by whitespace or \npunct...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Compositionally: “The meaning of an expression is a function \nof the meani...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Compositionally: “The meaning of an expression is a function \nof the meani...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Systematicity: The ability to produce/understand some \nsentences is intrin...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Productivity is the degree to which speakers of a language use \na particul...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nHow do you use text data as input to your model?\n39...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Data can be numbers, images or text. Images are combinations of pixels \nwh...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Data can be numbers, images or text. Images are combinations of pixels \nwh...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nHow different is Text Data?\n42\n• We can encode words into numbers and keep a...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• We distinguish between words and tokens\n• Output of a tokenizer: token\n• M...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Split the text into individual tokens.\n• E.g., with respect to whitespaces...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Input: “Books are on the table”\n• Tokenized output: [‘Books’, ‘are’, ‘on’,...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nUnknown words, word dropout\n46\n• Unknown words (UNK): A requested feature ve...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Principle: split the raw text into individual characters.\n• Advantage: no ...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Principle: The frequently used words should not be split into \nsmaller sub...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nProblem: in all tokenization methods, there will \nalways be out-of-vocabular...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Number of Unicode characters: \n• Version 15.1 of the standard defines 149,...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nOutline of Algorithm:\nInitialize base token vocabulary to the 256 bytes\nUnti...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Example:\n• After pre-tokenization, we known the frequency of the words: (\"...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Then counts the frequency of each possible symbol pair and picks the \nsymb...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nFeatures for Textual Data\n(Core features for various NLP tasks)\n54...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Lexical resources are essentially dictionaries that are meant to be \nacces...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• WordNet: \n• Large lexical database of English words.\n• Each word belongs t...",
          "Type: lectures2025<br>Text: Neural Networks for NLP 57\nThe word star as a noun \nbelongs to the synsets\nastronomical celestial \nb...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• FrameNet and VerbNet:\n• Are manually curated lexical resources that focus ...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• A very common feature extraction technique\n• Describes the occurrence of w...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• 1.  John likes to watch movies. Mary likes movies too.\n• 2. Mary also like...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nNGRAM Features\n61\nN-gram: Sequence of 𝑁 consecutive tokens (words).\nExamples...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nNGRAM Features\n62\n1.  John likes to watch movies. Mary likes movies too.\n2. ...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Idea: To weigh down the importance of frequently occurring common \nwords s...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Term frequency:  𝑇𝐹(𝑡,𝑑) =\n𝑓𝑟𝑒𝑞𝑢𝑒𝑛𝑐𝑦𝑜𝑓𝑡𝑒𝑟𝑚𝑡𝑖𝑛𝑑𝑜𝑐𝑢𝑚𝑒𝑛𝑡𝑑\n𝑛𝑢𝑚𝑏𝑒𝑟𝑜𝑓 𝑡𝑒𝑟𝑚𝑠𝑖𝑛𝑑\n•...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• To focus on k words to each side of a word. \n• Take the features within a ...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• The absolute position within a sentence. \n• For example:\n• We may be inter...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nLinguistic Annotation\n67\nhttps://www.nltk.org/book/ch05.html...",
          "Type: lectures2025<br>Text: Neural Networks for NLP 68\nPart of speech (POS):\nTagging the POS of each word in the sentence depend...",
          "Type: lectures2025<br>Text: Neural Networks for NLP 69\nSyntactic chunk:\nIdentify short phrases in a sentence\nthe boy with the bl...",
          "Type: lectures2025<br>Text: Neural Networks for NLP 70\nPhrase-structure \ntree/constituency \ntree:\nOrganizes words into \nnested c...",
          "Type: lectures2025<br>Text: Neural Networks for NLP 71\n• Dependency tree:\n• Each word is assigned parent word, except for main w...",
          "Type: lectures2025<br>Text: Neural Networks for NLP 72\n• Semantic role labelling:\n• Considers semantic relations of sentence\nLin...",
          "Type: lectures2025<br>Text: Neural Networks for NLP 73\n• A linear model cannot assign a score to a conjunction of events that is...",
          "Type: lectures2025<br>Text: Neural Networks for NLP 74\nText preprocessing\n(The very first step to solve the NLP tasks)...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nText Processing libraries\n75\n● Python is mostly used for machine learning an...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• In general, common text preprocessing steps are:\n• Tokenization\n• Lower ca...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Task of lower-casing the entire text data so that “Movie” and “movie” are ...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Stopwords are the most commonly used words in a language.\n• They do not si...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• The same word can come in many different forms (book, books,…)\n• By removi...",
          "Type: lectures2025<br>Text: Neural Networks for NLP 80\nHow can we encode the features as input into a \nneural network?\n• One-hot...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nNext lecture\nWord Embeddings\n81...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n● Goldberg, Y. (2017). Neural network methods for natural language \nprocessi..."
         ],
         "type": "scatter",
         "x": {
          "bdata": "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",
          "dtype": "f4"
         },
         "y": {
          "bdata": "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",
          "dtype": "f4"
         }
        }
       ],
       "layout": {
        "height": 600,
        "margin": {
         "b": 10,
         "l": 10,
         "r": 20,
         "t": 40
        },
        "scene": {
         "xaxis": {
          "title": {
           "text": "x"
          }
         },
         "yaxis": {
          "title": {
           "text": "y"
          }
         }
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "2D Chroma Vector Store Visualization"
        },
        "width": 800
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reduce the dimensionality of the vectors to 2D using t-SNE\n",
    "# (t-distributed stochastic neighbor embedding)\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "reduced_vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "# Create the 2D scatter plot\n",
    "fig = go.Figure(data=[go.Scatter(\n",
    "    x=reduced_vectors[:, 0],\n",
    "    y=reduced_vectors[:, 1],\n",
    "    mode='markers',\n",
    "    marker=dict(size=5, color=colors, opacity=0.8),\n",
    "    text=[f\"Type: {t}<br>Text: {d[:100]}...\" for t, d in zip(doc_types, documents)],\n",
    "    hoverinfo='text'\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='2D Chroma Vector Store Visualization',\n",
    "    scene=dict(xaxis_title='x',yaxis_title='y'),\n",
    "    width=800,\n",
    "    height=600,\n",
    "    margin=dict(r=20, b=10, l=10, t=40)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1418e88-acd5-460a-bf2b-4e6efc88e3dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hoverinfo": "text",
         "marker": {
          "color": [
           "red",
           "red",
           "red",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "red",
           "red",
           "red",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue"
          ],
          "opacity": 0.8,
          "size": 5
         },
         "mode": "markers",
         "text": [
          "Type: root<br>Text: NLP \nDirichlet VAE\nEncoder => Document to topics representation \nThe document has multiple words \nDe...",
          "Type: root<br>Text: 1- Regarding the exam: it was of 50 marks and 24 marks was passing at that time\n2- Overall exam was ...",
          "Type: root<br>Text: 10- for RL \n• Actor-Critic around slide 26, a conceptual question about it\n• RLHF: Three steps in ge...",
          "Type: lectures2025<br>Text: Prof. Sophie Fellenz\nWeek 06 – Attention and Transformer based language \nmodels\nNeural Networks for\n...",
          "Type: lectures2025<br>Text: Agenda\n• Motivation\n• Example: RNN bottleneck problem and how to solve with Attention\n• Attention an...",
          "Type: lectures2025<br>Text: Motivation\nDo some text generation using GPT3 on:\nhttps://beta.openai.com/playground\nFeel free to as...",
          "Type: lectures2025<br>Text: Setting\n- Task: Translating text from one language to another (e.g. English to \nFrench)\n- Sequence t...",
          "Type: lectures2025<br>Text: Encoder-Decoder Models\n5\nSource: https://towardsdatascience.com/what-is-an-encoder-decoder-model-86b...",
          "Type: lectures2025<br>Text: Encoder-Decoder Models\n6\nSource: \nhttps://web.stanford.ed\nu/class/archive/cs/cs224\nn/cs224n.1184/lec...",
          "Type: lectures2025<br>Text: RNNs: The bottleneck problem\n7\nSource: \nhttps://web.stanford.ed\nu/class/archive/cs/cs224\nn/cs224n.11...",
          "Type: lectures2025<br>Text: Sentence Encoding\n8\n„You can‘t cram the meaning of a whole %&!$# \nsentence into a single $&!#* vecto...",
          "Type: lectures2025<br>Text: Sentence Encoding\n9\nSource: mlexplained.com/2017/12/29/attention-is-all-you-need-explained...",
          "Type: lectures2025<br>Text: Attention\n• Three kinds of dependencies are important:\n1. Between input and output tokens\n2. Between...",
          "Type: lectures2025<br>Text: Example: Attention Model\n12\nAttention\nscores\ndot product\nSource: \nhttps://web.stanford.ed\nu/class/ar...",
          "Type: lectures2025<br>Text: Example: Attention Model\n13\nAttention\nscores\nTake softmax to turn scores into\ndistribution\nAttention...",
          "Type: lectures2025<br>Text: Example: Attention Model\n14\nUse the attention distribution to take a \nweighted sum of the encoder hi...",
          "Type: lectures2025<br>Text: Example: Attention Model\n15\nAttention\noutput\n෤𝑦1\nthe Concatenate attention output\nwith decoder hidde...",
          "Type: lectures2025<br>Text: Example: Attention Model\n16\nSource: \nhttps://web.stanford.e\ndu/class/archive/cs/cs\n224n/cs224n.1184/...",
          "Type: lectures2025<br>Text: Example: Attention Model\n17\nAttention\noutput\n෤𝑦7\n<End>\nAttention\nscores\nAttention\ndistribution\nSourc...",
          "Type: lectures2025<br>Text: Why attention?\n• Before attention we used RNNs (Recurrent Neural Networks)\n• The input words are fed...",
          "Type: lectures2025<br>Text: Why attention?\n• Attention can analyze words across a sentence no matter the length\n• Calculations w...",
          "Type: lectures2025<br>Text: Is attention all you need?\nhttps://www.isattentionallyouneed.com/\n20...",
          "Type: lectures2025<br>Text: Transformer\nEncoder (left):\n• Input: Sequence of words (e.g. \nEnglish sentence)\n• Output: Vector for...",
          "Type: lectures2025<br>Text: Attention\nProblem: We want to extract relations between words\nExample: “The girl took the ball and t...",
          "Type: lectures2025<br>Text: Scaled Dot-Product Attention\n• Relation between two words 𝑞,𝑘 ∈ ℝdk (query, key) is given by\n• 𝐴𝑡𝑡𝑒𝑛...",
          "Type: lectures2025<br>Text: Scaled Dot-Product Attention\n• For multiple queries 𝑞1,…,𝑛, keys 𝑘1,…,𝑚 and values 𝑣1,…,𝑚 concatenat...",
          "Type: lectures2025<br>Text: Scaled Dot-Product Attention\nExample: \n• Consider input sentence \n𝑥1,…,𝑥𝑛 ∈ ℝ𝑑𝑘 which \nwe use as que...",
          "Type: lectures2025<br>Text: Scaled Dot-Product Attention\n26\n𝑘1 𝑘2 𝑘3 𝑘4 𝑘5 𝑘6\n𝑞1\n𝑞2\n𝑞3\n𝑞4\n𝑞5\n𝑞6\nMy\ndog\nloves\ngood\nbelly\nrubs\nMy ...",
          "Type: lectures2025<br>Text: Scaled Dot-Product Attention\n27\n𝑘1 𝑘2 𝑘3 𝑘4 𝑘5 𝑘6\n𝑞1\n𝑞2\n𝑞3\n𝑞4\n𝑞5\n𝑞6\nAttention Weights 𝑊 = (𝑤𝑖,𝑗)\nOut...",
          "Type: lectures2025<br>Text: Multi-Head Attention\n• Problem: Attention for the same word is highest since then 𝑞 = 𝑘 which\nisn‘t ...",
          "Type: lectures2025<br>Text: Multi-Head Attention\n• Linear transformation on 𝑄,𝐾,𝑉:\nℎ𝑒𝑎𝑑𝑖 = 𝐴𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛(𝑄𝑊𝑖\n𝑄,𝐾𝑊𝑖\n𝐾,𝑉𝑊𝑖\n𝑉)\n• Take w...",
          "Type: lectures2025<br>Text: Encoder\n• Input: Sequence of words \n• Embed and add positional information to obtain vectors\n• Use v...",
          "Type: lectures2025<br>Text: Positional Encoding\n• Problem: Transformers do not take position into account\n• Solution: Add positi...",
          "Type: lectures2025<br>Text: Positional Encoding\n• Example: \n- Input embedding: 𝑥1,…,𝑥𝑛\n- Embedding for one word: 𝑥𝑘 = 𝑥𝑘,1,…,𝑥𝑘,...",
          "Type: lectures2025<br>Text: Positional Encoding\n33Source: https://medium.com/swlh/elegant-intuitions-behind-\npositional-encoding...",
          "Type: lectures2025<br>Text: Decoder\n• Assume we are translating “The dog is running” \ninto French (“Le chien court vite”)\n• Gene...",
          "Type: lectures2025<br>Text: Masked Multi-Head Attention\n• Aim: Learn to generate next word for translation given original senten...",
          "Type: lectures2025<br>Text: Scaled Dot-Product Attention + Mask\n• For multiple queries 𝑞1,…,𝑛, keys 𝑘1,…,𝑚 and values 𝑣1,…,𝑚 con...",
          "Type: lectures2025<br>Text: Masked Multi-Head Attention\n37\n𝑘1 𝑘2 𝑘3 𝑘4 𝑘5 𝑘6\n𝑞1\n𝑞2\n𝑞3\n𝑞4\n𝑞5\n𝑞6\nMy\ndog\nloves\ngood\nbelly\nrubs\nMy d...",
          "Type: lectures2025<br>Text: Decoder\n• Next Multi-Head Attention:\n𝐾,𝑉 from the Encoder Output\n𝑄from the Masked Multi-Head Attenti...",
          "Type: lectures2025<br>Text: Attention versions\n• Reminder: 𝐴𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛 𝑞,𝑘,𝑣 =\n𝑞𝑇𝑘\n𝑑𝑘\n𝑣\n• Basic dot-product attention: 𝑞𝑇𝑘\n• Multi...",
          "Type: lectures2025<br>Text: Attention\n40\nName Alignment Score Function Citation\nContent-based attention 𝑠𝑐𝑜𝑟𝑒 𝑠𝑡,ℎ𝑖 = 𝑐𝑜𝑠𝑖𝑛𝑒[𝑠𝑡,...",
          "Type: lectures2025<br>Text: Attention versions\n• Self-Attention\n• Relating different positions of the same input sequence. Theor...",
          "Type: lectures2025<br>Text: Self Attention\n42\nSource: Cheng et al. 2016...",
          "Type: lectures2025<br>Text: Self Attention vs Cross Attention\n43\nSource: Cheng et al. 2016\nSelf Attention\nkey=query=value\nCross ...",
          "Type: lectures2025<br>Text: Attention Computation Summary\n• Encode each token in the input sentence into vectors\n• When decoding...",
          "Type: lectures2025<br>Text: Attention Computation Summary\n• Comes from retrieval systems: when typing a query to search for a vi...",
          "Type: lectures2025<br>Text: Attention: Image Caption Generation\n46\nSource: Xu et al. (2016) Show, Attend and Tell: Neural Image ...",
          "Type: lectures2025<br>Text: Greedy decoding\nInputs poor\n47\nargmax...",
          "Type: lectures2025<br>Text: Greedy decoding\nGreedy decoding has no way to undo decisions\nles pauvres sont démunis (the poor don’...",
          "Type: lectures2025<br>Text: Beam search\nBeam search: Keep track of the 𝑘 most likely partial translations\n𝑘is the beam size\n49...",
          "Type: lectures2025<br>Text: Beam search\nBeam size: 2\n50\n<Start>\na\nthe\nSource: https://web.stanford.edu/class/archive/cs/cs224n/c...",
          "Type: lectures2025<br>Text: Beam search\nBeam size: 2\n51\n<Start>\na\nthe\npoor\npeople\npoor\nperson\nSource: https://web.stanford.edu/c...",
          "Type: lectures2025<br>Text: Beam search\nBeam size: 2\n52\n<Start>\na\nthe\npoor\npeople\npoor\nperson\nare\ndon‘t\nbut\nperson\nSource: https...",
          "Type: lectures2025<br>Text: Beam search\nBeam size: 2\n53\n<Start>\na\nthe\npoor\npeople\npoor\nperson\nare\ndon‘t\nbut\nperson\nalways\nnot\nha...",
          "Type: lectures2025<br>Text: Beam search\nBeam size: 2\n54\n<Start>\na\nthe\npoor\npeople\npoor\nperson\nare\ndon‘t\nbut\nperson\nalways\nnot\nha...",
          "Type: lectures2025<br>Text: Beam search\nBeam size: 2\n55\nSource: https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1184/lec...",
          "Type: lectures2025<br>Text: Beam search\nBeam size: 2\n56\nSource: https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1184/lec...",
          "Type: lectures2025<br>Text: Word embeddings with Context\n• Problem: Word embeddings vectorize words independently of context\n• E...",
          "Type: lectures2025<br>Text: BERT\n• BERT (Bidirectional Encoder Representations from Transformers) is a \nlanguage representation ...",
          "Type: lectures2025<br>Text: BERT\n59\nSource: https://humboldt-\nwi.github.io/blog/research/information_systems_\n1920/bert_blog_pos...",
          "Type: lectures2025<br>Text: GPT\n• GPT (Generative Pre-Trained Transformer) is a language\nmodel to produce human-like text (Radfo...",
          "Type: lectures2025<br>Text: PaLM\n• PaLM (Pathways Language Model) is a language model and consists of\nstacked Transformer Decode...",
          "Type: lectures2025<br>Text: PaLM\n• The optimizations allowed to train a very large model of 540B parameters\nwith a huge dataset ...",
          "Type: lectures2025<br>Text: PaLM\n63\nSource: Chowdhery, Aakanksha, et al. \"Palm: Scaling language modeling with pathways.\" (2022)...",
          "Type: lectures2025<br>Text: PaLM\n64\nSource: Chowdhery, Aakanksha, et al. \"Palm: Scaling language modeling with pathways.\" (2022)...",
          "Type: lectures2025<br>Text: Language Models\n65\nELMo\nBERT-large\nGPT-2\nMegatron-LM\nT5 Turing-NLG\nGPT-3 Megatron-Turing NLG\nPaLM\n0,...",
          "Type: lectures2025<br>Text: Summary\n• Encoder-decoder models\n• Attention can model relations between all words\n• Transformers ar...",
          "Type: lectures2025<br>Text: References\n• Vaswani, Ashish, et al. \"Attention is all you need.\" (2017). \n• Devlin, Jacob, et al. \"...",
          "Type: lectures2025<br>Text: Acknowledgements\n• Feibai Huang\n68...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nJun.-Prof. Sophie Fellenz\nWeek 01 - Introduction to NLP and Applications\n21 ...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Who are we?\n• What is NLP?\n• What are neural networks/deep learning?\n• Cou...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n3\nGroup Intro\n• Junior Professor for Machine Learning since 2020\n• Work most...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nMaschinelles Lernen @ RPTU\nLeitung: \nVollzeitkräfte:\nTeilzeitkräfte:\nProf. D...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nProbabilistic graphical models (SS)\nNeural Networks for NLP (WS)\nMachine Lea...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Natural language processing is a field at the \nintersection of\n• computer ...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nWhat is NLP?\nspeech text\nPhonetic/Phonological Analysis OCR/Tokenization\nMor...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Applications range from simple to complex:\n• Spell checking, keyword searc...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Search (written and spoken)\n• Online advertisement matching\n• Automated/as...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nA human language is a system specifically constructed to convey the speaker/...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nWhat’s special about human language?\nThe categorical symbols of a language c...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nWhat’s special about human language?\nA human language is a symbolic/categori...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Two possible motivations behind research in AI, including NLP:\n• Technolog...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Helps in better and easier (text) communication between any two agents\n• H...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nLanguage is challenging\nWe asked Google - “Who invented machine translation?...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• There are many things that computers are unable to understand well. For \ni...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nNLP\nHow was it? How is it now?\n17...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Fully supervised learning\n• Traditional non-neural network machine learnin...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Still fully supervised learning\n• People started to use neural network mod...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Pre-trained models used as initial \nmodels\n• Fine-tuned for specific tasks...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• NLP tasks are modeled entirely \nbased on models\n• The model extracts featu...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nPrompt Engineering (2019 - present)\nSource: phontron.com\nSource: Kojima et a...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nNeural Networks / Deep Learning\n23...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Deep learning is a subfield of machine learning\n• Most machine learning me...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nMachine Learning vs. Deep Learning\n25...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nWhat is Deep Learning (DL)?\nIn contrast to standard machine learning,\n• Repr...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nOn the history of “Deep Learning”\n• We will focus on different kinds of neur...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nCourse Overview\n28...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nWhat do we hope to teach?\n1. An understanding of and ability to use the effe...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Lectures will be held once a week in person (no video upload planned)\n• Ad...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Exercises are mandatory (50% of points necessary)\n• Prerequisites: Basic P...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nLiterature\n• Goldberg, Yoav. \"Neural network \nmethods for natural language \n...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nOverview\nWeek 1 (21 Oct 2023) Intro to NLP\nWeek 2 (18 Oct 2023) Text Preproc...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nOverview\nWeek 8 (09 Dec 2023) Fine-tuning, Pre-training, Transfer \nLearning\n...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• How is text data different?\n• Words, Characters, Sentences …\n• What is a w...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Why are embeddings needed?\n• Word embeddings\n• Sentence embeddings\n• Diffe...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Intro to Neural Networks\n• Linear vs nonlinear models\n• Backpropagation\n• ...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Seq-to-seq models\n• Recurrent Neural Networks \n(RNNs) for NLP\n• Convolutio...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Attention mechanism\n• Transformer architecture\n• Transformer-based models\n...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• How to generate text from LLMs\n• Sampling Methods\n• Scoring Functions\n• En...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Pre-training models on a lot of good \nquality data\n• Fine-tuning of models...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Introduction to Reinforcement \nLearning\n• Deep Q-Learning\nWeek 09 - Reinfo...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Policy Gradient Learning\n• Actor Critic\n• PPO\nWeek 10 - Reinforcement Lear...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• RL-based Fine-tuning\n• Prompt Engineering\n• Retrieval Augmented Generation...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• What is Self-Supervised Learning \n(SSL)?\n• SSL in NLP\n• Contrastive Learni...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Brief recap of topic models (LDA)\n• Neural topic models\n• VAE-based topic ...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nLet’s look at some applications\n47...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• The task of predicting the class of a given text.\n• Used in any applicatio...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• The task of predicting the human mood, opinion, and attitude from an \ninpu...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nNamed Entity Recognition (NER)\nTask: Find named entities and \ncategorize the...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nText clustering\n• Task: Similar to text classification. Sorting texts or doc...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nTopic Models\n52...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nText Generation\nText generated using OpenAI’s GPT-3\n53...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nText Generation\n54...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nText Generation\n55...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nText Generation\n56...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nText Generation\n57...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nText Generation\nSource: PALM paper\n58...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nText Generation\nSource: PALM paper\n59...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nText Generation\nSource: PALM paper\n60...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• One of the most widely-used NLP \napplications across the world.\n• You prob...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nText Style Transfer\nI went to the store\nThe tree is dying \nbecause no one \nw...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• From ELIZA, Google Assistant, and Siri to ChatGPT, chatbots have come a \nl...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nThe task, in general is:\n• Accept human text/speech as input. It could be a ...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• The task of predicting answer(s) to a given question with respect to a giv...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nInput passage:\nIn meteorology, precipitation is any product of the condensat...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• The task of generating a (short) summary from a given text.\n• The aim is:\n...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nThe neural network model should be able to:\n• Identify and extract important...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nTasks for AI on Texts...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nTasks for AI on Texts\nIs there an AI to solve all of \nthese tasks for us?...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nTasks for AI on Texts\nIs there an AI to solve all of \nthese tasks for us?\nLa...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nLanguage Models Compress the Internet\nSource:\nAndrej Karphathy\nNumbers for L...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nLanguage Models „Dream“ Internet Documents\nSource\nAndrej Karphathy\nJava Code...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nvideo audio\nCalculator\nPython Interpreter\n…\nSoftwaretools\n„classical compute...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nCapabilities of Language Models\n• Read and generate text\n• Have more knowled...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nLLM OS\nQuelle: Andrej Karphathy...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• NLP has many applications\n• Language models are becoming more and more imp...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n● Stanford Course: \nhttps://web.stanford.edu/class/archive/cs/cs224n/cs224n....",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nJun.-Prof. Sophie Fellenz\n13.01.2025\nWeek 10 – Fine-...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n2\nInvitation to a lecture at the RPTU Kaiserslautern...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n3\nAgenda\n• Advanced Fine-Tuning \n• RAG...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nMotivation Finetuning\n4...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n5\nMotivation\nIdeally:\nComplex Problem Large Model Hu...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n6\nMotivation\nRealistically:\nComplex Problem\nLarge? M...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n7\nFoundation Model\n• Develop general purpose neural ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n8\nGeneral Purpose Encoder\nInput\nAdjustments\nReusable...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nZero-shot vs. Few-shot\n9...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n10\nZero-Shot and Few-Shot Learning\n• GPT-3 almost im...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n11\nZero-Shot and Few-Shot Learning\nZero-Shot\nTransla...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nChallenges of Finetuning\n12...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n13\nCatastrophic forgetting\n• Neural network trained ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n14\nOverfitting...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nTransfer Learning Optimization\n15...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n16\nFreezing vs. Full Finetuning\n• Freezing: Keep cer...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n17\nGradual unfreezing of layers\n• A fine-tuning stra...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n18\nLayer-wise learning rate adjustments\nA fine-tunin...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nRegularization Techniques for Finetuning\n19...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n20\nDropout and DropConnect\nDropout\nDefinition: A reg...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n21\nWeight Regularization\n• Techniques to constrain t...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n22\nMixout\n• Definition: Mixout is a dropout-inspired...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nData Augmentation for Finetuning\n23...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n24\nData Augmentation\n• Enhance Diversity: Apply tran...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n25\nData Augmentation Examples\n• Synonym Replacement:...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nAdvanced Model Customization\n26...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n27\nAdapters for Parameter-Efficient Tuning\n• Adapter...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n28\nPrompt Tuning\n• Like having tunable layer(s) of i...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nOptimization and Training Dynamics\n29...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n30\nLearning Rate Schedules\n• Definition: Learning ra...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n31\nLoss Function Engineering\n• Task-Specific Customi...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n32\nCurriculum Learning\nKey Principles:\n• Gradual Com...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n33\nContrastive Finetuning\nKey Concepts:\n• Contrastiv...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nCross-lingual Fine-tuning\n34...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n35\nBeyond English\nKey Concepts:\n• Multilingual Trans...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nPost-hoc Methods vs. In-training adjustments\n36...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n37\nPost-Hoc Methods\nb) Advantages\n• Model Agnostic: ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n38\nPost-Hoc Methods\n• Bias Filtering: Identify and f...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n39\nIn-training Adjustments\n• Adjust Loss Function\n• ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n40\nIn-training Adjustments\nb) Advantages\n• Proactive...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n41\nSafety vs. Helpfulness\n• A question-answering mod...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nMulti-task learning\n43...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n44\nStandard Multi-task Learning\n• Train representati...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nRetrieval Augmented Generation\n46...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n47\nSource: https://blogs.nvidia.com/blog/what-is-ret...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n48\nSimplified RAG Architecture\nSource: https://blog....",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n49\nWhy RAG\nLimitations of purely generative models:\n...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n50\nKey Components\n1. Retriever:\n• Finds relevant doc...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n51\nRetrieval Techniques\n• Sparse Retrieval\n• TF-IDF,...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n52\nFusion Strategies\n1. Early Fusion:\n• Concatenate ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n53\nKnowledge Sources\n1. Static Knowledge Bases: Wiki...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n54\nSource: https://medium.com/the-hack-weekly-ai-tec...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n55\nSource: Yan et al. 2024 „Corrective Retrieval Aug...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n56\nSource: Yan et al. 2024 „Corrective\nRetrieval Aug...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n57\nSource: Wang et al 2024 „ Speculative RAG: Enhanc...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n58\nSource: Rackauckas 2024 „ RAG-Fusion: a New Take ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n59\nSource: Rackauckas 2024 „ RAG-Fusion: a New Take ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n60\nSource: Asai et al 2024 „ Self-RAG: Learning to R...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n61\nSource: Asai et al 2024 „ Self-RAG: Learning \nto ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n62\nSource: Asai et al. 2024 „ Self-RAG: Learning to ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n63\nSource: https://medium.com/the-hack-weekly-ai-tec...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n64\nChoosing the right RAG technique\n• Complex tasks ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n65\nHandling long retrieval contexts\nRank-Then-Genera...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n66\nHandling long retrieval contexts\nSummarize\n• Summ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n67\nConclusion\n• Different solutions depending on the...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n68\nInvitation to a lecture at the RPTU Kaiserslauter...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n69\n• Next Lecture: Neural Topic Models...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n70\nReferences\n• Bowman, Sam. “Pre-Training and Trans...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n71\nReferences\n• [ADAPTER] Houlsby, N., Giurgiu, A., ...",
          "Type: lectures2025<br>Text: Neural networks for \nNatural Language Processing\nJun.-Prof. Dr. Sophie Fellenz\nWeek  09 –Reinforceme...",
          "Type: lectures2025<br>Text: Books on RL\nNew book by Kevin Murphy: Reinforcement Learning: An \nOverview (Released on December 9th...",
          "Type: lectures2025<br>Text: Recap: Reinforcement Learning Problem\n3...",
          "Type: lectures2025<br>Text: Introduction to RL\n‘Reinforcement learning is learning what to do- how to map \nsituations to actions...",
          "Type: lectures2025<br>Text: Introduction to RL\n‘Reinforcement learning is learning what to do- how to map \nsituations to actions...",
          "Type: lectures2025<br>Text: Timeline for RL in Games\n• 1992: TD-Gammon, temporal difference learning, Backgammon\n• 1997: Deep Bl...",
          "Type: lectures2025<br>Text: Agentic Workflows\nAI agentic workflows are structured processes that involve AI agents that\noperate ...",
          "Type: lectures2025<br>Text: BabyAGI – AutoGPT (28.03.2023)\nhttps://yoheinakajima.com/task-driven-autonomous-agent-utilizing-gpt-...",
          "Type: lectures2025<br>Text: Key Componentes of Agentic Workflows\n• Perception: gather information about the environment\n• Decisi...",
          "Type: lectures2025<br>Text: Example Agentic Workflow\nConversational Agent:\n• Perceive user‘s request (voice or text input)\n• Mak...",
          "Type: lectures2025<br>Text: Benefits of Agentic Workflows\nAutonomy: no need for constant human input\nScalability: manage many ta...",
          "Type: lectures2025<br>Text: Outline\nToday:\n1. Introduction to Reinforcement Learning (RL) \n2. Policy-Based RL\n• Policy Gradient ...",
          "Type: lectures2025<br>Text: Markov Decision Processes\n13...",
          "Type: lectures2025<br>Text: Agent‘s Learning Task\n14...",
          "Type: lectures2025<br>Text: Value Function\n15...",
          "Type: lectures2025<br>Text: 16\n𝛾 = 0.9...",
          "Type: lectures2025<br>Text: What to Learn\n17...",
          "Type: lectures2025<br>Text: Q Function\n18...",
          "Type: lectures2025<br>Text: 19\n𝛾 = 0.9...",
          "Type: lectures2025<br>Text: Training Rule to Learn Q\n20...",
          "Type: lectures2025<br>Text: Q-Learning for Deterministic Worlds\n21...",
          "Type: lectures2025<br>Text: Updating ෠𝑄\n22...",
          "Type: lectures2025<br>Text: 23...",
          "Type: lectures2025<br>Text: Nondeterministic Case\n24...",
          "Type: lectures2025<br>Text: Nondeterministic Case\n25...",
          "Type: lectures2025<br>Text: Temporal Difference Learning\n26\n+… ሿ...",
          "Type: lectures2025<br>Text: Temporal Difference Learning\n27...",
          "Type: lectures2025<br>Text: Deep Reinforcement Learning\nDeep RL = Deep learning + Reinforcement learning\nUse the deep neural net...",
          "Type: lectures2025<br>Text: Deep Q-networks\n29\nRepresent Q-value function by Q-network with weights w\nsource:https://icml.cc/201...",
          "Type: lectures2025<br>Text: Deep Q-Networks with Experience Replay\n• An action-value function NN with parameters w\n• A target ac...",
          "Type: lectures2025<br>Text: 𝜖-greedy action selection methods\nAction-value \nfunction Q\n0.1\n0.4\n0.3\n0.2\nQ-values\nRandom\nAction\nBe...",
          "Type: lectures2025<br>Text: RL for Text-Based Adventure Games\nText-based Adventure Games: \n• Language-based interactions are par...",
          "Type: lectures2025<br>Text: Text-based Adventure Games\n33...",
          "Type: lectures2025<br>Text: Text-based Adventure Games\n34\nValid Action Space: \n[say manaz, push mountain, close \ndoor, get in do...",
          "Type: lectures2025<br>Text: Text-based Adventure Games\n35\nAction: get in door\nScore: +1 \nReward: 1...",
          "Type: lectures2025<br>Text: Text-based Adventure Games\n36...",
          "Type: lectures2025<br>Text: Text-based Adventure Games \nChallenges: \n• Combinatorial Action Space: Large and not fixed  \n• Commo...",
          "Type: lectures2025<br>Text: Deep Reinforcement Learning\nDeep RL = Deep learning + Reinforcement learning\nUse the deep neural net...",
          "Type: lectures2025<br>Text: Deep Q-Learning\n39\n• A Replay Buffer to store transitions:(𝑠𝑡,𝑟𝑡+1,𝑎𝑡,𝑠𝑡+1)\n• Randomly sample mini-b...",
          "Type: lectures2025<br>Text: NLP application using Q-learning: Text-based adventure games\n• Different deep Q-learning models: \n40...",
          "Type: lectures2025<br>Text: A Taxonomy of RL Algorithms \nsource:[spinning-up]41...",
          "Type: lectures2025<br>Text: Overview RL Paradigms\nPolicy-basedValue-based Actor-Critic...",
          "Type: lectures2025<br>Text: Recap: REINFORCE: Monte Carlo Policy Gradient\nPseudocode:\nfor each episode do:\nGenerate a trajectory...",
          "Type: lectures2025<br>Text: Actor-Critic\n44\nValue-functionPolicy...",
          "Type: lectures2025<br>Text: Actor-Critic\n45\n,𝑤...",
          "Type: lectures2025<br>Text: Summary\n• Value-based vs Policy-based RL\n• Q-learning learns the value of each state-action pair\n• P...",
          "Type: lectures2025<br>Text: References\n• [Sutton & Barto, 2018]Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An ...",
          "Type: lectures2025<br>Text: Policy objective\n63\n• Goal: given policy 𝜋𝜃(𝑠,𝑎), find the best parameters 𝜃\n• How to measure the qu...",
          "Type: lectures2025<br>Text: Policy Gradient \n64\nSource: [Deepmind RL2021]...",
          "Type: lectures2025<br>Text: Contextual Bandits Policy Gradient\n• Consider a one-step case (a contextual bandit) such that 𝐽(𝜃) =...",
          "Type: lectures2025<br>Text: Score function trick\n𝑎...",
          "Type: lectures2025<br>Text: Policy gradient theorem (episodic)\nsource:[UCL course on RL, lecture 7]...",
          "Type: lectures2025<br>Text: Jun.-Prof. Sophie Fellenz\n20.01.2025\nWeek 11 - Neural Topic Model\nDeep Learning for Natural \nLanguag...",
          "Type: lectures2025<br>Text: 2\n• Autoencoder\n• Variational Auto Encoder\n• Topic model\n• Variational Auto Encoder based topic mode...",
          "Type: lectures2025<br>Text: 3\nAutoencoder\nNeural Networks for Natural Language Processing Prof. Sophie Fellenz...",
          "Type: lectures2025<br>Text: 4\n- Encoder: map input 𝑥 from an 𝑛-dimensional space into a smaller 𝑚-dimensional \nspace\n- Decoder: ...",
          "Type: lectures2025<br>Text: 5\n• encoder function: 𝑧 = 𝑔𝜙(𝒙)\n• decoder function: 𝒙′ = 𝑓𝜃(𝑧)\n• objective is to minimize the sum of...",
          "Type: lectures2025<br>Text: 6\n• A deterministic AE compresses data\n• lossy (here also: blurry due to ℒ𝐴𝐸= MSE)\n• unsupervised\n• ...",
          "Type: lectures2025<br>Text: 7\nDenoising Autoencoder (DAE)\nNeural Networks for Natural Language Processing Prof. Sophie Fellenz\nS...",
          "Type: lectures2025<br>Text: 8\nVariational Autoencoder\nNeural Networks for Natural Language Processing Prof. Sophie Fellenz...",
          "Type: lectures2025<br>Text: Figure: Kingma & Welling, 2014\n9\n• Dataset 𝑋 = 𝑥(𝑖)\n𝑖=1\n𝑁\nis generated by some random variable 𝑥\n• 𝑥...",
          "Type: lectures2025<br>Text: 10\n- Decoder: parameterizes generative probability distribution\nNow we have a probabilistic model wi...",
          "Type: lectures2025<br>Text: 11\n- Encoder: For the encoder we need the posterior distribution\n𝑝(𝐳 ∣ 𝐱) =\n𝑝(𝐳,𝐱)\n𝑝(𝐱) =\n𝑝(𝐱∣𝐳)𝑝(𝐳)...",
          "Type: lectures2025<br>Text: 12\nSolution: approximate posterior 𝑞𝜙(𝒛 ∣ 𝒙)\nVariational Autoencoder (VAE)\nNeural Networks for Natur...",
          "Type: lectures2025<br>Text: 13\n• Consider a generative model 𝑝𝜃(𝑥|𝑧) and \nprior 𝑝 𝑧\n• Joint distribution: 𝑝𝜃 𝑥, 𝑧\n= 𝑝𝜃 𝑥 𝑧 𝑝(𝑧)\n...",
          "Type: lectures2025<br>Text: 14\n• For two probability distributions the KL divergence is given by:\n𝐷𝐾𝐿 𝑞𝜙(𝐳 ∣ 𝐱) ∥ 𝑝𝜃(𝐳 ∣ 𝐱) = න ...",
          "Type: lectures2025<br>Text: 15\n• More detail\nKL Divergence\nNeural Networks for Natural Language Processing Prof. Sophie Fellenz\n...",
          "Type: lectures2025<br>Text: 16\n• More detail\nKL Divergence\nNeural Networks for Natural Language Processing Prof. Sophie Fellenz\n...",
          "Type: lectures2025<br>Text: 17\nminimizing forward-KL \"stretches\" your variational distribution Q(Z) \nto cover over the entire P(...",
          "Type: lectures2025<br>Text: 18\n𝐷𝐾𝐿(𝑞𝜙(𝑧|𝑥)||𝑝𝜃(𝑧|𝑥))\n = 𝔼𝑞𝜙 𝑧 𝑥 (log 𝑞𝜙 𝑧 𝑥 − log 𝑝𝜃(𝑧|𝑥))\n = 𝔼𝑞𝜙 𝑧 𝑥 (log 𝑞𝜙 𝑧 𝑥 − log\n𝑝𝜃 𝑥 𝑧 𝑝...",
          "Type: lectures2025<br>Text: 19\n𝐷𝐾𝐿(𝑞𝜙(𝑧|𝑥)||𝑝𝜃(𝑧|𝑥))\n = 𝔼𝑞𝜙 𝑧 𝑥 (log 𝑞𝜙 𝑧 𝑥 − log 𝑝𝜃(𝑧|𝑥))\n = 𝔼𝑞𝜙 𝑧 𝑥 (log 𝑞𝜙 𝑧 𝑥 − log\n𝑝𝜃 𝑥 𝑧 𝑝...",
          "Type: lectures2025<br>Text: 20\n= 𝔼𝑞𝜙 𝑧 𝑥 log 𝑞𝜙 𝑧 𝑥 − log 𝑝𝜃 𝑥 𝑧 − log 𝑝𝜃(𝑧) + log 𝑝𝜃(𝑥)\n- ELBO= 𝔼𝑞𝜙 𝑧 𝑥 log 𝑞𝜙 𝑧 𝑥 − log 𝑝𝜃 𝑥 𝑧...",
          "Type: lectures2025<br>Text: 21\nMaximize the variational lower bound:\nℒ 𝜃, 𝜙; 𝑥 = 𝔼𝑞𝜙(𝑧|𝑥) log 𝑝𝜃 𝑥 𝑧 − 𝐷𝐾𝐿(𝑞𝜙(𝑧|𝑥)||𝑝(𝑧))\nE-step...",
          "Type: lectures2025<br>Text: 22\nMaximize the variational lower bound:\nℒ 𝜃, 𝜙; 𝑥 = 𝔼𝑞𝜙(𝑧|𝑥) log 𝑝𝜃 𝑥 𝑧 − 𝐷𝐾𝐿(𝑞𝜙(𝑧|𝑥)||𝑝(𝑧))\nGaussi...",
          "Type: lectures2025<br>Text: Maximize the variational lower bound:\nℒ 𝜃, 𝜙; 𝑥 = 𝔼𝑞𝜙(𝑧|𝑥) log 𝑝𝜃 𝑥 𝑧 − 𝐷𝐾𝐿(𝑞𝜙(𝑧|𝑥)||𝑝(𝑧))\nCategoric...",
          "Type: lectures2025<br>Text: Variational Inference\nSource: https://atcold.github.io/NYU-DLSP20/en/week08/08-3/...",
          "Type: lectures2025<br>Text: Maximize the variational lower bound:\nℒ 𝜃, 𝜙; 𝑥 = 𝔼𝑞𝜙(𝑧|𝑥) log 𝑝𝜃 𝑥 𝑧 − 𝐷𝐾𝐿(𝑞𝜙(𝑧|𝑥)||𝑝(𝑧))\nGaussian ...",
          "Type: lectures2025<br>Text: Variational Inference\nSource: https://atcold.github.io/NYU-DLSP20/en/week08/08-3/...",
          "Type: lectures2025<br>Text: Gaussian VAEs\nInput Output\nDecoderz~𝑁(𝜇, 𝜎)\nz\n𝜇\n𝜎\nVariational \nparameters 𝜙 Posterior parameters 𝜃\nG...",
          "Type: lectures2025<br>Text: 28\nPosterior distribution -> Inference model\n• Variational approximation\n• Recognition model\n• Infer...",
          "Type: lectures2025<br>Text: 29\n„The Model“ (prior + conditional, or joint) -> generative model\n• The (data) likelihood model\n• G...",
          "Type: lectures2025<br>Text: 30\n• Variational lower bound\n𝐿 𝜃, 𝜙; 𝑥 = 𝔼𝑞𝜙(𝑧|𝑥) log 𝑝𝜃(𝑥|𝑧) − 𝐾𝐿 𝑞𝜙 𝑧 𝑥 ||𝑝(𝑧)\n  = 𝔼𝑞𝜙(𝑧|𝑥) log 𝑝𝜃...",
          "Type: lectures2025<br>Text: 31\nReparameterization trick...",
          "Type: lectures2025<br>Text: 32\n• Optimize 𝐿(𝜃, 𝜙; 𝑥) wrt. 𝜙 of 𝑞𝜙 𝑧 𝑥\n• ELBO: 𝐿(𝜃, 𝜙; 𝑥) = 𝔼𝑞𝜙(𝑧∣𝑥) log 𝑝𝜃(𝑥, 𝑧) + 𝐻 𝑞𝜙(𝑧 ∣ 𝑥)\n•...",
          "Type: lectures2025<br>Text: 33\n• Gradient estimate with reparameterization trick\n    𝑧 ∼ 𝑞𝜙(𝑧 ∣ 𝑥) ⇔ 𝑧 = 𝑔𝜙(𝜖, 𝑥), 𝜖 ∼ 𝑝(𝜖)\n∇𝜙𝔼𝑞...",
          "Type: lectures2025<br>Text: 34\n• Score function gradient is broadly applicable to nearly any \nvariational distribution, regardle...",
          "Type: lectures2025<br>Text: VAEs: Algorithm\n[Kingma & Welling, 2014]...",
          "Type: lectures2025<br>Text: Projecting Means in Latent Space\nSource: https://atcold.github.io/pytorch-Deep-Learning/en/week08/08...",
          "Type: lectures2025<br>Text: Projecting Means in Latent Space\nSource: https://atcold.github.io/pytorch-Deep-Learning/en/week08/08...",
          "Type: lectures2025<br>Text: 38\nLatent code interpolation and sentences generation from VAEs [Bowman et \nal., 2015] [5]\nVAE: Exam...",
          "Type: lectures2025<br>Text: 39\nTopic Models\nNeural Networks for Natural Language Processing Prof. Sophie Fellenz...",
          "Type: lectures2025<br>Text: ▪ Word clouds (important words are bigger)\n▪ Probability distributions over words\n40\nTopics\nNeural N...",
          "Type: lectures2025<br>Text: 41\nTopic Models\n▪ Input: unstructured text data\n▪ Output: Topics\n▪ No annotations, labels, tags …\n▪ ...",
          "Type: lectures2025<br>Text: 0\n0,5\n1\n42\nSPATIAL ORGANIZATION OF NEURAL NETWORKS: A \nPROBABILISTIC MODELING APPROACH\nprobabilistic...",
          "Type: lectures2025<br>Text: 43\nSPATIAL ORGANIZATION OF NEURAL NETWORKS: A \nPROBABILISTIC MODELING APPROACH\nTopics Documents\nTopi...",
          "Type: lectures2025<br>Text: 44\nVAE-based Topic Models\nNeural Networks for Natural Language Processing Prof. Sophie Fellenz...",
          "Type: lectures2025<br>Text: 45\nDirichlet VAE\nInput (BoW) Output (BoW)\nz~𝐷𝑖𝑟(𝛼)𝛼\n𝑧 ∈ ℝ𝑚\nDistribution over topics\nFood\nPlace\nNice\n...",
          "Type: lectures2025<br>Text: 46\n• The Dirichlet distribution is a distribution over the (K-1)-\ndimensional simplex\n• It is parame...",
          "Type: lectures2025<br>Text: 47\nProbability Simplex\n1-D Simplex\n2-D Simplex\nTopic 1 Topic 2\n0.1/0.9\n0.5/0.5\nTopic 1 Topic 2\nTopic...",
          "Type: lectures2025<br>Text: 48\n• If 𝜋 ∼ 𝐷𝑖𝑟𝑖𝑐ℎ𝑙𝑒𝑡(𝛼1, … , 𝛼𝐾) then 𝜋𝑘 ≥ 0 for all k, and \n∑𝑘=1\n𝐾 𝜋𝑘 = 1\n• Expectation: 𝔼 𝜋1, … ,...",
          "Type: lectures2025<br>Text: 49\n•The concentration parameter 𝛼 determines the distribution \nover atom sizes\n•Small values of 𝛼 gi...",
          "Type: lectures2025<br>Text: 50\n• One document should be assigned to as few topics as possible\n• Why?\n• If one document is assign...",
          "Type: lectures2025<br>Text: 51\n• Basic neural topic modelling (NTM) architecture, was proposed by Miao et al. \n(2015) (Gaussian)...",
          "Type: lectures2025<br>Text: 52\n• menu minutes service ordered new order came went table way\n• wait try minutes going time good v...",
          "Type: lectures2025<br>Text: 53\n• nice service went wait great [UNK] think want food time\n• ordered [UNK] nice going like people ...",
          "Type: lectures2025<br>Text: 54\nℒ 𝜃, 𝜙; 𝑥 = 𝔼𝑞𝜙(𝑧|𝑥) log 𝑝𝜃 𝑥 𝑧 − 𝐷𝐾𝐿(𝑞𝜙(𝑧|𝑥)||𝑝(𝑧))\nℒ 𝜃, 𝜙; 𝑥 = 𝔼𝑞𝜙(𝑧|𝑥) log 𝑝𝜃 𝑥 𝑧 − 𝜷𝐷𝐾𝐿(𝑞𝜙(𝑧|...",
          "Type: lectures2025<br>Text: 55\n• is are they your do buy always enjoy go favorite\n• to look 's really so have looking pretty bur...",
          "Type: lectures2025<br>Text: 56\n• Thai, soup, rice, tuna, roll\n• Bartender, tables, drinks, server, restaurant\n• Burger, fries, e...",
          "Type: lectures2025<br>Text: 61\n• VAEs are generative models that learn a generative distribution \nfor the data\n• Reparameterizat...",
          "Type: lectures2025<br>Text: 62\n• Kingma, Diederik, and Max Welling. “Auto-encoding variational \nBayes” https://arxiv.org/abs/131...",
          "Type: lectures2025<br>Text: Neural networks for \nNatural Language Processing\nJun.-Prof. Dr. Sophie Fellenz\nWeek  08 – Fine-tunin...",
          "Type: lectures2025<br>Text: Agenda\n•BERT Fine-Tuning\n•GPT Fine-Tuning\n•RLHF Fine-tuning...",
          "Type: lectures2025<br>Text: BERT\n3...",
          "Type: lectures2025<br>Text: 4\nBERT\nSource: https://humboldt-\nwi.github.io/blog/research/information_systems_1\n920/bert_blog_post...",
          "Type: lectures2025<br>Text: 5\nBERT Pre-Training\nMasked Language model (MLM): Take sentence, mask single words\nand let BERT predi...",
          "Type: lectures2025<br>Text: 6\nBERT Pre-Training\nNext sentence prediction (NSP): Take two sentences and ask\nBERT if the second se...",
          "Type: lectures2025<br>Text: 7\nBERT Pre-Training\n• Take two sentences\n• Mask some words and embed into 𝐸𝑖\n• BERT outputs vectors ...",
          "Type: lectures2025<br>Text: 8\nBERT Pre-Training (MLM)\n• For 𝐸𝑖 masked :\n• Transform 𝑇𝑖 into probabilities over vocabulary\n• Chec...",
          "Type: lectures2025<br>Text: 9\nBERT Pre-Training (NSP)\n• Transform 𝐶 into 2-dim. probability \n• Sentences belong (don‘t belong) t...",
          "Type: lectures2025<br>Text: 10\nBERT Fine-Tuning\n• For application adjust model by adding layers on input and \noutput to fit the ...",
          "Type: lectures2025<br>Text: 11\nBERT Fine-Tuning...",
          "Type: lectures2025<br>Text: GPT\n12...",
          "Type: lectures2025<br>Text: 13\nGPT\n• GPT (Generative Pre-Trained Transformer) is a \nlanguage model to produce human-like text\n• ...",
          "Type: lectures2025<br>Text: 14\nGPT Unsupervised Pre-Training\n• Input context 𝑥𝑖−1,…,𝑥𝑖−𝑘\n• Output probability distribution over ...",
          "Type: lectures2025<br>Text: 15\nGPT Supervised Fine-Tuning\n• Given labelled dataset of sentences 𝐶\n• For sentence 𝑥 = (𝑥1,…,𝑥𝑛) w...",
          "Type: lectures2025<br>Text: 16\nGPT Supervised Fine-Tuning\n• The objective function is\n𝐿2 𝐶 = ෍\n(𝑥,𝑦)\nlog(𝑃 𝑦|𝑥,Θ )\n• To improve ...",
          "Type: lectures2025<br>Text: 17\nGPT Supervised Fine-Tuning\n• The exact training procedure depends on the task\n• Example (Entailme...",
          "Type: lectures2025<br>Text: 18\nGPT Supervised Fine-Tuning\nSource: Radford, \nAlec, et al. \n\"Improving language \nunderstanding by ...",
          "Type: lectures2025<br>Text: Fine-tuning based on Reinforcement Learning\n19...",
          "Type: lectures2025<br>Text: ChatGPT:Optimizing Language models for dialogue...",
          "Type: lectures2025<br>Text: Why RLHF?\nHow to create a loss function for\n• What is funny?\n• What is ethical?\n• What is safe?\n• Wh...",
          "Type: lectures2025<br>Text: Core technique: Learning from Human Feedback (RLHF)...",
          "Type: lectures2025<br>Text: Core technique: Learning from Human Feedback (RLHF)\n• Using human preferences as a reward signal to ...",
          "Type: lectures2025<br>Text: Reinforcement learning on human feedback (RLHF)\n• Three steps in general:\n1. Pretraining a language ...",
          "Type: lectures2025<br>Text: RLHF: Step 1, Pretraining the language model\nSource: \nhttps://huggingface.co/blog/rlhf...",
          "Type: lectures2025<br>Text: RLHF: Step 2, reward model training\nSource: \nhttps://huggingface.co/blog/rlhf...",
          "Type: lectures2025<br>Text: RLHF: Step 3, fine-tuning with RL\nSource: \nhttps://huggingface.co/blog/rlhf...",
          "Type: lectures2025<br>Text: What is ChatGPT? \n• It is a sibling model of InstructGPT (Ouyang et al. 2022) which is trained (thro...",
          "Type: lectures2025<br>Text: InstructGPT\n30\nSource: https://openai.com/blog/instruction-\nfollowing/...",
          "Type: lectures2025<br>Text: InstructGPT: High-level methodology\n31...",
          "Type: lectures2025<br>Text: Step 1: Supervised Fine-Tuning (SFT)\nLabelers were asked to write three kinds of prompts:\n• Plain: W...",
          "Type: lectures2025<br>Text: Step 1: Supervised Fine-Tuning (SFT)\n• 40 contract workers were hired and screened\n• Labelers were a...",
          "Type: lectures2025<br>Text: Step 2: Reward model (RM)\n• Labelers were asked to rank between K=4 and K=9 model outputs according ...",
          "Type: lectures2025<br>Text: Step 3: RLHF with PPO\n• Fine-tune SFT model using PPO\n• PPO dataset contains 31k prompts from the AP...",
          "Type: lectures2025<br>Text: Reinforcement Learning and PPO\n36...",
          "Type: lectures2025<br>Text: Timeline for RL in Games\n• 1992: TD-Gammon, temporal difference learning, Backgammon\n• 1997: Deep Bl...",
          "Type: lectures2025<br>Text: Applying RL in NLP with Robotics\n38\nSource: https://say-can.github.io/...",
          "Type: lectures2025<br>Text: Outline\nToday:\n1. Introduction to Reinforcement Learning (RL) \n2. Policy-Based RL\n• Policy Gradient ...",
          "Type: lectures2025<br>Text: Policy-based RL\nAdvantages: \n• True objective\n• Can learn stochastic policies\n• Effective in high-di...",
          "Type: lectures2025<br>Text: Introduction to RL\n‘Reinforcement learning is learning what to do- how to map \nsituations to actions...",
          "Type: lectures2025<br>Text: Introduction to RL\n‘Reinforcement learning is learning what to do- how to map \nsituations to actions...",
          "Type: lectures2025<br>Text: Introduction to RL for NLP\n„The window is open!“ (is this a statement of fact?)\n„Can you close the w...",
          "Type: lectures2025<br>Text: Reinforcement Learning Problem\n44...",
          "Type: lectures2025<br>Text: Markov Decision Processes\n45...",
          "Type: lectures2025<br>Text: Agent‘s Learning Task\n46...",
          "Type: lectures2025<br>Text: Example: Aliased Grid World\nsource:[UCL course on RL, lecture 7]...",
          "Type: lectures2025<br>Text: Example: Aliased Grid World\nsource:[UCL course on RL, lecture 7]...",
          "Type: lectures2025<br>Text: Example: Aliased Grid World\nsource:[UCL course on RL, lecture 7]...",
          "Type: lectures2025<br>Text: Policy objective\n50\n• Goal: given policy 𝜋𝜃(𝑠,𝑎), find the best parameters 𝜃\n• How to measure the qu...",
          "Type: lectures2025<br>Text: Policy Gradient \n51\nSource: [Deepmind RL2021]...",
          "Type: lectures2025<br>Text: Contextual Bandits Policy Gradient\n• Consider a one-step case (a contextual bandit) such that 𝐽(𝜃) =...",
          "Type: lectures2025<br>Text: Score function trick\n𝑎...",
          "Type: lectures2025<br>Text: Policy gradient theorem (episodic)\nsource:[UCL course on RL, lecture 7]...",
          "Type: lectures2025<br>Text: Theorem Proof\n55source:[UCL course on RL, lecture 7]...",
          "Type: lectures2025<br>Text: Proof continued\n56\nsource:[UCL course on RL, lecture 7]...",
          "Type: lectures2025<br>Text: Policy gradient training\n57source:[UCL course on RL, lecture 7]...",
          "Type: lectures2025<br>Text: REINFORCE: Monte Carlo Policy Gradient\nPseudocode:\nfor each episode do:\nGenerate a trajectory Rollou...",
          "Type: lectures2025<br>Text: Actor-Critic\n59\nValue-functionPolicy...",
          "Type: lectures2025<br>Text: Overview RL Paradigms\nPolicy-basedValue-based Actor-Critic...",
          "Type: lectures2025<br>Text: Proximal Policy Optimization (PPO)\n• Motivation: Avoid too large policy updates -> improve the train...",
          "Type: lectures2025<br>Text: Proximal Policy Optimization (PPO)\n62\nsource:[UCL course on RL, lecture 7]...",
          "Type: lectures2025<br>Text: Step 3: RLHF with PPO\n𝑜𝑏𝑗𝑒𝑐𝑡𝑖𝑣𝑒 𝜙\n= 𝐸 𝑥,𝑦 ∼𝐷𝜋𝜙\n𝑅𝐿 𝑟𝜃 𝑥,𝑦 −𝛽log(𝜋𝜙\n𝑅𝐿(𝑦|𝑥)/𝜋𝑆𝐹𝑇(𝑦|𝑥)) +𝛾𝐸𝑥∼𝐷𝑝𝑟𝑒𝑡𝑟𝑎𝑖𝑛 ...",
          "Type: lectures2025<br>Text: Variations on the methodology\nAlmost all papers to date have tweaks:\nAnthropic\n• Initial policy help...",
          "Type: lectures2025<br>Text: InstructGPT vs Anthropic\nSource: https://youtu.be/2MBJOuVq380\nSource: Bai et al. “Training a Helpful...",
          "Type: lectures2025<br>Text: InstructGPT vs Anthropic\nSource: https://youtu.be/2MBJOuVq380\nSource: Bai et al. “Training a Helpful...",
          "Type: lectures2025<br>Text: InstructGPT vs Anthropic\nSource: https://youtu.be/2MBJOuVq380\nSource: Bai et al. “Training a Helpful...",
          "Type: lectures2025<br>Text: Summary\n• Policy-gradient methods directly optimize policy (in our case language model)\n• PPO tries ...",
          "Type: lectures2025<br>Text: References\n• [Sutton & Barto, 2018]Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An ...",
          "Type: lectures2025<br>Text: Jun.-Prof. Sophie Fellenz\nWeek 4 – Language Modeling and Neural Networks\n11 Nov 2024\nNeural Networks...",
          "Type: lectures2025<br>Text: • Is Skipgram based on neural networks?\n• What is the difference between a neural network word\nembed...",
          "Type: lectures2025<br>Text: • Difference:\n• Skipgram embeddings are used inside neural\nnetworks (first layer)\n• NN embeddings ar...",
          "Type: lectures2025<br>Text: Language Models\n4...",
          "Type: lectures2025<br>Text: • Jane went to the store.\n• store to Jane went the.\n• Jane went store.\n• Jane goed to the store.\n• T...",
          "Type: lectures2025<br>Text: • Jane went to the store.\n• store to Jane went the.\n• Jane went store.\n• Jane goed to the store.\n• T...",
          "Type: lectures2025<br>Text: 𝑃 𝑋 = ෑ\n𝑖=1\n𝐼\n𝑃(𝑥𝑖|𝑥1,…,𝑥𝑖−1)\nThe big problem: How do we predict\n𝑃 𝑥𝑖 𝑥1,…,𝑥𝑖−1\n??\nProbabilistic lan...",
          "Type: lectures2025<br>Text: Score sentences:\n• Jane went to the store . -> high\n• Store to Jane went the . -> low\n• (same as cal...",
          "Type: lectures2025<br>Text: Count-based Language Models\n9...",
          "Type: lectures2025<br>Text: Independence assumption: 𝑃 𝑥𝑖 𝑥1,…,𝑥𝑖−1 ≈ 𝑃(𝑥𝑖)\nCount-based maximum-likelihood estimation:\n𝑃𝑀𝐿𝐸 𝑥𝑖 =...",
          "Type: lectures2025<br>Text: Limit context length to 𝑛, count, and divide\n𝑃𝑀𝐿 𝑥𝑖 𝑥𝑖−𝑛+1,…,𝑥𝑖−1 ≔ 𝑐(𝑥𝑖−𝑛+1,…,𝑥𝑖)\n𝑐(𝑥𝑖−𝑛+1,…,𝑥𝑖−1)\n...",
          "Type: lectures2025<br>Text: Additive/Dirichlet:\n𝑃 𝑥𝑖 𝑥𝑖−𝑛+1,…,𝑥𝑖−1 ≔ 𝑐 𝑥𝑖−𝑛+1,…,𝑥𝑖 +𝛼𝑃(𝑥𝑖|𝑥𝑖−𝑛+2,…,𝑥𝑖−1)\n𝑐 𝑥𝑖−𝑛+1,…,𝑥𝑖−1 +𝛼\nDisc...",
          "Type: lectures2025<br>Text: Cannot share strength among similar words\nsolution: class based language models\nCannot condition on ...",
          "Type: lectures2025<br>Text: Cannot handle long-distance dependencies\nsolution: cache, trigger, topic, syntactic models, etc.\n14\n...",
          "Type: lectures2025<br>Text: • Neural language models (next) achieve better \nperformance, but\n• n-gram models are extremely fast ...",
          "Type: lectures2025<br>Text: LM Evaluation\n16...",
          "Type: lectures2025<br>Text: Log-likelihood:\n𝐿𝐿 ℇ𝑡𝑒𝑠𝑡 = ෍\n𝐸∈ℇ𝑡𝑒𝑠𝑡\nlog𝑃(𝐸)\nPer-word Log Likelihood:\n𝑊𝐿𝐿 ℇ𝑡𝑒𝑠𝑡 = 1\nσ𝐸∈ℇ𝑡𝑒𝑠𝑡 𝐸 ෍\n𝐸∈ℇ...",
          "Type: lectures2025<br>Text: • Important: the vocabulary must be the same over models \nyou compare\n• Or more accurately, all mode...",
          "Type: lectures2025<br>Text: Log-linear models\n19...",
          "Type: lectures2025<br>Text: • Calculate features of the context\n• Based on the features, calculate probabilities\n• Optimize feat...",
          "Type: lectures2025<br>Text: Calculate features of the context, calculate probabilities\nFeature weights optimized by SGD, etc 21\n...",
          "Type: lectures2025<br>Text: Previous words: “giving a\"\n22\nExample:\nWords we‘re\npredicting\nHow likely\nare they?\nHow likely\nare th...",
          "Type: lectures2025<br>Text: • Calculate the gradient of the loss function with respect to \nthe parameters\n• How? Use the chain r...",
          "Type: lectures2025<br>Text: 24\nWhat Problems are Handled?\nCannot share strength among similar words\nnot solved yet \nCannot condi...",
          "Type: lectures2025<br>Text: Cannot handle long-distance dependencies\nNot solved yet\n25\nProblems and solutions?\nFor tennis class ...",
          "Type: lectures2025<br>Text: Beyond linear models\n26...",
          "Type: lectures2025<br>Text: Students take tests → high Teachers take tests → low\nStudents write tests → low Teachers write tests...",
          "Type: lectures2025<br>Text: Original Motivation: Neurons in the Brain\nCurrent Conception: Computation Graphs\n28\n“Neural” Nets...",
          "Type: lectures2025<br>Text: 29\n𝑥1\n𝑥2\n𝑥3\nInput neurons:\n𝑤1,1\n𝑤2,1\n𝑤3,1\n𝑏1\n𝑥1𝑤1,1 +𝑥2𝑤2,1 +𝑥3𝑤3,1 +𝑏1\nOutput neurons:\n𝑥1𝑤1,2 +𝑥2𝑤2...",
          "Type: lectures2025<br>Text: 30\n𝑥1\n𝑥2\n𝑥3\nInput neurons: 𝑏\n𝑥1𝑤1,1 +𝑥2𝑤2,1 +𝑥3𝑤3,1 +𝑏1\nOutput neurons:\n𝑥1𝑤1,2 +𝑥2𝑤2,2 +𝑥3𝑤3,2 +𝑏2\n𝑥...",
          "Type: lectures2025<br>Text: 31\n𝑥1\n𝑥2\n𝑥3\nInput neurons: 𝑏\n𝑥1𝑤1,1 +𝑥2𝑤2,1 +𝑥3𝑤3,1 +𝑏1\nOutput neurons:\n𝑥1𝑤1,2 +𝑥2𝑤2,2 +𝑥3𝑤3,2 +𝑏2\n𝑥...",
          "Type: lectures2025<br>Text: 32\nInput layer Output\n𝑶𝒖𝒕𝒑𝒖𝒕 = 𝝈(…𝝈 𝒙𝑻𝑾𝟏 +𝒃𝟏\n𝑻 …𝑾𝒏 +𝒃𝒏𝑻)\nUsually we use multiple layers\nHidden layer...",
          "Type: lectures2025<br>Text: 33\nInput layer Output\nHow does this work specifically?\nExample: Given cat or dog image.\nTask: Find o...",
          "Type: lectures2025<br>Text: 34\nInput layer Output\nWhy does this work?\n• One can prove that any function, i.e. the cat-dog recogn...",
          "Type: lectures2025<br>Text: 35\nInput layer Output\nHow do we find out how large it needs to be?\n• Experimentation!\nHow do we find...",
          "Type: lectures2025<br>Text: • Given: \nTraining data, i.e. input data 𝑥 where desired output 𝑦is\nknown.\nNeural Network 𝑛𝑊,𝑏\n• Out...",
          "Type: lectures2025<br>Text: expression:\n𝑥\ngraph:\nA node is a {tensor, matrix, vector, scalar} value\n37\n𝑥...",
          "Type: lectures2025<br>Text: • An edge represents a function argument (and also a data dependency). \nThey are just pointers to no...",
          "Type: lectures2025<br>Text: expression:\n𝑥𝑇𝑊+𝑏\ngraph:\nFunctions can be nullary, unary, binary, ... n-ary. Often they are unary or...",
          "Type: lectures2025<br>Text: expression:\n𝜎(𝑥𝑇𝑊+𝑏𝑇)\ngraph:\n40\n𝑥\n𝑓1 𝑥,𝑊 = 𝑥𝑇𝑊\n𝑊\n∗\n𝑓2 𝑢,𝑏 = 𝑢+𝑏𝑇\n𝑏\n+\n𝑓3 𝑢 = 𝜎(𝑢)\n𝜎...",
          "Type: lectures2025<br>Text: expression:\n𝜎(𝑥𝑇𝑊+𝑏𝑇)−𝑦𝑇\ngraph:\n41\n𝑥\n𝑓1 𝑥,𝑊 = 𝑥𝑇𝑊\n𝑊\n∗\n𝑓2 𝑢,𝑏 = 𝑢+𝑏𝑇\n𝑏\n+\n𝑓3 𝑢 = 𝜎(𝑢)\n𝜎\n𝑓4 𝑢,𝑦 = 𝑢−𝑦\n−...",
          "Type: lectures2025<br>Text: expression:\nLoss = 𝜎(𝑥𝑇𝑊+𝑏𝑇 −𝑦𝑇ԡ2 = 𝑓5(𝑓4(𝑓3(𝑓2(𝑓1(𝑥,𝑊),𝑏)),𝑦))\ngraph:\n42\n𝑥\n𝑓1 𝑥,𝑊 = 𝑥𝑇𝑊\n𝑊\n∗\n𝑓2 𝑢,𝑏 ...",
          "Type: lectures2025<br>Text: • Graph construction\n• Forward propagation\nIn topological order, compute the value of the node \ngive...",
          "Type: lectures2025<br>Text: 44\nForward Propagation\n𝑥\n𝑓1 𝑥,𝑊 = 𝑥𝑇𝑊\n𝑊\n∗\n𝑓2 𝑢,𝑏 = 𝑢+𝑏𝑇\n𝑏\n+\n𝑓3 𝑢 = 𝜎(𝑢)\n𝜎\n𝑓4 𝑢,𝑦 = 𝑢−𝑦\n−\n𝑦\n𝑓5 𝑢 = 𝑢 ...",
          "Type: lectures2025<br>Text: 45\nForward Propagation\n𝑥\n𝑓1 𝑥,𝑊 = 𝑥𝑇𝑊\n𝑊\n𝑢1\n𝑓2 𝑢,𝑏 = 𝑢+𝑏𝑇\n𝑏\n+\n𝑓3 𝑢 = 𝜎(𝑢)\n𝜎\n𝑓4 𝑢,𝑦 = 𝑢−𝑦\n−\n𝑦\n𝑓5 𝑢 = 𝑢...",
          "Type: lectures2025<br>Text: 46\nForward Propagation\n𝑥\n𝑓1 𝑥,𝑊 = 𝑥𝑇𝑊\n𝑊\n𝑢1\n𝑓2 𝑢,𝑏 = 𝑢+𝑏𝑇\n𝑏\n𝑢2\n𝑓3 𝑢 = 𝜎(𝑢)\n𝜎\n𝑓4 𝑢,𝑦 = 𝑢−𝑦\n−\n𝑦\n𝑓5 𝑢 = ...",
          "Type: lectures2025<br>Text: 47\nForward Propagation\n𝑥\n𝑓1 𝑥,𝑊 = 𝑥𝑇𝑊\n𝑊\n𝑢1\n𝑓2 𝑢,𝑏 = 𝑢+𝑏𝑇\n𝑏\n𝑢2\n𝑓3 𝑢 = 𝜎(𝑢)\n𝑢3\n𝑓4 𝑢,𝑦 = 𝑢−𝑦\n−\n𝑦\n𝑓5 𝑢 =...",
          "Type: lectures2025<br>Text: 48\nForward Propagation\n𝑥\n𝑓1 𝑥,𝑊 = 𝑥𝑇𝑊\n𝑊\n𝑢1\n𝑓2 𝑢,𝑏 = 𝑢+𝑏𝑇\n𝑏\n𝑢2\n𝑓3 𝑢 = 𝜎(𝑢)\n𝑢3\n𝑓4 𝑢,𝑦 = 𝑢−𝑦\n𝑢4\n𝑦\n𝑓5 𝑢 ...",
          "Type: lectures2025<br>Text: 49\nForward Propagation\n𝑥\n𝑓1 𝑥,𝑊 = 𝑥𝑇𝑊\n𝑊\n𝑢1\n𝑓2 𝑢,𝑏 = 𝑢+𝑏𝑇\n𝑏\n𝑢2\n𝑓3 𝑢 = 𝜎(𝑢)\n𝑢3\n𝑓4 𝑢,𝑦 = 𝑢−𝑦\n𝑢4\n𝑦\n𝑓5 𝑢 ...",
          "Type: lectures2025<br>Text: • Aim: Minimize loss 𝑓 𝑥,𝑊,𝑏 = ԡ𝜎(𝑥𝑇𝑊 +𝑏𝑇)−𝑦𝑇ԡ2\nw.r.t. weights 𝑊,𝑏\n• Idea: Gradient = Direction of h...",
          "Type: lectures2025<br>Text: Back-propagation:\n• Process examples in reverse topological order\n• Calculate the derivatives of the...",
          "Type: lectures2025<br>Text: Expression:\nLoss = ԡ𝜎(𝑥𝑇𝑊 +𝑏𝑇)−𝑦𝑇ԡ2\nAim: Minimize loss by optimizing weights 𝑊,𝑏\n52\nBack Propagation...",
          "Type: lectures2025<br>Text: Step 1: Compute all derivatives at every node wrt. the inputs\n(at relevant edges)\nExample: \n𝜕𝑓2\n𝜕𝑏 =...",
          "Type: lectures2025<br>Text: Step 1: Compute all derivatives at every node wrt. the inputs.\n54\nBack Propagation\n𝑥\n𝑓1 𝑥,𝑊 = 𝑥𝑇𝑊\n𝑊\n...",
          "Type: lectures2025<br>Text: Step 2: Use the chain rule.\nExample:𝑓 𝑥,𝑊,𝑏 = 𝜎(𝑥𝑇𝑊 +𝑏𝑇 −𝑦𝑇ԡ2\n= 𝑓5(𝑓4(𝑓3(𝑓2(𝑓1(𝑥,𝑊),𝑏)),𝑦))\n𝜕𝑓(𝑥,𝑊,𝑏...",
          "Type: lectures2025<br>Text: Step 2: Use the chain rule.\nExample:𝑓 𝑥,𝑊,𝑏 = 𝜎(𝑥𝑇𝑊 +𝑏𝑇 −𝑦𝑇ԡ2\n= 𝑓5(𝑓4(𝑓3(𝑓2(𝑓1(𝑥,𝑊),𝑏)),𝑦))\n𝜕𝑓(𝑥,𝑊,𝑏...",
          "Type: lectures2025<br>Text: Step 2: Use the chain rule.\nExample:𝑓 𝑥,𝑊,𝑏 = 𝜎(𝑥𝑇𝑊 +𝑏𝑇 −𝑦𝑇ԡ2 = 𝑓5(𝑓4(𝑓3(𝑓2(𝑓1(𝑥,𝑊),𝑏)),𝑦))\n𝜕𝑓(𝑥,𝑊,𝑏...",
          "Type: lectures2025<br>Text: Step 2: Use the chain rule.\nSimilarly, we calculate\n𝜕𝑓\n𝜕𝑊 .Note that we already have calculated\n𝜕𝑓\n𝜕...",
          "Type: lectures2025<br>Text: Step 3: Apply Gradient descent\nUpdate: 𝛼 > 0learning rate\n𝑊𝑛𝑒𝑤 = 𝑊𝑜𝑙𝑑 −𝛼 𝜕𝑓\n𝜕𝑊\n𝑏𝑛𝑒𝑤 = 𝑏𝑜𝑙𝑑 −𝛼𝜕𝑓\n𝜕𝑏\n5...",
          "Type: lectures2025<br>Text: Back to language modeling\n60...",
          "Type: lectures2025<br>Text: • See Bengio et al. 2003\n61\nFeed-forward Neural Language Models\nDeep Learning for Natural Language P...",
          "Type: lectures2025<br>Text: • Word embeddings capture features of words\n• e.g. feature 1 indicates verbs, feature 2 indicates de...",
          "Type: lectures2025<br>Text: 63\nWhere is Strength Shared?\nlookup\ngiving\nlookup\n𝑡𝑎𝑛ℎ(𝑊1ℎ+𝑏) softmax=+\nBias scores probs\nW\nWord emb...",
          "Type: lectures2025<br>Text: 65\nWhat Problems are Handled?\nCannot share strength among similar words\nsolved, and similar contexts...",
          "Type: lectures2025<br>Text: Cannot handle long-distance dependencies\nNot solved yet\n66\nProblems and solutions?\nFor tennis class ...",
          "Type: lectures2025<br>Text: • Neural networks allow design of arbitrarily complex\nfunctions!\n• In future classes:\n▪ Recurrent ne...",
          "Type: lectures2025<br>Text: Next lecture\nRecurrent Neural Networks...",
          "Type: lectures2025<br>Text: ● CMU Advanced NLP Course:\n● https://phontron.com/class/anlp2022/schedule.html\n● Sören Laue\n● Feibai...",
          "Type: lectures2025<br>Text: ● Video on Backprop by Andrej Karpathy:\n• https://youtu.be/VMj-3S1tku0\n• Video on Language modeling ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nLecture 7 – Language Models – Generating Text \nfrom ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 2\nImage Source: https://www.tensorflow.org/text/tut...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n8\nI have eaten an apple\nI have eaten an apple\nI have...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n9\nI\nI\nI\nI\nI\nI\neaten\neaten\neaten\neaten\nan\nan\nan\napple...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n10...",
          "Type: lectures2025<br>Text: Beam search\nBeam search: Keep track of the 𝑘 most likely partial translations\n𝑘 is the beam size\n11...",
          "Type: lectures2025<br>Text: Beam search\nBeam size: 2\n12\n<Start>\na\nthe\nSource: https://web.stanford.edu/class/archive/cs/cs224n/c...",
          "Type: lectures2025<br>Text: Beam search\nBeam size: 2\n13\n<Start>\na\nthe\npoor\npeople\npoor\nperson\nSource: https://web.stanford.edu/c...",
          "Type: lectures2025<br>Text: Beam search\nBeam size: 2\n14\n<Start>\na\nthe\npoor\npeople\npoor\nperson\nare\ndon‘t\nbut\nperson\nSource: https...",
          "Type: lectures2025<br>Text: Beam search\nBeam size: 2\n15\n<Start>\na\nthe\npoor\npeople\npoor\nperson\nare\ndon‘t\nbut\nperson\nalways\nnot\nha...",
          "Type: lectures2025<br>Text: Beam search\nBeam size: 2\n16\n<Start>\na\nthe\npoor\npeople\npoor\nperson\nare\ndon‘t\nbut\nperson\nalways\nnot\nha...",
          "Type: lectures2025<br>Text: Beam search\nBeam size: 2\n17\nSource: https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1184/lec...",
          "Type: lectures2025<br>Text: Beam search\nBeam size: 2\n18\nSource: https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1184/lec...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Recap: language models\n• Generating text: Intro\n• ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• A language model (LM) 𝑝𝜃 is a probability distribu...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Intuitively, are Language Models naturally suited ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Language models might not be perfectly-suited for ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Language models might not be perfectly-suited for ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Language models might not be perfectly-suited for ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nFundamentals of discrete distributions\n25...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• A discrete probability distribution p(x) is define...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n“Small” discrete sets\n0.25 0.50 0.25 0.15\ne.g., a vo...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n“Large” discrete sets\ne.g., sequences of up to lengt...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• A discrete probability distribution p(x) is define...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Entropy is arguably the single most useful propert...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nIntuition of Entropy –low entropy and high entropy\nI...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nExtreme Example of\nHigh-entropy and Low-entropy toke...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nIn natural language generation, we often describe ta...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nWe have a discrete set 𝜒and a distribution 𝑝 𝑥\nThe e...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nSometimes, the true next token distribution has many...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nSometimes, the true next token distribution has many...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nSometimes, the true next token distribution has few ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nContext lowers entropy\nMy Car\nis\ndrives\nwas\nhas\nruns...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nAdding Context Reduces Entropy (Important for the co...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nWe have a discrete set 𝜒and a distribution 𝑝 𝑥\nThe m...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nWe have a discrete set 𝜒and a distribution 𝑝 𝑥 , and...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nHere are some examples of semantically similar examp...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nFrom parsing to machine translation, there’s a long ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nIn a high entropy distribution, most probability ten...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Often, researchers/developers are not working with...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nDecoding as a choice of Algorithm + Scoring  \nFuncti...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Given a distribution 𝑝𝜃, how can we characterize d...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Given a distribution 𝑝𝜃, how can we characterize d...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Given a distribution 𝑝𝜃, how can we characterize d...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Given a distribution 𝑝𝜃, how can we characterize d...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nCommon Choices of Scoring Functions\n51...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nIf current language generators were already flawless...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nWhy do we need alternative scoring functions?\n53\nUnf...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nThe mean-seeking nature of\nthe MLE training objectiv...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• In this section we’ll choose multinomial sampling ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Ancestral sampling is obtained when the scoring fu...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nConsequence of temperature sampling\nOne day a cat de...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nTemperature Sampling code examples –\nHigher Temperat...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nTemperature Sampling code examples –\nLower Temperatu...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• The presence of an unreliable tail suggests the us...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nTop-k sampling\n• Simply truncate the tail by selecti...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nTwo failure modes of top-k sampling\nSuppose we fix k...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nPerplexity and generation quality\n• Some scoring fun...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nPerplexity and generation quality\n• High-quality tex...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nMirostat\nAt each generation step\n• Choose the top-k ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nMirostat\nTune the size of the top-k set to sample a ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nNucleus sampling\n• Select tail size dynamically by o...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nDecreasing the pi in Nucleus Sampling decreases the ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nA failure mode of nucleus sampling\nSuppose we fix 𝜋=...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• The absolute probability principle — that words ou...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nEpsilon Sampling vs Top-K Sampling vs  Nucleus Sampl...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nPrompting: The Precursor to Controlled \nGeneration\n7...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nWhat is prompting?\n• So far, we’ve looked at how to ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nPrompting in our framework\n• Prompting can be viewed...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nPrompting in our framework\n• Prompting can be viewed...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nWhy does prompting work?\nRecall: Context (often) low...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nWhy does prompting work?\nIntuition: providing more c...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nPrompting to solve lower entropy tasks\n81\nEnglish:\nT...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nTypes of Prompting: Demonstrations\nFor a task with i...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nTypes of Prompting: Learning Prompts\nFor a task with...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nTypes of Prompting: Chain-of-Thought\nFor a task with...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nIs prompting the definite solution?\n85...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nCurrent Trends in Language Generation: Larger Models...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nImplications of Larger Models\n• Many of the methods ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nFine-Tuning Approaches\n• If we’re willing to retrain...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nInstruction-Tuned and RLHF Language Models\nBase pret...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nInstruction-Tuned and RLHF Language Models\nInstructi...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nInstruction-Tuned and RLHF Language Models\nRLHF like...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nConclusion\n• High-entropy vs low-entropy generation\n...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nNext time: Reinforcement Learning for NLP\n94\nLiu et ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nReferences\nProbability distributions over strings\nFo...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nAcknowledgements\n• “Generating Text from Language Mo...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nLecture 5 – RNN/LSTM/CNN Language Models\n20.11.2023\n...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Recap: N-gram language models\n• RNN language model...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n▪ Language Modeling is the task of predicting what w...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nWe use language models everyday!\n4...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nWe use language models everyday!\n5...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• the woman bought a _ _ _ _\n• Question: How to lear...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nYou can also use a language model to generate text\nG...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nYou can also use a language model to generate text.\n...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Recall the Language Modeling task:\n▪ Input: sequen...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nA fixed-window neural Language Model\nthe\n woman boug...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n▪ Improvements over n-gram LM:\n▪ No sparsity problem...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n▪ A family of neural architectures\nRecurrent Neural ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nA (typical) RNN Language Model\nbought awords / one-h...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nA (typical) RNN Language Model\nbought awords / one-h...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nA (typical) RNN Language Model\nbought awords / one-h...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nA (typical) RNN Language Model\nbought awords / one-h...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nA (typical) RNN Language Model\nbought awords / one-h...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n▪ RNN Advantages:\n• Can process any length input\n• M...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• RNN Disadvantages:\n• Recurrent computation is slow...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Get a big corpus of text which is a sequence of wo...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nTraining a RNN Language Model\nbought a\nthe woman\nCor...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nTraining a RNN Language Model\nbought a\nthe woman\nCor...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nTraining a RNN Language Model\nbought a\nthe woman\nCor...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nTraining a RNN Language Model\nbought a\nthe woman\nCor...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nTraining a RNN Language Model\nbought a\nthe woman\nCor...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• However: Computing loss and gradients across entir...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nBackpropagation for RNNs\n𝑾𝒉\n𝒉 𝟎\n… 𝑾𝒉\n𝒉 𝒕−𝟑\n𝑾𝒉\n𝒉 𝒕−𝟐\n...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nBackpropagation for RNNs\n𝑾𝒉\n𝒉 𝟎\n… 𝑾𝒉\n𝒉 𝒕−𝟑\n𝑾𝒉\n𝒉 𝒕−𝟐\n...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nMultivariable Chain Rule\n29...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nBackpropagation for RNNs: Proof sketch\n30...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nBackpropagation for RNNs\n𝑾𝒉\n𝒉 𝟎\n… 𝑾𝒉\n𝒉 𝒕−𝟑\n𝑾𝒉\n𝒉 𝒕−𝟐\n...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nBackpropagation for RNNs\n𝑾𝒉\n𝒉 𝟎\n… 𝑾𝒉\n𝒉 𝒕−𝟑\n𝑾𝒉\n𝒉 𝒕−𝟐\n...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Let’s have some fun!\n• You can train a RNN-LM on a...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Let’s have some fun!\n• You can train a RNN-LM on a...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Let’s have some fun!\n• You can train a RNN-LM on a...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• 1. Vanishing gradients\n• 2. Exploding gradients\nPr...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nProblems with RNNs\n1. Vanishing gradients\n2. Explodi...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nVanishing gradient intuition\n𝑾\n𝒉 𝟏\n𝑾\n𝒉 𝟐\n𝑾\n𝒉 𝟑\n 𝒉 𝟒\n...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nVanishing gradient intuition\n𝑾\n𝒉 𝟏\n𝑾\n𝒉 𝟐\n𝑾\n𝒉 𝟑\n 𝒉 𝟒\n...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nVanishing gradient intuition\n𝑾\n𝒉 𝟏\n𝑾\n𝒉 𝟐\n𝑾\n𝒉 𝟑\n 𝒉 𝟒\n...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nLM task: When she tried to print her tickets, she fo...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• If the gradient becomes too big, then the SGD upda...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• A solution for exploding gradient!\n• Gradient clip...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nHow to fix the vanishing gradient problem?\n• The mai...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• A type of RNN proposed by Hochreiter and Schmidhub...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• The selection of which information is erased/writt...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nWe have a sequence of inputs 𝑥(t), and we will compu...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nWe have a sequence of inputs 𝑥(t), and we will compu...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nWe have a sequence of inputs 𝑥(t), and we will compu...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nWe have a sequence of inputs 𝑥(t), and we will compu...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nLong Short-Term Memory RNNs (LSTMs)\nNew cell content...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nLong Short-Term Memory RNNs (LSTMs)\nNew cell content...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nLong Short-Term Memory RNNs (LSTMs)\nNew cell content...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nLong Short-Term Memory RNNs (LSTMs)\nNew cell content...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nYou can think of the LSTM equations visually like th...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nLong Short-Term Memory RNNs (LSTMs)\nYou can think of...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• The LSTM architecture makes it easier for the RNN ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• In 2013–2015, LSTMs started achieving state-of-the...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• No! It can be a problem for all neural architectur...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nBidirectional and Multi-layer RNNs: motivation\nthe m...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nBidirectional and Multi-layer RNNs: motivation\nthe m...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• On timestep 𝑡:\n• Forward RNN ℎ 𝑡 = 𝑅𝑁𝑁𝐹𝑊(ℎ 𝑡−1 ,𝑥(...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Note: bidirectional RNNs are only applicable if yo...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• RNNs are already “deep” on one dimension (they unr...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nMulti-layer RNNs\nthe movie was terribly exciting !\nR...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• High-performing RNNs are often multi-layer (but ar...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Cannot share strength among similar words\n• solved...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n▪ Cannot handle long-distance dependencies\n▪ Solved!...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nCNNs\n69...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nA 1D convolution for text\nTentative 0.2 0.1 -0.3 0.4...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nPadding\nØ 0.0 0.0 0.0 0.0\nTentative 0.2 0.1 -0.3 0.4...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nMultiple filters\nØ 0.0 0.0 0.0 0.0\nTentative 0.2 0.1...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nMax pooling\nØ 0.0 0.0 0.0 0.0\nTentative 0.2 0.1 -0.3...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nAverage pooling\nØ 0.0 0.0 0.0 0.0\nTentative 0.2 0.1 ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nStride=2\nØ 0.0 0.0 0.0 0.0\nTentative 0.2 0.1 -0.3 0....",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nKim (2014)\n76...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• RNNs are capable of learning long sequences and pr...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nNext lecture\nAttention and Transformers...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n▪ “Long short-term memory”, Hochreiter and Schmidhub...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Asmita Bhat\n• Stanford CS224N, Lecture 6 and 7\nAck...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 1\nLecture 3 – Word Embeddings\nJun.-Prof. Sophie Fel...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 2\n• Word meaning\n• Word2vec intro\n• Word2vec, more ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 3\n• One-hot encodings\n• Dense encodings\nWord embedd...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 4\n4\nWord meaning...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 5\n• Meaning of word “Meaning” (according to Webster...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 6\n• Common solution: Use e.g. Wordnet\n• Wordnet is ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 7\n• Great as a resource but missing nuance\n• e.g. “...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 8\n• Localist representation (traditional rule-based...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 9\n• Example: in web search, if user searches for “S...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 10\n• Distributionalsemantics: A word’s meaning is g...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 11\nWe will build a dense vector for each word, chos...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 12\n• Distributional semantics (as opposed to other ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 13\nVisualizing Word Vectors\n[source: https://medium...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 14\n1. Frequency based word vectors\n• Count vectors\n...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 15\n• Idea: Represent a word as a vector of frequenc...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 16\n• Example: Consider the term frequency count of ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 17\n• Measures co-occurrence of words.\n• Idea: Words...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 18\n• Overview: Given a corpus of documents, co-occu...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 19\n• Co-occurrence Matrix: (She is happy. She is we...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 20\n• Simple count co-occurrence vectors\n• Vectors i...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 21\n• Prediction based embeddings are obtained by pr...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 22\n• Word2Vec (Mikolov et al. 2013) is a framework ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 23\n• Word2Vec (Mikolov et al. 2013) is a framework ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 24\nExample windows and process for computing P(𝑤𝑡+𝑗...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 25\nExample windows and process for computing P(𝑤𝑡+𝑗...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 26\n• For each position 𝑡 = 1,…,𝑇, predict context w...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 27\nWe want to minimize the objective function:\n𝐽 𝜃 ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 28\nWord2Vec : prediction function\n𝑃 𝑜 𝑐 = exp(𝑢𝑜𝑇𝑣𝑐...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 29\n𝑃 𝑜 𝑐 = exp(𝑢𝑜𝑇𝑣𝑐)\n𝛴𝑤∈𝑉 exp(𝑢𝑤𝑇 𝑣𝑐)\nThis is an e...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 30\nWord2Vec : CBOW\nINPUT PROJECTION OUTPUT\nw(t-2)\nS...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 31\nWord2Vec : Skip-Grams\nINPUT PROJECTION OUTPUT\nw(...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 32\nSkipgram\nSource: http://mccormickml.com/2016/04/...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 33\nMinimize: 𝐽 𝜃 = −\n1\n𝑇 σ𝑡=1\n𝑇 σ−𝑚≤𝑗≤𝑚\n𝑗≠0\nlog𝑃 𝑤𝑡...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 34\n𝜕\n𝜕𝑣𝑐\nlogexp(𝑢𝑜𝑇𝑣𝑐) = 𝜕\n𝜕𝑣𝑐\n𝑢𝑜𝑇𝑣𝑐 = 𝑢𝑜\n𝜕\n𝜕𝑣𝑐\nlog...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 35\nSkip gram gradient\n𝜕\n𝜕𝑣𝑐\nlog exp(𝑢𝑜𝑇𝑣𝑐)\n𝛴𝑤∈𝑉 exp...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 36\nThe skip-gram model with negative sampling\n• The...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 37\nThe skip-gram model with negative sampling\nFrom ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 38\nCount-based vs prediction-based WE\nCount based\n•...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 39\n𝐽 𝜃 = 1\n2 ෍\nⅈ,𝑗=1\n𝑊\n𝑓 𝑋𝑖𝑗 𝑢𝑖\n𝑇𝑣𝑗 −log𝑋𝑖𝑗\n2\n• Fas...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 40\nNearest words to frog:\n1. frogs\n2. toad\n3. litor...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 41\n• Related to general evaluation in NLP: Intrinsi...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 42\n• Word vector analogies\n• a : b = c : ? 𝑑 = argm...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 43\nGloVe Visualizations\n[source: https://web.stanfo...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 44\nGloVe Visualizations: Company - CEOs\n[source: ht...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 45\nGloVe Visualizations: Comparatives - Superlative...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 46\nExpression Nearest token\nParis – France + Italy ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 47\n• More data helps\n• Wikipedia is better than new...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 48\n• Dimensionality\n• Good dimension is ~300\nAnalog...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 49\n• Word vector distances and their correlation wi...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 50\n• Extrinsic evaluation of word vectors: All subs...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 51\n• Most words have lots of meanings!\n• Especially...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 52\n• A sharp point or staff\n• A type of elongated f...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 53\nIdea: Cluster word windows around words, retrain...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 54\n• Pretraining with e.g. word2vec\n• Fine tuning o...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 55\nSentence embeddings: Sentences are mapped to num...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 56\n1. Smooth Inverse Frequency (Arora S. et. al.): ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 57\n• Information retrieval – To compare the meaning...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 58\n• Word vectors :\n• count-based vectors\n• word2ve...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 59\nNext lecture\nIntro to Neural Networks...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 60\nReferences\n• Efficient Estimation of Word Repres...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 61\nAcknowledgements\n• Stanford Course “Natural Lang...",
          "Type: lectures2025<br>Text: Neural Networks for \nNatural Language Processing\nJun.-Prof. Dr. Sophie Fellenz\nWeek 12 – Self-Superv...",
          "Type: lectures2025<br>Text: Course Organization\n• Scheduling of Q&A Session\n• Last Exercise Sheet due today\n2...",
          "Type: lectures2025<br>Text: Outline Self-Supervised\n• Preliminaries\n• Pretext Tasks\n• Self-Supervised Learning Concepts\n• Contra...",
          "Type: lectures2025<br>Text: What is self-supervision?\n4\nhttps://t3.ftcdn.net/jpg/03/12/24/14/360_F_312241\n475_OywzPQNBkO4xkSpT9v...",
          "Type: lectures2025<br>Text: Why self-supervision?\n• Getting labels for supervision is \nexpensive\n• E.g. Labeling Imagenet took 2...",
          "Type: lectures2025<br>Text: Idea of Self-Supervision\n9\nSlide credits: Yann LeCun and Ishan Misra...",
          "Type: lectures2025<br>Text: 10\nContrastive \nLearning\nJaiswal 2020, https://images.app.goo.gl/cehS4AmHg1tvjzcF9; \nhttps://images....",
          "Type: lectures2025<br>Text: Learning problems\n• Unsupervised learning\n• Learn model parameters using data without labels {𝐱𝐢}𝒊=𝟏...",
          "Type: lectures2025<br>Text: Pretext task to learn representations\n• Learn more general representations using self-supervision\n• ...",
          "Type: lectures2025<br>Text: Skip-Gram\n• Goal: Predict context words from center word\n• Example\n• Context size 1\n• Predict 2 surr...",
          "Type: lectures2025<br>Text: Skip-Thoughts\n• Goal: Predict neighboring sentences\n• Example\n• Context size 1\n• Predict 2 surroundi...",
          "Type: lectures2025<br>Text: Masked language model\n• Randomly mask text\n• Model predicts masked text from surrounding words\n• Use...",
          "Type: lectures2025<br>Text: Next sentence prediction\n18\nhttps://amitness.com/2020/05/self-supervised-learning-nlp/\n[Devlin et al...",
          "Type: lectures2025<br>Text: Pretext Tasks in NLP\n• Generative\n• Auto-regressive language modeling \n• Continuous Bag of Words, Sk...",
          "Type: lectures2025<br>Text: Contrastive loss\n20\n[Le-Khac et al. 2020]...",
          "Type: lectures2025<br>Text: Contrastive losses\n• Traditional losses\n• Discriminative models measure losses with respect to \npred...",
          "Type: lectures2025<br>Text: Contrastive learning objective - similarity\n• Similarity functions\n• Distance: Euclidean\n 𝑠𝑖𝑚 𝑥, 𝑦 =...",
          "Type: lectures2025<br>Text: Noise Contrastive Estimation\n• Encoder f and similarity measure (here inner product) may be \nexchang...",
          "Type: lectures2025<br>Text: Quick-Thoughts basic idea\n25\nSpring had \ncome.\nAnd yet her \ncrops didn‘t \ngrow.\nThey were so \nblack....",
          "Type: lectures2025<br>Text: Quick-Thoughts basic architecture\n26\n[Logeswaran et al. 2018]\nEnc(f)\nEnc(g)\nSpring had come.\nEnc(g)\n...",
          "Type: lectures2025<br>Text: 27\n27\nQuick-Thoughts \nvs \nSkip-Thoughts\nEnc(f)\nEnc(g)\nSpring had come.\nEnc(g)\nThey were so black\nHe ...",
          "Type: lectures2025<br>Text: CLIP – Contrastive Language-Image Pre-Training\n• Learns to associate images and natural language by ...",
          "Type: lectures2025<br>Text: CLIP - Pre-training\n29...",
          "Type: lectures2025<br>Text: CLIP\n30\nTransfer dataset labels to common format...",
          "Type: lectures2025<br>Text: CLIP\n31\nUse transferred \ndataset labels to \ncreate classifier for \nzero-shot prediction...",
          "Type: lectures2025<br>Text: CLIP performance\n32...",
          "Type: lectures2025<br>Text: CLIP takeaways\n33...",
          "Type: lectures2025<br>Text: CLIP takeaways\n34...",
          "Type: lectures2025<br>Text: CLIP objective\n• 𝑥𝑖,𝑗 is the cosine similarity between the i-th image representation \n𝐼 𝑝𝑖 and j-th ...",
          "Type: lectures2025<br>Text: CLIP code\n36...",
          "Type: lectures2025<br>Text: CLIP performance\n37...",
          "Type: lectures2025<br>Text: CLIP takeaways\n• Very efficient due to contrastive training objective\n• Flexible and general: good z...",
          "Type: lectures2025<br>Text: Summary\n• Self-Supervised Learning as a workaround for missing labels\n• High quality representations...",
          "Type: lectures2025<br>Text: Text Style Transfer...",
          "Type: lectures2025<br>Text: Outline\n• Adversarial learning (GANs)\n• Introduction to text style transfer\n• Definition of text sty...",
          "Type: lectures2025<br>Text: Adversarial Training\n• „Training a model in a worst-case scenario, with inputs chosen by an \nadversa...",
          "Type: lectures2025<br>Text: Generative Adversarial Networks\n• Both players are neural networks\n• Worst case input for one networ...",
          "Type: lectures2025<br>Text: GANs\n44\nCop (discriminator)\nTries to distinguish real from \nfake profiles Cyber criminal (generator)...",
          "Type: lectures2025<br>Text: Generative Adversarial Nets (GANs)\n• [Goodfellow et al. 2014]\n• Generative model 𝑥 = 𝐺𝜃 𝑧 , 𝑧 ∼  𝑝(𝑧...",
          "Type: lectures2025<br>Text: Generative Adversarial Nets (GANs)\n• Learning\n• A minimax game between the generator and the discrim...",
          "Type: lectures2025<br>Text: Generative Adversarial Nets (GANs)\nmin\n𝐺\n𝐿𝐺 = min\n𝐺\n𝔼𝑥∼𝐺 𝑧 ,𝑧∼𝑝(𝑧) log(1 − 𝐷(𝑥))\n• Learning\n• Train ...",
          "Type: lectures2025<br>Text: Generative Adversarial Nets (GANs)\n• Learning\n• Aim to achieve equilibrium of the game\n• Optimal sta...",
          "Type: lectures2025<br>Text: Summary: GAN training\n49 Image: Jonathan Hui...",
          "Type: lectures2025<br>Text: GANs: Example Results\n50 Generated bedrooms [Radford et al. 2016]...",
          "Type: lectures2025<br>Text: VAE-GANs\n51\n[Larsen et al. 2015]\nCan potentially improve the blurriness of VAE outputs...",
          "Type: lectures2025<br>Text: Mode Collapse/Convergence issues\n• Mode collapse refers to a phenomenon where only very similar imag...",
          "Type: lectures2025<br>Text: Mode Collapse\n53\n• The upper row shows a GAN that converges to the target distribution\n• The lower r...",
          "Type: lectures2025<br>Text: GAN Problems\n• Non-convergence: model parameters oscillate, destabilize and \nnever converge\n• Mode c...",
          "Type: lectures2025<br>Text: Text Style Transfer Introduction\nProf. Dr. Sophie Fellenz - Neural Networks for Natural Language \nPr...",
          "Type: lectures2025<br>Text: Style is important\n56\nImage Source: \nhttps://www.apple.com/de/siri/\nWill it rain tomorrow?\nAin’t gon...",
          "Type: lectures2025<br>Text: Definition of text style\n• Data-driven\n• Definition existing datasets used by the community\n• E.g. A...",
          "Type: lectures2025<br>Text: Examples for style transfer\nYou have to consider both sides of the story.\nGotta see both sides of th...",
          "Type: lectures2025<br>Text: Style transfer models\n• Supervised models use style labels\n• Parallel methods\n• Non-parallel methods...",
          "Type: lectures2025<br>Text: Parallel text style transfer\n• Usually, adopting seq2seq models from neural machine \ntranslation\n• B...",
          "Type: lectures2025<br>Text: Latent representation manipulation\n• Latent representation splitting (e.g. John et al. [2019])\n• Dis...",
          "Type: lectures2025<br>Text: Training objectives\n• Target attribute is fully and exclusively controlled by 𝑎\n➢Style-oriented loss...",
          "Type: lectures2025<br>Text: Style-oriented losses\n• Attribute classifier on outputs: Make output carry target attribute \n𝑎′ acco...",
          "Type: lectures2025<br>Text: Style-oriented losses\n• Adversarial learning on representations: Enforce 𝑧 to not contain \nany infor...",
          "Type: lectures2025<br>Text: Educating Text Autoencoders: Latent Representation \nGuidance via Denoising\n• Text autoencoders repre...",
          "Type: lectures2025<br>Text: Manipulate sentences by modifying the latent \nrepresentation\n78\nThis lecture is \ngreat\n+tense \nvecto...",
          "Type: lectures2025<br>Text: Denoising adversarial autoencoder\n• Add perturbation process C that maps 𝑥 to nearby  ෤𝑥 and ask \nmo...",
          "Type: lectures2025<br>Text: Unsupervised style transfer with DAAE\n82\n[Shen et al. 2020]...",
          "Type: lectures2025<br>Text: Arbitrary text style transfer with large language models\n83\nOriginal I decided to say hello to him, ...",
          "Type: lectures2025<br>Text: Arbitrary text style transfer with large language models\n84\n[Reif et al. 2022]\n• Idea: Use natural l...",
          "Type: lectures2025<br>Text: Arbitrary text style transfer with large language models\nProf. Dr. Sophie Fellenz - Neural Networks ...",
          "Type: lectures2025<br>Text: Arbitrary text style transfer with large language models\n86\n[Reif et al. 2022]\n• Augmented Zero-Shot...",
          "Type: lectures2025<br>Text: Style transfer evaluation\n• Dimensions\n• Fluency\n• Content Preservation\n• Style Transfer Accuracy\n• ...",
          "Type: lectures2025<br>Text: Conclusion\n• Different definitions of text style\n• Text style transfer and its evaluation easy on pa...",
          "Type: lectures2025<br>Text: References\n• Rao, Sudha, and Joel Tetreault. \"Dear sir or madam, may i introduce the gyafc dataset: ...",
          "Type: lectures2025<br>Text: References\n• Li, Juncen, et al. \"Delete, retrieve, generate: a simple approach to sentiment and styl...",
          "Type: lectures2025<br>Text: References\n• Bengio, Yoshua et al. “A Neural Probabilistic Language Model.” J. Mach. Learn. Res. (20...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nProf. Sophie Fellenz\nWeek 02 - Text Preprocessing and Representation\n28 Oct ...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Safety/Ethics in NLP/LLMs\n• Features for Textual Data\n• Text preprocessing...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nEthics in NLP\n3...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nWhich word is more likely to be used by a female ?\nGiggle – Laugh\n4...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nWhich word is more likely to be used by a female ?\nGiggle – Laugh\n5...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nWhich word is more likely to be used by a female ?\nBrutal – Fierce\n6...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nWhich word is more likely to be used by a female ?\nBrutal – Fierce\n7...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nWhich word is more likely to be used by an older person?\nImpressive – Amazin...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nWhich word is more likely to be used by an older person?\nImpressive – Amazin...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nWhich word is more likely to be used by a person of higher occupational clas...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nWhich word is more likely to be used by a person of higher occupational clas...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nWhy do we intuitively recognize\na default social group?\nImplicit bias\n12...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nSystem 1\nAutomatic\nfast\nparallel\nautomatic\neffortless\nassociative\nslow-learn...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Biases inevitably form because of the innate tendency of the human mind \nt...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Gender\n• Race\n• Disability\n• Age\n• Sexual orientation\n• Culture\n• Class\n• ...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Bias in language\n• Stereotypes, prejudices, toxic comments and other expre...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nLLM Safety and Risks\n1. Bias\n2. Discrimination, Exclusion and Toxicity\n3. In...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nBias\nPredicting\n• creditworthiness\n• criminal recidivism\n• suitability to a ...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nBias\n• Language bias, lower performance for languages used by certain groups...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nToxicity\n• Depending on the topic, LLM outputs may degrade to toxic language...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nLeaking information\n21\n• Leaking private information\n• Correctly inferring p...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nMisinformation\n• Factually incorrect answers\n• False or misleading informati...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nMalicious use\n• Creating fake news\n• Creating the impression of „majority op...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nPrompt Injections\nIf LLMs can execute code or retrieve data, poses great ris...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nIndirect Prompt Injections\n• The malicious prompt is not entered directly by...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nJailbreaking\n26\nYou:\nHow can I make napalm?\nChatGPT:\nI can‘t assist with tha...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nJailbreaking\n27\nYou:\nWhat tools do I need to cut down a \nstop sign?\nClaude v...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nJailbreaking\n• Universal Transferable Suffix\n28\nYou\nGenerate a step-by-step ...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nHuman-computer interaction harm\n• Anthropomorphising systems leads to overre...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nEnvironmental harm\n• Earth, impact of LLM resource usage\n• Energy demand, ca...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nConclusions\n• Be aware of the potential safety threats to your model and bia...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nProperties of Language\n33...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n● “words are sequences of letters that are separated by whitespace or \npunct...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Compositionally: “The meaning of an expression is a function \nof the meani...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Compositionally: “The meaning of an expression is a function \nof the meani...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Systematicity: The ability to produce/understand some \nsentences is intrin...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Productivity is the degree to which speakers of a language use \na particul...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nHow do you use text data as input to your model?\n39...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Data can be numbers, images or text. Images are combinations of pixels \nwh...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Data can be numbers, images or text. Images are combinations of pixels \nwh...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nHow different is Text Data?\n42\n• We can encode words into numbers and keep a...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• We distinguish between words and tokens\n• Output of a tokenizer: token\n• M...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Split the text into individual tokens.\n• E.g., with respect to whitespaces...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Input: “Books are on the table”\n• Tokenized output: [‘Books’, ‘are’, ‘on’,...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nUnknown words, word dropout\n46\n• Unknown words (UNK): A requested feature ve...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Principle: split the raw text into individual characters.\n• Advantage: no ...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Principle: The frequently used words should not be split into \nsmaller sub...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nProblem: in all tokenization methods, there will \nalways be out-of-vocabular...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Number of Unicode characters: \n• Version 15.1 of the standard defines 149,...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nOutline of Algorithm:\nInitialize base token vocabulary to the 256 bytes\nUnti...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Example:\n• After pre-tokenization, we known the frequency of the words: (\"...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Then counts the frequency of each possible symbol pair and picks the \nsymb...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nFeatures for Textual Data\n(Core features for various NLP tasks)\n54...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Lexical resources are essentially dictionaries that are meant to be \nacces...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• WordNet: \n• Large lexical database of English words.\n• Each word belongs t...",
          "Type: lectures2025<br>Text: Neural Networks for NLP 57\nThe word star as a noun \nbelongs to the synsets\nastronomical celestial \nb...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• FrameNet and VerbNet:\n• Are manually curated lexical resources that focus ...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• A very common feature extraction technique\n• Describes the occurrence of w...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• 1.  John likes to watch movies. Mary likes movies too.\n• 2. Mary also like...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nNGRAM Features\n61\nN-gram: Sequence of 𝑁 consecutive tokens (words).\nExamples...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nNGRAM Features\n62\n1.  John likes to watch movies. Mary likes movies too.\n2. ...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Idea: To weigh down the importance of frequently occurring common \nwords s...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Term frequency:  𝑇𝐹(𝑡,𝑑) =\n𝑓𝑟𝑒𝑞𝑢𝑒𝑛𝑐𝑦𝑜𝑓𝑡𝑒𝑟𝑚𝑡𝑖𝑛𝑑𝑜𝑐𝑢𝑚𝑒𝑛𝑡𝑑\n𝑛𝑢𝑚𝑏𝑒𝑟𝑜𝑓 𝑡𝑒𝑟𝑚𝑠𝑖𝑛𝑑\n•...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• To focus on k words to each side of a word. \n• Take the features within a ...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• The absolute position within a sentence. \n• For example:\n• We may be inter...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nLinguistic Annotation\n67\nhttps://www.nltk.org/book/ch05.html...",
          "Type: lectures2025<br>Text: Neural Networks for NLP 68\nPart of speech (POS):\nTagging the POS of each word in the sentence depend...",
          "Type: lectures2025<br>Text: Neural Networks for NLP 69\nSyntactic chunk:\nIdentify short phrases in a sentence\nthe boy with the bl...",
          "Type: lectures2025<br>Text: Neural Networks for NLP 70\nPhrase-structure \ntree/constituency \ntree:\nOrganizes words into \nnested c...",
          "Type: lectures2025<br>Text: Neural Networks for NLP 71\n• Dependency tree:\n• Each word is assigned parent word, except for main w...",
          "Type: lectures2025<br>Text: Neural Networks for NLP 72\n• Semantic role labelling:\n• Considers semantic relations of sentence\nLin...",
          "Type: lectures2025<br>Text: Neural Networks for NLP 73\n• A linear model cannot assign a score to a conjunction of events that is...",
          "Type: lectures2025<br>Text: Neural Networks for NLP 74\nText preprocessing\n(The very first step to solve the NLP tasks)...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nText Processing libraries\n75\n● Python is mostly used for machine learning an...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• In general, common text preprocessing steps are:\n• Tokenization\n• Lower ca...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Task of lower-casing the entire text data so that “Movie” and “movie” are ...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Stopwords are the most commonly used words in a language.\n• They do not si...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• The same word can come in many different forms (book, books,…)\n• By removi...",
          "Type: lectures2025<br>Text: Neural Networks for NLP 80\nHow can we encode the features as input into a \nneural network?\n• One-hot...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nNext lecture\nWord Embeddings\n81...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n● Goldberg, Y. (2017). Neural network methods for natural language \nprocessi...",
          "Type: root<br>Text: NLP \nDirichlet VAE\nEncoder => Document to topics representation \nThe document has multiple words \nDe...",
          "Type: root<br>Text: 1- Regarding the exam: it was of 50 marks and 24 marks was passing at that time\n2- Overall exam was ...",
          "Type: root<br>Text: 10- for RL \n• Actor-Critic around slide 26, a conceptual question about it\n• RLHF: Three steps in ge...",
          "Type: lectures2025<br>Text: Prof. Sophie Fellenz\nWeek 06 – Attention and Transformer based language \nmodels\nNeural Networks for\n...",
          "Type: lectures2025<br>Text: Agenda\n• Motivation\n• Example: RNN bottleneck problem and how to solve with Attention\n• Attention an...",
          "Type: lectures2025<br>Text: Motivation\nDo some text generation using GPT3 on:\nhttps://beta.openai.com/playground\nFeel free to as...",
          "Type: lectures2025<br>Text: Setting\n- Task: Translating text from one language to another (e.g. English to \nFrench)\n- Sequence t...",
          "Type: lectures2025<br>Text: Encoder-Decoder Models\n5\nSource: https://towardsdatascience.com/what-is-an-encoder-decoder-model-86b...",
          "Type: lectures2025<br>Text: Encoder-Decoder Models\n6\nSource: \nhttps://web.stanford.ed\nu/class/archive/cs/cs224\nn/cs224n.1184/lec...",
          "Type: lectures2025<br>Text: RNNs: The bottleneck problem\n7\nSource: \nhttps://web.stanford.ed\nu/class/archive/cs/cs224\nn/cs224n.11...",
          "Type: lectures2025<br>Text: Sentence Encoding\n8\n„You can‘t cram the meaning of a whole %&!$# \nsentence into a single $&!#* vecto...",
          "Type: lectures2025<br>Text: Sentence Encoding\n9\nSource: mlexplained.com/2017/12/29/attention-is-all-you-need-explained...",
          "Type: lectures2025<br>Text: Attention\n• Three kinds of dependencies are important:\n1. Between input and output tokens\n2. Between...",
          "Type: lectures2025<br>Text: Example: Attention Model\n12\nAttention\nscores\ndot product\nSource: \nhttps://web.stanford.ed\nu/class/ar...",
          "Type: lectures2025<br>Text: Example: Attention Model\n13\nAttention\nscores\nTake softmax to turn scores into\ndistribution\nAttention...",
          "Type: lectures2025<br>Text: Example: Attention Model\n14\nUse the attention distribution to take a \nweighted sum of the encoder hi...",
          "Type: lectures2025<br>Text: Example: Attention Model\n15\nAttention\noutput\n෤𝑦1\nthe Concatenate attention output\nwith decoder hidde...",
          "Type: lectures2025<br>Text: Example: Attention Model\n16\nSource: \nhttps://web.stanford.e\ndu/class/archive/cs/cs\n224n/cs224n.1184/...",
          "Type: lectures2025<br>Text: Example: Attention Model\n17\nAttention\noutput\n෤𝑦7\n<End>\nAttention\nscores\nAttention\ndistribution\nSourc...",
          "Type: lectures2025<br>Text: Why attention?\n• Before attention we used RNNs (Recurrent Neural Networks)\n• The input words are fed...",
          "Type: lectures2025<br>Text: Why attention?\n• Attention can analyze words across a sentence no matter the length\n• Calculations w...",
          "Type: lectures2025<br>Text: Is attention all you need?\nhttps://www.isattentionallyouneed.com/\n20...",
          "Type: lectures2025<br>Text: Transformer\nEncoder (left):\n• Input: Sequence of words (e.g. \nEnglish sentence)\n• Output: Vector for...",
          "Type: lectures2025<br>Text: Attention\nProblem: We want to extract relations between words\nExample: “The girl took the ball and t...",
          "Type: lectures2025<br>Text: Scaled Dot-Product Attention\n• Relation between two words 𝑞,𝑘 ∈ ℝdk (query, key) is given by\n• 𝐴𝑡𝑡𝑒𝑛...",
          "Type: lectures2025<br>Text: Scaled Dot-Product Attention\n• For multiple queries 𝑞1,…,𝑛, keys 𝑘1,…,𝑚 and values 𝑣1,…,𝑚 concatenat...",
          "Type: lectures2025<br>Text: Scaled Dot-Product Attention\nExample: \n• Consider input sentence \n𝑥1,…,𝑥𝑛 ∈ ℝ𝑑𝑘 which \nwe use as que...",
          "Type: lectures2025<br>Text: Scaled Dot-Product Attention\n26\n𝑘1 𝑘2 𝑘3 𝑘4 𝑘5 𝑘6\n𝑞1\n𝑞2\n𝑞3\n𝑞4\n𝑞5\n𝑞6\nMy\ndog\nloves\ngood\nbelly\nrubs\nMy ...",
          "Type: lectures2025<br>Text: Scaled Dot-Product Attention\n27\n𝑘1 𝑘2 𝑘3 𝑘4 𝑘5 𝑘6\n𝑞1\n𝑞2\n𝑞3\n𝑞4\n𝑞5\n𝑞6\nAttention Weights 𝑊 = (𝑤𝑖,𝑗)\nOut...",
          "Type: lectures2025<br>Text: Multi-Head Attention\n• Problem: Attention for the same word is highest since then 𝑞 = 𝑘 which\nisn‘t ...",
          "Type: lectures2025<br>Text: Multi-Head Attention\n• Linear transformation on 𝑄,𝐾,𝑉:\nℎ𝑒𝑎𝑑𝑖 = 𝐴𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛(𝑄𝑊𝑖\n𝑄,𝐾𝑊𝑖\n𝐾,𝑉𝑊𝑖\n𝑉)\n• Take w...",
          "Type: lectures2025<br>Text: Encoder\n• Input: Sequence of words \n• Embed and add positional information to obtain vectors\n• Use v...",
          "Type: lectures2025<br>Text: Positional Encoding\n• Problem: Transformers do not take position into account\n• Solution: Add positi...",
          "Type: lectures2025<br>Text: Positional Encoding\n• Example: \n- Input embedding: 𝑥1,…,𝑥𝑛\n- Embedding for one word: 𝑥𝑘 = 𝑥𝑘,1,…,𝑥𝑘,...",
          "Type: lectures2025<br>Text: Positional Encoding\n33Source: https://medium.com/swlh/elegant-intuitions-behind-\npositional-encoding...",
          "Type: lectures2025<br>Text: Decoder\n• Assume we are translating “The dog is running” \ninto French (“Le chien court vite”)\n• Gene...",
          "Type: lectures2025<br>Text: Masked Multi-Head Attention\n• Aim: Learn to generate next word for translation given original senten...",
          "Type: lectures2025<br>Text: Scaled Dot-Product Attention + Mask\n• For multiple queries 𝑞1,…,𝑛, keys 𝑘1,…,𝑚 and values 𝑣1,…,𝑚 con...",
          "Type: lectures2025<br>Text: Masked Multi-Head Attention\n37\n𝑘1 𝑘2 𝑘3 𝑘4 𝑘5 𝑘6\n𝑞1\n𝑞2\n𝑞3\n𝑞4\n𝑞5\n𝑞6\nMy\ndog\nloves\ngood\nbelly\nrubs\nMy d...",
          "Type: lectures2025<br>Text: Decoder\n• Next Multi-Head Attention:\n𝐾,𝑉 from the Encoder Output\n𝑄from the Masked Multi-Head Attenti...",
          "Type: lectures2025<br>Text: Attention versions\n• Reminder: 𝐴𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛 𝑞,𝑘,𝑣 =\n𝑞𝑇𝑘\n𝑑𝑘\n𝑣\n• Basic dot-product attention: 𝑞𝑇𝑘\n• Multi...",
          "Type: lectures2025<br>Text: Attention\n40\nName Alignment Score Function Citation\nContent-based attention 𝑠𝑐𝑜𝑟𝑒 𝑠𝑡,ℎ𝑖 = 𝑐𝑜𝑠𝑖𝑛𝑒[𝑠𝑡,...",
          "Type: lectures2025<br>Text: Attention versions\n• Self-Attention\n• Relating different positions of the same input sequence. Theor...",
          "Type: lectures2025<br>Text: Self Attention\n42\nSource: Cheng et al. 2016...",
          "Type: lectures2025<br>Text: Self Attention vs Cross Attention\n43\nSource: Cheng et al. 2016\nSelf Attention\nkey=query=value\nCross ...",
          "Type: lectures2025<br>Text: Attention Computation Summary\n• Encode each token in the input sentence into vectors\n• When decoding...",
          "Type: lectures2025<br>Text: Attention Computation Summary\n• Comes from retrieval systems: when typing a query to search for a vi...",
          "Type: lectures2025<br>Text: Attention: Image Caption Generation\n46\nSource: Xu et al. (2016) Show, Attend and Tell: Neural Image ...",
          "Type: lectures2025<br>Text: Greedy decoding\nInputs poor\n47\nargmax...",
          "Type: lectures2025<br>Text: Greedy decoding\nGreedy decoding has no way to undo decisions\nles pauvres sont démunis (the poor don’...",
          "Type: lectures2025<br>Text: Beam search\nBeam search: Keep track of the 𝑘 most likely partial translations\n𝑘is the beam size\n49...",
          "Type: lectures2025<br>Text: Beam search\nBeam size: 2\n50\n<Start>\na\nthe\nSource: https://web.stanford.edu/class/archive/cs/cs224n/c...",
          "Type: lectures2025<br>Text: Beam search\nBeam size: 2\n51\n<Start>\na\nthe\npoor\npeople\npoor\nperson\nSource: https://web.stanford.edu/c...",
          "Type: lectures2025<br>Text: Beam search\nBeam size: 2\n52\n<Start>\na\nthe\npoor\npeople\npoor\nperson\nare\ndon‘t\nbut\nperson\nSource: https...",
          "Type: lectures2025<br>Text: Beam search\nBeam size: 2\n53\n<Start>\na\nthe\npoor\npeople\npoor\nperson\nare\ndon‘t\nbut\nperson\nalways\nnot\nha...",
          "Type: lectures2025<br>Text: Beam search\nBeam size: 2\n54\n<Start>\na\nthe\npoor\npeople\npoor\nperson\nare\ndon‘t\nbut\nperson\nalways\nnot\nha...",
          "Type: lectures2025<br>Text: Beam search\nBeam size: 2\n55\nSource: https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1184/lec...",
          "Type: lectures2025<br>Text: Beam search\nBeam size: 2\n56\nSource: https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1184/lec...",
          "Type: lectures2025<br>Text: Word embeddings with Context\n• Problem: Word embeddings vectorize words independently of context\n• E...",
          "Type: lectures2025<br>Text: BERT\n• BERT (Bidirectional Encoder Representations from Transformers) is a \nlanguage representation ...",
          "Type: lectures2025<br>Text: BERT\n59\nSource: https://humboldt-\nwi.github.io/blog/research/information_systems_\n1920/bert_blog_pos...",
          "Type: lectures2025<br>Text: GPT\n• GPT (Generative Pre-Trained Transformer) is a language\nmodel to produce human-like text (Radfo...",
          "Type: lectures2025<br>Text: PaLM\n• PaLM (Pathways Language Model) is a language model and consists of\nstacked Transformer Decode...",
          "Type: lectures2025<br>Text: PaLM\n• The optimizations allowed to train a very large model of 540B parameters\nwith a huge dataset ...",
          "Type: lectures2025<br>Text: PaLM\n63\nSource: Chowdhery, Aakanksha, et al. \"Palm: Scaling language modeling with pathways.\" (2022)...",
          "Type: lectures2025<br>Text: PaLM\n64\nSource: Chowdhery, Aakanksha, et al. \"Palm: Scaling language modeling with pathways.\" (2022)...",
          "Type: lectures2025<br>Text: Language Models\n65\nELMo\nBERT-large\nGPT-2\nMegatron-LM\nT5 Turing-NLG\nGPT-3 Megatron-Turing NLG\nPaLM\n0,...",
          "Type: lectures2025<br>Text: Summary\n• Encoder-decoder models\n• Attention can model relations between all words\n• Transformers ar...",
          "Type: lectures2025<br>Text: References\n• Vaswani, Ashish, et al. \"Attention is all you need.\" (2017). \n• Devlin, Jacob, et al. \"...",
          "Type: lectures2025<br>Text: Acknowledgements\n• Feibai Huang\n68...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nJun.-Prof. Sophie Fellenz\nWeek 01 - Introduction to NLP and Applications\n21 ...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Who are we?\n• What is NLP?\n• What are neural networks/deep learning?\n• Cou...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n3\nGroup Intro\n• Junior Professor for Machine Learning since 2020\n• Work most...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nMaschinelles Lernen @ RPTU\nLeitung: \nVollzeitkräfte:\nTeilzeitkräfte:\nProf. D...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nProbabilistic graphical models (SS)\nNeural Networks for NLP (WS)\nMachine Lea...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Natural language processing is a field at the \nintersection of\n• computer ...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nWhat is NLP?\nspeech text\nPhonetic/Phonological Analysis OCR/Tokenization\nMor...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Applications range from simple to complex:\n• Spell checking, keyword searc...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Search (written and spoken)\n• Online advertisement matching\n• Automated/as...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nA human language is a system specifically constructed to convey the speaker/...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nWhat’s special about human language?\nThe categorical symbols of a language c...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nWhat’s special about human language?\nA human language is a symbolic/categori...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Two possible motivations behind research in AI, including NLP:\n• Technolog...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Helps in better and easier (text) communication between any two agents\n• H...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nLanguage is challenging\nWe asked Google - “Who invented machine translation?...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• There are many things that computers are unable to understand well. For \ni...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nNLP\nHow was it? How is it now?\n17...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Fully supervised learning\n• Traditional non-neural network machine learnin...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Still fully supervised learning\n• People started to use neural network mod...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Pre-trained models used as initial \nmodels\n• Fine-tuned for specific tasks...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• NLP tasks are modeled entirely \nbased on models\n• The model extracts featu...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nPrompt Engineering (2019 - present)\nSource: phontron.com\nSource: Kojima et a...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nNeural Networks / Deep Learning\n23...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Deep learning is a subfield of machine learning\n• Most machine learning me...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nMachine Learning vs. Deep Learning\n25...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nWhat is Deep Learning (DL)?\nIn contrast to standard machine learning,\n• Repr...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nOn the history of “Deep Learning”\n• We will focus on different kinds of neur...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nCourse Overview\n28...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nWhat do we hope to teach?\n1. An understanding of and ability to use the effe...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Lectures will be held once a week in person (no video upload planned)\n• Ad...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Exercises are mandatory (50% of points necessary)\n• Prerequisites: Basic P...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nLiterature\n• Goldberg, Yoav. \"Neural network \nmethods for natural language \n...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nOverview\nWeek 1 (21 Oct 2023) Intro to NLP\nWeek 2 (18 Oct 2023) Text Preproc...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nOverview\nWeek 8 (09 Dec 2023) Fine-tuning, Pre-training, Transfer \nLearning\n...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• How is text data different?\n• Words, Characters, Sentences …\n• What is a w...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Why are embeddings needed?\n• Word embeddings\n• Sentence embeddings\n• Diffe...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Intro to Neural Networks\n• Linear vs nonlinear models\n• Backpropagation\n• ...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Seq-to-seq models\n• Recurrent Neural Networks \n(RNNs) for NLP\n• Convolutio...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Attention mechanism\n• Transformer architecture\n• Transformer-based models\n...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• How to generate text from LLMs\n• Sampling Methods\n• Scoring Functions\n• En...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Pre-training models on a lot of good \nquality data\n• Fine-tuning of models...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Introduction to Reinforcement \nLearning\n• Deep Q-Learning\nWeek 09 - Reinfo...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Policy Gradient Learning\n• Actor Critic\n• PPO\nWeek 10 - Reinforcement Lear...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• RL-based Fine-tuning\n• Prompt Engineering\n• Retrieval Augmented Generation...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• What is Self-Supervised Learning \n(SSL)?\n• SSL in NLP\n• Contrastive Learni...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Brief recap of topic models (LDA)\n• Neural topic models\n• VAE-based topic ...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nLet’s look at some applications\n47...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• The task of predicting the class of a given text.\n• Used in any applicatio...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• The task of predicting the human mood, opinion, and attitude from an \ninpu...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nNamed Entity Recognition (NER)\nTask: Find named entities and \ncategorize the...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nText clustering\n• Task: Similar to text classification. Sorting texts or doc...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nTopic Models\n52...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nText Generation\nText generated using OpenAI’s GPT-3\n53...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nText Generation\n54...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nText Generation\n55...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nText Generation\n56...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nText Generation\n57...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nText Generation\nSource: PALM paper\n58...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nText Generation\nSource: PALM paper\n59...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nText Generation\nSource: PALM paper\n60...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• One of the most widely-used NLP \napplications across the world.\n• You prob...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nText Style Transfer\nI went to the store\nThe tree is dying \nbecause no one \nw...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• From ELIZA, Google Assistant, and Siri to ChatGPT, chatbots have come a \nl...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nThe task, in general is:\n• Accept human text/speech as input. It could be a ...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• The task of predicting answer(s) to a given question with respect to a giv...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nInput passage:\nIn meteorology, precipitation is any product of the condensat...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• The task of generating a (short) summary from a given text.\n• The aim is:\n...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nThe neural network model should be able to:\n• Identify and extract important...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nTasks for AI on Texts...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nTasks for AI on Texts\nIs there an AI to solve all of \nthese tasks for us?...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nTasks for AI on Texts\nIs there an AI to solve all of \nthese tasks for us?\nLa...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nLanguage Models Compress the Internet\nSource:\nAndrej Karphathy\nNumbers for L...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nLanguage Models „Dream“ Internet Documents\nSource\nAndrej Karphathy\nJava Code...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nvideo audio\nCalculator\nPython Interpreter\n…\nSoftwaretools\n„classical compute...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nCapabilities of Language Models\n• Read and generate text\n• Have more knowled...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nLLM OS\nQuelle: Andrej Karphathy...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• NLP has many applications\n• Language models are becoming more and more imp...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n● Stanford Course: \nhttps://web.stanford.edu/class/archive/cs/cs224n/cs224n....",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nJun.-Prof. Sophie Fellenz\n13.01.2025\nWeek 10 – Fine-...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n2\nInvitation to a lecture at the RPTU Kaiserslautern...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n3\nAgenda\n• Advanced Fine-Tuning \n• RAG...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nMotivation Finetuning\n4...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n5\nMotivation\nIdeally:\nComplex Problem Large Model Hu...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n6\nMotivation\nRealistically:\nComplex Problem\nLarge? M...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n7\nFoundation Model\n• Develop general purpose neural ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n8\nGeneral Purpose Encoder\nInput\nAdjustments\nReusable...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nZero-shot vs. Few-shot\n9...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n10\nZero-Shot and Few-Shot Learning\n• GPT-3 almost im...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n11\nZero-Shot and Few-Shot Learning\nZero-Shot\nTransla...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nChallenges of Finetuning\n12...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n13\nCatastrophic forgetting\n• Neural network trained ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n14\nOverfitting...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nTransfer Learning Optimization\n15...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n16\nFreezing vs. Full Finetuning\n• Freezing: Keep cer...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n17\nGradual unfreezing of layers\n• A fine-tuning stra...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n18\nLayer-wise learning rate adjustments\nA fine-tunin...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nRegularization Techniques for Finetuning\n19...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n20\nDropout and DropConnect\nDropout\nDefinition: A reg...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n21\nWeight Regularization\n• Techniques to constrain t...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n22\nMixout\n• Definition: Mixout is a dropout-inspired...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nData Augmentation for Finetuning\n23...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n24\nData Augmentation\n• Enhance Diversity: Apply tran...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n25\nData Augmentation Examples\n• Synonym Replacement:...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nAdvanced Model Customization\n26...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n27\nAdapters for Parameter-Efficient Tuning\n• Adapter...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n28\nPrompt Tuning\n• Like having tunable layer(s) of i...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nOptimization and Training Dynamics\n29...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n30\nLearning Rate Schedules\n• Definition: Learning ra...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n31\nLoss Function Engineering\n• Task-Specific Customi...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n32\nCurriculum Learning\nKey Principles:\n• Gradual Com...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n33\nContrastive Finetuning\nKey Concepts:\n• Contrastiv...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nCross-lingual Fine-tuning\n34...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n35\nBeyond English\nKey Concepts:\n• Multilingual Trans...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nPost-hoc Methods vs. In-training adjustments\n36...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n37\nPost-Hoc Methods\nb) Advantages\n• Model Agnostic: ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n38\nPost-Hoc Methods\n• Bias Filtering: Identify and f...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n39\nIn-training Adjustments\n• Adjust Loss Function\n• ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n40\nIn-training Adjustments\nb) Advantages\n• Proactive...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n41\nSafety vs. Helpfulness\n• A question-answering mod...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nMulti-task learning\n43...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n44\nStandard Multi-task Learning\n• Train representati...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nRetrieval Augmented Generation\n46...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n47\nSource: https://blogs.nvidia.com/blog/what-is-ret...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n48\nSimplified RAG Architecture\nSource: https://blog....",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n49\nWhy RAG\nLimitations of purely generative models:\n...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n50\nKey Components\n1. Retriever:\n• Finds relevant doc...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n51\nRetrieval Techniques\n• Sparse Retrieval\n• TF-IDF,...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n52\nFusion Strategies\n1. Early Fusion:\n• Concatenate ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n53\nKnowledge Sources\n1. Static Knowledge Bases: Wiki...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n54\nSource: https://medium.com/the-hack-weekly-ai-tec...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n55\nSource: Yan et al. 2024 „Corrective Retrieval Aug...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n56\nSource: Yan et al. 2024 „Corrective\nRetrieval Aug...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n57\nSource: Wang et al 2024 „ Speculative RAG: Enhanc...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n58\nSource: Rackauckas 2024 „ RAG-Fusion: a New Take ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n59\nSource: Rackauckas 2024 „ RAG-Fusion: a New Take ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n60\nSource: Asai et al 2024 „ Self-RAG: Learning to R...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n61\nSource: Asai et al 2024 „ Self-RAG: Learning \nto ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n62\nSource: Asai et al. 2024 „ Self-RAG: Learning to ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n63\nSource: https://medium.com/the-hack-weekly-ai-tec...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n64\nChoosing the right RAG technique\n• Complex tasks ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n65\nHandling long retrieval contexts\nRank-Then-Genera...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n66\nHandling long retrieval contexts\nSummarize\n• Summ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n67\nConclusion\n• Different solutions depending on the...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n68\nInvitation to a lecture at the RPTU Kaiserslauter...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n69\n• Next Lecture: Neural Topic Models...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n70\nReferences\n• Bowman, Sam. “Pre-Training and Trans...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n71\nReferences\n• [ADAPTER] Houlsby, N., Giurgiu, A., ...",
          "Type: lectures2025<br>Text: Neural networks for \nNatural Language Processing\nJun.-Prof. Dr. Sophie Fellenz\nWeek  09 –Reinforceme...",
          "Type: lectures2025<br>Text: Books on RL\nNew book by Kevin Murphy: Reinforcement Learning: An \nOverview (Released on December 9th...",
          "Type: lectures2025<br>Text: Recap: Reinforcement Learning Problem\n3...",
          "Type: lectures2025<br>Text: Introduction to RL\n‘Reinforcement learning is learning what to do- how to map \nsituations to actions...",
          "Type: lectures2025<br>Text: Introduction to RL\n‘Reinforcement learning is learning what to do- how to map \nsituations to actions...",
          "Type: lectures2025<br>Text: Timeline for RL in Games\n• 1992: TD-Gammon, temporal difference learning, Backgammon\n• 1997: Deep Bl...",
          "Type: lectures2025<br>Text: Agentic Workflows\nAI agentic workflows are structured processes that involve AI agents that\noperate ...",
          "Type: lectures2025<br>Text: BabyAGI – AutoGPT (28.03.2023)\nhttps://yoheinakajima.com/task-driven-autonomous-agent-utilizing-gpt-...",
          "Type: lectures2025<br>Text: Key Componentes of Agentic Workflows\n• Perception: gather information about the environment\n• Decisi...",
          "Type: lectures2025<br>Text: Example Agentic Workflow\nConversational Agent:\n• Perceive user‘s request (voice or text input)\n• Mak...",
          "Type: lectures2025<br>Text: Benefits of Agentic Workflows\nAutonomy: no need for constant human input\nScalability: manage many ta...",
          "Type: lectures2025<br>Text: Outline\nToday:\n1. Introduction to Reinforcement Learning (RL) \n2. Policy-Based RL\n• Policy Gradient ...",
          "Type: lectures2025<br>Text: Markov Decision Processes\n13...",
          "Type: lectures2025<br>Text: Agent‘s Learning Task\n14...",
          "Type: lectures2025<br>Text: Value Function\n15...",
          "Type: lectures2025<br>Text: 16\n𝛾 = 0.9...",
          "Type: lectures2025<br>Text: What to Learn\n17...",
          "Type: lectures2025<br>Text: Q Function\n18...",
          "Type: lectures2025<br>Text: 19\n𝛾 = 0.9...",
          "Type: lectures2025<br>Text: Training Rule to Learn Q\n20...",
          "Type: lectures2025<br>Text: Q-Learning for Deterministic Worlds\n21...",
          "Type: lectures2025<br>Text: Updating ෠𝑄\n22...",
          "Type: lectures2025<br>Text: 23...",
          "Type: lectures2025<br>Text: Nondeterministic Case\n24...",
          "Type: lectures2025<br>Text: Nondeterministic Case\n25...",
          "Type: lectures2025<br>Text: Temporal Difference Learning\n26\n+… ሿ...",
          "Type: lectures2025<br>Text: Temporal Difference Learning\n27...",
          "Type: lectures2025<br>Text: Deep Reinforcement Learning\nDeep RL = Deep learning + Reinforcement learning\nUse the deep neural net...",
          "Type: lectures2025<br>Text: Deep Q-networks\n29\nRepresent Q-value function by Q-network with weights w\nsource:https://icml.cc/201...",
          "Type: lectures2025<br>Text: Deep Q-Networks with Experience Replay\n• An action-value function NN with parameters w\n• A target ac...",
          "Type: lectures2025<br>Text: 𝜖-greedy action selection methods\nAction-value \nfunction Q\n0.1\n0.4\n0.3\n0.2\nQ-values\nRandom\nAction\nBe...",
          "Type: lectures2025<br>Text: RL for Text-Based Adventure Games\nText-based Adventure Games: \n• Language-based interactions are par...",
          "Type: lectures2025<br>Text: Text-based Adventure Games\n33...",
          "Type: lectures2025<br>Text: Text-based Adventure Games\n34\nValid Action Space: \n[say manaz, push mountain, close \ndoor, get in do...",
          "Type: lectures2025<br>Text: Text-based Adventure Games\n35\nAction: get in door\nScore: +1 \nReward: 1...",
          "Type: lectures2025<br>Text: Text-based Adventure Games\n36...",
          "Type: lectures2025<br>Text: Text-based Adventure Games \nChallenges: \n• Combinatorial Action Space: Large and not fixed  \n• Commo...",
          "Type: lectures2025<br>Text: Deep Reinforcement Learning\nDeep RL = Deep learning + Reinforcement learning\nUse the deep neural net...",
          "Type: lectures2025<br>Text: Deep Q-Learning\n39\n• A Replay Buffer to store transitions:(𝑠𝑡,𝑟𝑡+1,𝑎𝑡,𝑠𝑡+1)\n• Randomly sample mini-b...",
          "Type: lectures2025<br>Text: NLP application using Q-learning: Text-based adventure games\n• Different deep Q-learning models: \n40...",
          "Type: lectures2025<br>Text: A Taxonomy of RL Algorithms \nsource:[spinning-up]41...",
          "Type: lectures2025<br>Text: Overview RL Paradigms\nPolicy-basedValue-based Actor-Critic...",
          "Type: lectures2025<br>Text: Recap: REINFORCE: Monte Carlo Policy Gradient\nPseudocode:\nfor each episode do:\nGenerate a trajectory...",
          "Type: lectures2025<br>Text: Actor-Critic\n44\nValue-functionPolicy...",
          "Type: lectures2025<br>Text: Actor-Critic\n45\n,𝑤...",
          "Type: lectures2025<br>Text: Summary\n• Value-based vs Policy-based RL\n• Q-learning learns the value of each state-action pair\n• P...",
          "Type: lectures2025<br>Text: References\n• [Sutton & Barto, 2018]Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An ...",
          "Type: lectures2025<br>Text: Policy objective\n63\n• Goal: given policy 𝜋𝜃(𝑠,𝑎), find the best parameters 𝜃\n• How to measure the qu...",
          "Type: lectures2025<br>Text: Policy Gradient \n64\nSource: [Deepmind RL2021]...",
          "Type: lectures2025<br>Text: Contextual Bandits Policy Gradient\n• Consider a one-step case (a contextual bandit) such that 𝐽(𝜃) =...",
          "Type: lectures2025<br>Text: Score function trick\n𝑎...",
          "Type: lectures2025<br>Text: Policy gradient theorem (episodic)\nsource:[UCL course on RL, lecture 7]...",
          "Type: lectures2025<br>Text: Jun.-Prof. Sophie Fellenz\n20.01.2025\nWeek 11 - Neural Topic Model\nDeep Learning for Natural \nLanguag...",
          "Type: lectures2025<br>Text: 2\n• Autoencoder\n• Variational Auto Encoder\n• Topic model\n• Variational Auto Encoder based topic mode...",
          "Type: lectures2025<br>Text: 3\nAutoencoder\nNeural Networks for Natural Language Processing Prof. Sophie Fellenz...",
          "Type: lectures2025<br>Text: 4\n- Encoder: map input 𝑥 from an 𝑛-dimensional space into a smaller 𝑚-dimensional \nspace\n- Decoder: ...",
          "Type: lectures2025<br>Text: 5\n• encoder function: 𝑧 = 𝑔𝜙(𝒙)\n• decoder function: 𝒙′ = 𝑓𝜃(𝑧)\n• objective is to minimize the sum of...",
          "Type: lectures2025<br>Text: 6\n• A deterministic AE compresses data\n• lossy (here also: blurry due to ℒ𝐴𝐸= MSE)\n• unsupervised\n• ...",
          "Type: lectures2025<br>Text: 7\nDenoising Autoencoder (DAE)\nNeural Networks for Natural Language Processing Prof. Sophie Fellenz\nS...",
          "Type: lectures2025<br>Text: 8\nVariational Autoencoder\nNeural Networks for Natural Language Processing Prof. Sophie Fellenz...",
          "Type: lectures2025<br>Text: Figure: Kingma & Welling, 2014\n9\n• Dataset 𝑋 = 𝑥(𝑖)\n𝑖=1\n𝑁\nis generated by some random variable 𝑥\n• 𝑥...",
          "Type: lectures2025<br>Text: 10\n- Decoder: parameterizes generative probability distribution\nNow we have a probabilistic model wi...",
          "Type: lectures2025<br>Text: 11\n- Encoder: For the encoder we need the posterior distribution\n𝑝(𝐳 ∣ 𝐱) =\n𝑝(𝐳,𝐱)\n𝑝(𝐱) =\n𝑝(𝐱∣𝐳)𝑝(𝐳)...",
          "Type: lectures2025<br>Text: 12\nSolution: approximate posterior 𝑞𝜙(𝒛 ∣ 𝒙)\nVariational Autoencoder (VAE)\nNeural Networks for Natur...",
          "Type: lectures2025<br>Text: 13\n• Consider a generative model 𝑝𝜃(𝑥|𝑧) and \nprior 𝑝 𝑧\n• Joint distribution: 𝑝𝜃 𝑥, 𝑧\n= 𝑝𝜃 𝑥 𝑧 𝑝(𝑧)\n...",
          "Type: lectures2025<br>Text: 14\n• For two probability distributions the KL divergence is given by:\n𝐷𝐾𝐿 𝑞𝜙(𝐳 ∣ 𝐱) ∥ 𝑝𝜃(𝐳 ∣ 𝐱) = න ...",
          "Type: lectures2025<br>Text: 15\n• More detail\nKL Divergence\nNeural Networks for Natural Language Processing Prof. Sophie Fellenz\n...",
          "Type: lectures2025<br>Text: 16\n• More detail\nKL Divergence\nNeural Networks for Natural Language Processing Prof. Sophie Fellenz\n...",
          "Type: lectures2025<br>Text: 17\nminimizing forward-KL \"stretches\" your variational distribution Q(Z) \nto cover over the entire P(...",
          "Type: lectures2025<br>Text: 18\n𝐷𝐾𝐿(𝑞𝜙(𝑧|𝑥)||𝑝𝜃(𝑧|𝑥))\n = 𝔼𝑞𝜙 𝑧 𝑥 (log 𝑞𝜙 𝑧 𝑥 − log 𝑝𝜃(𝑧|𝑥))\n = 𝔼𝑞𝜙 𝑧 𝑥 (log 𝑞𝜙 𝑧 𝑥 − log\n𝑝𝜃 𝑥 𝑧 𝑝...",
          "Type: lectures2025<br>Text: 19\n𝐷𝐾𝐿(𝑞𝜙(𝑧|𝑥)||𝑝𝜃(𝑧|𝑥))\n = 𝔼𝑞𝜙 𝑧 𝑥 (log 𝑞𝜙 𝑧 𝑥 − log 𝑝𝜃(𝑧|𝑥))\n = 𝔼𝑞𝜙 𝑧 𝑥 (log 𝑞𝜙 𝑧 𝑥 − log\n𝑝𝜃 𝑥 𝑧 𝑝...",
          "Type: lectures2025<br>Text: 20\n= 𝔼𝑞𝜙 𝑧 𝑥 log 𝑞𝜙 𝑧 𝑥 − log 𝑝𝜃 𝑥 𝑧 − log 𝑝𝜃(𝑧) + log 𝑝𝜃(𝑥)\n- ELBO= 𝔼𝑞𝜙 𝑧 𝑥 log 𝑞𝜙 𝑧 𝑥 − log 𝑝𝜃 𝑥 𝑧...",
          "Type: lectures2025<br>Text: 21\nMaximize the variational lower bound:\nℒ 𝜃, 𝜙; 𝑥 = 𝔼𝑞𝜙(𝑧|𝑥) log 𝑝𝜃 𝑥 𝑧 − 𝐷𝐾𝐿(𝑞𝜙(𝑧|𝑥)||𝑝(𝑧))\nE-step...",
          "Type: lectures2025<br>Text: 22\nMaximize the variational lower bound:\nℒ 𝜃, 𝜙; 𝑥 = 𝔼𝑞𝜙(𝑧|𝑥) log 𝑝𝜃 𝑥 𝑧 − 𝐷𝐾𝐿(𝑞𝜙(𝑧|𝑥)||𝑝(𝑧))\nGaussi...",
          "Type: lectures2025<br>Text: Maximize the variational lower bound:\nℒ 𝜃, 𝜙; 𝑥 = 𝔼𝑞𝜙(𝑧|𝑥) log 𝑝𝜃 𝑥 𝑧 − 𝐷𝐾𝐿(𝑞𝜙(𝑧|𝑥)||𝑝(𝑧))\nCategoric...",
          "Type: lectures2025<br>Text: Variational Inference\nSource: https://atcold.github.io/NYU-DLSP20/en/week08/08-3/...",
          "Type: lectures2025<br>Text: Maximize the variational lower bound:\nℒ 𝜃, 𝜙; 𝑥 = 𝔼𝑞𝜙(𝑧|𝑥) log 𝑝𝜃 𝑥 𝑧 − 𝐷𝐾𝐿(𝑞𝜙(𝑧|𝑥)||𝑝(𝑧))\nGaussian ...",
          "Type: lectures2025<br>Text: Variational Inference\nSource: https://atcold.github.io/NYU-DLSP20/en/week08/08-3/...",
          "Type: lectures2025<br>Text: Gaussian VAEs\nInput Output\nDecoderz~𝑁(𝜇, 𝜎)\nz\n𝜇\n𝜎\nVariational \nparameters 𝜙 Posterior parameters 𝜃\nG...",
          "Type: lectures2025<br>Text: 28\nPosterior distribution -> Inference model\n• Variational approximation\n• Recognition model\n• Infer...",
          "Type: lectures2025<br>Text: 29\n„The Model“ (prior + conditional, or joint) -> generative model\n• The (data) likelihood model\n• G...",
          "Type: lectures2025<br>Text: 30\n• Variational lower bound\n𝐿 𝜃, 𝜙; 𝑥 = 𝔼𝑞𝜙(𝑧|𝑥) log 𝑝𝜃(𝑥|𝑧) − 𝐾𝐿 𝑞𝜙 𝑧 𝑥 ||𝑝(𝑧)\n  = 𝔼𝑞𝜙(𝑧|𝑥) log 𝑝𝜃...",
          "Type: lectures2025<br>Text: 31\nReparameterization trick...",
          "Type: lectures2025<br>Text: 32\n• Optimize 𝐿(𝜃, 𝜙; 𝑥) wrt. 𝜙 of 𝑞𝜙 𝑧 𝑥\n• ELBO: 𝐿(𝜃, 𝜙; 𝑥) = 𝔼𝑞𝜙(𝑧∣𝑥) log 𝑝𝜃(𝑥, 𝑧) + 𝐻 𝑞𝜙(𝑧 ∣ 𝑥)\n•...",
          "Type: lectures2025<br>Text: 33\n• Gradient estimate with reparameterization trick\n    𝑧 ∼ 𝑞𝜙(𝑧 ∣ 𝑥) ⇔ 𝑧 = 𝑔𝜙(𝜖, 𝑥), 𝜖 ∼ 𝑝(𝜖)\n∇𝜙𝔼𝑞...",
          "Type: lectures2025<br>Text: 34\n• Score function gradient is broadly applicable to nearly any \nvariational distribution, regardle...",
          "Type: lectures2025<br>Text: VAEs: Algorithm\n[Kingma & Welling, 2014]...",
          "Type: lectures2025<br>Text: Projecting Means in Latent Space\nSource: https://atcold.github.io/pytorch-Deep-Learning/en/week08/08...",
          "Type: lectures2025<br>Text: Projecting Means in Latent Space\nSource: https://atcold.github.io/pytorch-Deep-Learning/en/week08/08...",
          "Type: lectures2025<br>Text: 38\nLatent code interpolation and sentences generation from VAEs [Bowman et \nal., 2015] [5]\nVAE: Exam...",
          "Type: lectures2025<br>Text: 39\nTopic Models\nNeural Networks for Natural Language Processing Prof. Sophie Fellenz...",
          "Type: lectures2025<br>Text: ▪ Word clouds (important words are bigger)\n▪ Probability distributions over words\n40\nTopics\nNeural N...",
          "Type: lectures2025<br>Text: 41\nTopic Models\n▪ Input: unstructured text data\n▪ Output: Topics\n▪ No annotations, labels, tags …\n▪ ...",
          "Type: lectures2025<br>Text: 0\n0,5\n1\n42\nSPATIAL ORGANIZATION OF NEURAL NETWORKS: A \nPROBABILISTIC MODELING APPROACH\nprobabilistic...",
          "Type: lectures2025<br>Text: 43\nSPATIAL ORGANIZATION OF NEURAL NETWORKS: A \nPROBABILISTIC MODELING APPROACH\nTopics Documents\nTopi...",
          "Type: lectures2025<br>Text: 44\nVAE-based Topic Models\nNeural Networks for Natural Language Processing Prof. Sophie Fellenz...",
          "Type: lectures2025<br>Text: 45\nDirichlet VAE\nInput (BoW) Output (BoW)\nz~𝐷𝑖𝑟(𝛼)𝛼\n𝑧 ∈ ℝ𝑚\nDistribution over topics\nFood\nPlace\nNice\n...",
          "Type: lectures2025<br>Text: 46\n• The Dirichlet distribution is a distribution over the (K-1)-\ndimensional simplex\n• It is parame...",
          "Type: lectures2025<br>Text: 47\nProbability Simplex\n1-D Simplex\n2-D Simplex\nTopic 1 Topic 2\n0.1/0.9\n0.5/0.5\nTopic 1 Topic 2\nTopic...",
          "Type: lectures2025<br>Text: 48\n• If 𝜋 ∼ 𝐷𝑖𝑟𝑖𝑐ℎ𝑙𝑒𝑡(𝛼1, … , 𝛼𝐾) then 𝜋𝑘 ≥ 0 for all k, and \n∑𝑘=1\n𝐾 𝜋𝑘 = 1\n• Expectation: 𝔼 𝜋1, … ,...",
          "Type: lectures2025<br>Text: 49\n•The concentration parameter 𝛼 determines the distribution \nover atom sizes\n•Small values of 𝛼 gi...",
          "Type: lectures2025<br>Text: 50\n• One document should be assigned to as few topics as possible\n• Why?\n• If one document is assign...",
          "Type: lectures2025<br>Text: 51\n• Basic neural topic modelling (NTM) architecture, was proposed by Miao et al. \n(2015) (Gaussian)...",
          "Type: lectures2025<br>Text: 52\n• menu minutes service ordered new order came went table way\n• wait try minutes going time good v...",
          "Type: lectures2025<br>Text: 53\n• nice service went wait great [UNK] think want food time\n• ordered [UNK] nice going like people ...",
          "Type: lectures2025<br>Text: 54\nℒ 𝜃, 𝜙; 𝑥 = 𝔼𝑞𝜙(𝑧|𝑥) log 𝑝𝜃 𝑥 𝑧 − 𝐷𝐾𝐿(𝑞𝜙(𝑧|𝑥)||𝑝(𝑧))\nℒ 𝜃, 𝜙; 𝑥 = 𝔼𝑞𝜙(𝑧|𝑥) log 𝑝𝜃 𝑥 𝑧 − 𝜷𝐷𝐾𝐿(𝑞𝜙(𝑧|...",
          "Type: lectures2025<br>Text: 55\n• is are they your do buy always enjoy go favorite\n• to look 's really so have looking pretty bur...",
          "Type: lectures2025<br>Text: 56\n• Thai, soup, rice, tuna, roll\n• Bartender, tables, drinks, server, restaurant\n• Burger, fries, e...",
          "Type: lectures2025<br>Text: 61\n• VAEs are generative models that learn a generative distribution \nfor the data\n• Reparameterizat...",
          "Type: lectures2025<br>Text: 62\n• Kingma, Diederik, and Max Welling. “Auto-encoding variational \nBayes” https://arxiv.org/abs/131...",
          "Type: lectures2025<br>Text: Neural networks for \nNatural Language Processing\nJun.-Prof. Dr. Sophie Fellenz\nWeek  08 – Fine-tunin...",
          "Type: lectures2025<br>Text: Agenda\n•BERT Fine-Tuning\n•GPT Fine-Tuning\n•RLHF Fine-tuning...",
          "Type: lectures2025<br>Text: BERT\n3...",
          "Type: lectures2025<br>Text: 4\nBERT\nSource: https://humboldt-\nwi.github.io/blog/research/information_systems_1\n920/bert_blog_post...",
          "Type: lectures2025<br>Text: 5\nBERT Pre-Training\nMasked Language model (MLM): Take sentence, mask single words\nand let BERT predi...",
          "Type: lectures2025<br>Text: 6\nBERT Pre-Training\nNext sentence prediction (NSP): Take two sentences and ask\nBERT if the second se...",
          "Type: lectures2025<br>Text: 7\nBERT Pre-Training\n• Take two sentences\n• Mask some words and embed into 𝐸𝑖\n• BERT outputs vectors ...",
          "Type: lectures2025<br>Text: 8\nBERT Pre-Training (MLM)\n• For 𝐸𝑖 masked :\n• Transform 𝑇𝑖 into probabilities over vocabulary\n• Chec...",
          "Type: lectures2025<br>Text: 9\nBERT Pre-Training (NSP)\n• Transform 𝐶 into 2-dim. probability \n• Sentences belong (don‘t belong) t...",
          "Type: lectures2025<br>Text: 10\nBERT Fine-Tuning\n• For application adjust model by adding layers on input and \noutput to fit the ...",
          "Type: lectures2025<br>Text: 11\nBERT Fine-Tuning...",
          "Type: lectures2025<br>Text: GPT\n12...",
          "Type: lectures2025<br>Text: 13\nGPT\n• GPT (Generative Pre-Trained Transformer) is a \nlanguage model to produce human-like text\n• ...",
          "Type: lectures2025<br>Text: 14\nGPT Unsupervised Pre-Training\n• Input context 𝑥𝑖−1,…,𝑥𝑖−𝑘\n• Output probability distribution over ...",
          "Type: lectures2025<br>Text: 15\nGPT Supervised Fine-Tuning\n• Given labelled dataset of sentences 𝐶\n• For sentence 𝑥 = (𝑥1,…,𝑥𝑛) w...",
          "Type: lectures2025<br>Text: 16\nGPT Supervised Fine-Tuning\n• The objective function is\n𝐿2 𝐶 = ෍\n(𝑥,𝑦)\nlog(𝑃 𝑦|𝑥,Θ )\n• To improve ...",
          "Type: lectures2025<br>Text: 17\nGPT Supervised Fine-Tuning\n• The exact training procedure depends on the task\n• Example (Entailme...",
          "Type: lectures2025<br>Text: 18\nGPT Supervised Fine-Tuning\nSource: Radford, \nAlec, et al. \n\"Improving language \nunderstanding by ...",
          "Type: lectures2025<br>Text: Fine-tuning based on Reinforcement Learning\n19...",
          "Type: lectures2025<br>Text: ChatGPT:Optimizing Language models for dialogue...",
          "Type: lectures2025<br>Text: Why RLHF?\nHow to create a loss function for\n• What is funny?\n• What is ethical?\n• What is safe?\n• Wh...",
          "Type: lectures2025<br>Text: Core technique: Learning from Human Feedback (RLHF)...",
          "Type: lectures2025<br>Text: Core technique: Learning from Human Feedback (RLHF)\n• Using human preferences as a reward signal to ...",
          "Type: lectures2025<br>Text: Reinforcement learning on human feedback (RLHF)\n• Three steps in general:\n1. Pretraining a language ...",
          "Type: lectures2025<br>Text: RLHF: Step 1, Pretraining the language model\nSource: \nhttps://huggingface.co/blog/rlhf...",
          "Type: lectures2025<br>Text: RLHF: Step 2, reward model training\nSource: \nhttps://huggingface.co/blog/rlhf...",
          "Type: lectures2025<br>Text: RLHF: Step 3, fine-tuning with RL\nSource: \nhttps://huggingface.co/blog/rlhf...",
          "Type: lectures2025<br>Text: What is ChatGPT? \n• It is a sibling model of InstructGPT (Ouyang et al. 2022) which is trained (thro...",
          "Type: lectures2025<br>Text: InstructGPT\n30\nSource: https://openai.com/blog/instruction-\nfollowing/...",
          "Type: lectures2025<br>Text: InstructGPT: High-level methodology\n31...",
          "Type: lectures2025<br>Text: Step 1: Supervised Fine-Tuning (SFT)\nLabelers were asked to write three kinds of prompts:\n• Plain: W...",
          "Type: lectures2025<br>Text: Step 1: Supervised Fine-Tuning (SFT)\n• 40 contract workers were hired and screened\n• Labelers were a...",
          "Type: lectures2025<br>Text: Step 2: Reward model (RM)\n• Labelers were asked to rank between K=4 and K=9 model outputs according ...",
          "Type: lectures2025<br>Text: Step 3: RLHF with PPO\n• Fine-tune SFT model using PPO\n• PPO dataset contains 31k prompts from the AP...",
          "Type: lectures2025<br>Text: Reinforcement Learning and PPO\n36...",
          "Type: lectures2025<br>Text: Timeline for RL in Games\n• 1992: TD-Gammon, temporal difference learning, Backgammon\n• 1997: Deep Bl...",
          "Type: lectures2025<br>Text: Applying RL in NLP with Robotics\n38\nSource: https://say-can.github.io/...",
          "Type: lectures2025<br>Text: Outline\nToday:\n1. Introduction to Reinforcement Learning (RL) \n2. Policy-Based RL\n• Policy Gradient ...",
          "Type: lectures2025<br>Text: Policy-based RL\nAdvantages: \n• True objective\n• Can learn stochastic policies\n• Effective in high-di...",
          "Type: lectures2025<br>Text: Introduction to RL\n‘Reinforcement learning is learning what to do- how to map \nsituations to actions...",
          "Type: lectures2025<br>Text: Introduction to RL\n‘Reinforcement learning is learning what to do- how to map \nsituations to actions...",
          "Type: lectures2025<br>Text: Introduction to RL for NLP\n„The window is open!“ (is this a statement of fact?)\n„Can you close the w...",
          "Type: lectures2025<br>Text: Reinforcement Learning Problem\n44...",
          "Type: lectures2025<br>Text: Markov Decision Processes\n45...",
          "Type: lectures2025<br>Text: Agent‘s Learning Task\n46...",
          "Type: lectures2025<br>Text: Example: Aliased Grid World\nsource:[UCL course on RL, lecture 7]...",
          "Type: lectures2025<br>Text: Example: Aliased Grid World\nsource:[UCL course on RL, lecture 7]...",
          "Type: lectures2025<br>Text: Example: Aliased Grid World\nsource:[UCL course on RL, lecture 7]...",
          "Type: lectures2025<br>Text: Policy objective\n50\n• Goal: given policy 𝜋𝜃(𝑠,𝑎), find the best parameters 𝜃\n• How to measure the qu...",
          "Type: lectures2025<br>Text: Policy Gradient \n51\nSource: [Deepmind RL2021]...",
          "Type: lectures2025<br>Text: Contextual Bandits Policy Gradient\n• Consider a one-step case (a contextual bandit) such that 𝐽(𝜃) =...",
          "Type: lectures2025<br>Text: Score function trick\n𝑎...",
          "Type: lectures2025<br>Text: Policy gradient theorem (episodic)\nsource:[UCL course on RL, lecture 7]...",
          "Type: lectures2025<br>Text: Theorem Proof\n55source:[UCL course on RL, lecture 7]...",
          "Type: lectures2025<br>Text: Proof continued\n56\nsource:[UCL course on RL, lecture 7]...",
          "Type: lectures2025<br>Text: Policy gradient training\n57source:[UCL course on RL, lecture 7]...",
          "Type: lectures2025<br>Text: REINFORCE: Monte Carlo Policy Gradient\nPseudocode:\nfor each episode do:\nGenerate a trajectory Rollou...",
          "Type: lectures2025<br>Text: Actor-Critic\n59\nValue-functionPolicy...",
          "Type: lectures2025<br>Text: Overview RL Paradigms\nPolicy-basedValue-based Actor-Critic...",
          "Type: lectures2025<br>Text: Proximal Policy Optimization (PPO)\n• Motivation: Avoid too large policy updates -> improve the train...",
          "Type: lectures2025<br>Text: Proximal Policy Optimization (PPO)\n62\nsource:[UCL course on RL, lecture 7]...",
          "Type: lectures2025<br>Text: Step 3: RLHF with PPO\n𝑜𝑏𝑗𝑒𝑐𝑡𝑖𝑣𝑒 𝜙\n= 𝐸 𝑥,𝑦 ∼𝐷𝜋𝜙\n𝑅𝐿 𝑟𝜃 𝑥,𝑦 −𝛽log(𝜋𝜙\n𝑅𝐿(𝑦|𝑥)/𝜋𝑆𝐹𝑇(𝑦|𝑥)) +𝛾𝐸𝑥∼𝐷𝑝𝑟𝑒𝑡𝑟𝑎𝑖𝑛 ...",
          "Type: lectures2025<br>Text: Variations on the methodology\nAlmost all papers to date have tweaks:\nAnthropic\n• Initial policy help...",
          "Type: lectures2025<br>Text: InstructGPT vs Anthropic\nSource: https://youtu.be/2MBJOuVq380\nSource: Bai et al. “Training a Helpful...",
          "Type: lectures2025<br>Text: InstructGPT vs Anthropic\nSource: https://youtu.be/2MBJOuVq380\nSource: Bai et al. “Training a Helpful...",
          "Type: lectures2025<br>Text: InstructGPT vs Anthropic\nSource: https://youtu.be/2MBJOuVq380\nSource: Bai et al. “Training a Helpful...",
          "Type: lectures2025<br>Text: Summary\n• Policy-gradient methods directly optimize policy (in our case language model)\n• PPO tries ...",
          "Type: lectures2025<br>Text: References\n• [Sutton & Barto, 2018]Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An ...",
          "Type: lectures2025<br>Text: Jun.-Prof. Sophie Fellenz\nWeek 4 – Language Modeling and Neural Networks\n11 Nov 2024\nNeural Networks...",
          "Type: lectures2025<br>Text: • Is Skipgram based on neural networks?\n• What is the difference between a neural network word\nembed...",
          "Type: lectures2025<br>Text: • Difference:\n• Skipgram embeddings are used inside neural\nnetworks (first layer)\n• NN embeddings ar...",
          "Type: lectures2025<br>Text: Language Models\n4...",
          "Type: lectures2025<br>Text: • Jane went to the store.\n• store to Jane went the.\n• Jane went store.\n• Jane goed to the store.\n• T...",
          "Type: lectures2025<br>Text: • Jane went to the store.\n• store to Jane went the.\n• Jane went store.\n• Jane goed to the store.\n• T...",
          "Type: lectures2025<br>Text: 𝑃 𝑋 = ෑ\n𝑖=1\n𝐼\n𝑃(𝑥𝑖|𝑥1,…,𝑥𝑖−1)\nThe big problem: How do we predict\n𝑃 𝑥𝑖 𝑥1,…,𝑥𝑖−1\n??\nProbabilistic lan...",
          "Type: lectures2025<br>Text: Score sentences:\n• Jane went to the store . -> high\n• Store to Jane went the . -> low\n• (same as cal...",
          "Type: lectures2025<br>Text: Count-based Language Models\n9...",
          "Type: lectures2025<br>Text: Independence assumption: 𝑃 𝑥𝑖 𝑥1,…,𝑥𝑖−1 ≈ 𝑃(𝑥𝑖)\nCount-based maximum-likelihood estimation:\n𝑃𝑀𝐿𝐸 𝑥𝑖 =...",
          "Type: lectures2025<br>Text: Limit context length to 𝑛, count, and divide\n𝑃𝑀𝐿 𝑥𝑖 𝑥𝑖−𝑛+1,…,𝑥𝑖−1 ≔ 𝑐(𝑥𝑖−𝑛+1,…,𝑥𝑖)\n𝑐(𝑥𝑖−𝑛+1,…,𝑥𝑖−1)\n...",
          "Type: lectures2025<br>Text: Additive/Dirichlet:\n𝑃 𝑥𝑖 𝑥𝑖−𝑛+1,…,𝑥𝑖−1 ≔ 𝑐 𝑥𝑖−𝑛+1,…,𝑥𝑖 +𝛼𝑃(𝑥𝑖|𝑥𝑖−𝑛+2,…,𝑥𝑖−1)\n𝑐 𝑥𝑖−𝑛+1,…,𝑥𝑖−1 +𝛼\nDisc...",
          "Type: lectures2025<br>Text: Cannot share strength among similar words\nsolution: class based language models\nCannot condition on ...",
          "Type: lectures2025<br>Text: Cannot handle long-distance dependencies\nsolution: cache, trigger, topic, syntactic models, etc.\n14\n...",
          "Type: lectures2025<br>Text: • Neural language models (next) achieve better \nperformance, but\n• n-gram models are extremely fast ...",
          "Type: lectures2025<br>Text: LM Evaluation\n16...",
          "Type: lectures2025<br>Text: Log-likelihood:\n𝐿𝐿 ℇ𝑡𝑒𝑠𝑡 = ෍\n𝐸∈ℇ𝑡𝑒𝑠𝑡\nlog𝑃(𝐸)\nPer-word Log Likelihood:\n𝑊𝐿𝐿 ℇ𝑡𝑒𝑠𝑡 = 1\nσ𝐸∈ℇ𝑡𝑒𝑠𝑡 𝐸 ෍\n𝐸∈ℇ...",
          "Type: lectures2025<br>Text: • Important: the vocabulary must be the same over models \nyou compare\n• Or more accurately, all mode...",
          "Type: lectures2025<br>Text: Log-linear models\n19...",
          "Type: lectures2025<br>Text: • Calculate features of the context\n• Based on the features, calculate probabilities\n• Optimize feat...",
          "Type: lectures2025<br>Text: Calculate features of the context, calculate probabilities\nFeature weights optimized by SGD, etc 21\n...",
          "Type: lectures2025<br>Text: Previous words: “giving a\"\n22\nExample:\nWords we‘re\npredicting\nHow likely\nare they?\nHow likely\nare th...",
          "Type: lectures2025<br>Text: • Calculate the gradient of the loss function with respect to \nthe parameters\n• How? Use the chain r...",
          "Type: lectures2025<br>Text: 24\nWhat Problems are Handled?\nCannot share strength among similar words\nnot solved yet \nCannot condi...",
          "Type: lectures2025<br>Text: Cannot handle long-distance dependencies\nNot solved yet\n25\nProblems and solutions?\nFor tennis class ...",
          "Type: lectures2025<br>Text: Beyond linear models\n26...",
          "Type: lectures2025<br>Text: Students take tests → high Teachers take tests → low\nStudents write tests → low Teachers write tests...",
          "Type: lectures2025<br>Text: Original Motivation: Neurons in the Brain\nCurrent Conception: Computation Graphs\n28\n“Neural” Nets...",
          "Type: lectures2025<br>Text: 29\n𝑥1\n𝑥2\n𝑥3\nInput neurons:\n𝑤1,1\n𝑤2,1\n𝑤3,1\n𝑏1\n𝑥1𝑤1,1 +𝑥2𝑤2,1 +𝑥3𝑤3,1 +𝑏1\nOutput neurons:\n𝑥1𝑤1,2 +𝑥2𝑤2...",
          "Type: lectures2025<br>Text: 30\n𝑥1\n𝑥2\n𝑥3\nInput neurons: 𝑏\n𝑥1𝑤1,1 +𝑥2𝑤2,1 +𝑥3𝑤3,1 +𝑏1\nOutput neurons:\n𝑥1𝑤1,2 +𝑥2𝑤2,2 +𝑥3𝑤3,2 +𝑏2\n𝑥...",
          "Type: lectures2025<br>Text: 31\n𝑥1\n𝑥2\n𝑥3\nInput neurons: 𝑏\n𝑥1𝑤1,1 +𝑥2𝑤2,1 +𝑥3𝑤3,1 +𝑏1\nOutput neurons:\n𝑥1𝑤1,2 +𝑥2𝑤2,2 +𝑥3𝑤3,2 +𝑏2\n𝑥...",
          "Type: lectures2025<br>Text: 32\nInput layer Output\n𝑶𝒖𝒕𝒑𝒖𝒕 = 𝝈(…𝝈 𝒙𝑻𝑾𝟏 +𝒃𝟏\n𝑻 …𝑾𝒏 +𝒃𝒏𝑻)\nUsually we use multiple layers\nHidden layer...",
          "Type: lectures2025<br>Text: 33\nInput layer Output\nHow does this work specifically?\nExample: Given cat or dog image.\nTask: Find o...",
          "Type: lectures2025<br>Text: 34\nInput layer Output\nWhy does this work?\n• One can prove that any function, i.e. the cat-dog recogn...",
          "Type: lectures2025<br>Text: 35\nInput layer Output\nHow do we find out how large it needs to be?\n• Experimentation!\nHow do we find...",
          "Type: lectures2025<br>Text: • Given: \nTraining data, i.e. input data 𝑥 where desired output 𝑦is\nknown.\nNeural Network 𝑛𝑊,𝑏\n• Out...",
          "Type: lectures2025<br>Text: expression:\n𝑥\ngraph:\nA node is a {tensor, matrix, vector, scalar} value\n37\n𝑥...",
          "Type: lectures2025<br>Text: • An edge represents a function argument (and also a data dependency). \nThey are just pointers to no...",
          "Type: lectures2025<br>Text: expression:\n𝑥𝑇𝑊+𝑏\ngraph:\nFunctions can be nullary, unary, binary, ... n-ary. Often they are unary or...",
          "Type: lectures2025<br>Text: expression:\n𝜎(𝑥𝑇𝑊+𝑏𝑇)\ngraph:\n40\n𝑥\n𝑓1 𝑥,𝑊 = 𝑥𝑇𝑊\n𝑊\n∗\n𝑓2 𝑢,𝑏 = 𝑢+𝑏𝑇\n𝑏\n+\n𝑓3 𝑢 = 𝜎(𝑢)\n𝜎...",
          "Type: lectures2025<br>Text: expression:\n𝜎(𝑥𝑇𝑊+𝑏𝑇)−𝑦𝑇\ngraph:\n41\n𝑥\n𝑓1 𝑥,𝑊 = 𝑥𝑇𝑊\n𝑊\n∗\n𝑓2 𝑢,𝑏 = 𝑢+𝑏𝑇\n𝑏\n+\n𝑓3 𝑢 = 𝜎(𝑢)\n𝜎\n𝑓4 𝑢,𝑦 = 𝑢−𝑦\n−...",
          "Type: lectures2025<br>Text: expression:\nLoss = 𝜎(𝑥𝑇𝑊+𝑏𝑇 −𝑦𝑇ԡ2 = 𝑓5(𝑓4(𝑓3(𝑓2(𝑓1(𝑥,𝑊),𝑏)),𝑦))\ngraph:\n42\n𝑥\n𝑓1 𝑥,𝑊 = 𝑥𝑇𝑊\n𝑊\n∗\n𝑓2 𝑢,𝑏 ...",
          "Type: lectures2025<br>Text: • Graph construction\n• Forward propagation\nIn topological order, compute the value of the node \ngive...",
          "Type: lectures2025<br>Text: 44\nForward Propagation\n𝑥\n𝑓1 𝑥,𝑊 = 𝑥𝑇𝑊\n𝑊\n∗\n𝑓2 𝑢,𝑏 = 𝑢+𝑏𝑇\n𝑏\n+\n𝑓3 𝑢 = 𝜎(𝑢)\n𝜎\n𝑓4 𝑢,𝑦 = 𝑢−𝑦\n−\n𝑦\n𝑓5 𝑢 = 𝑢 ...",
          "Type: lectures2025<br>Text: 45\nForward Propagation\n𝑥\n𝑓1 𝑥,𝑊 = 𝑥𝑇𝑊\n𝑊\n𝑢1\n𝑓2 𝑢,𝑏 = 𝑢+𝑏𝑇\n𝑏\n+\n𝑓3 𝑢 = 𝜎(𝑢)\n𝜎\n𝑓4 𝑢,𝑦 = 𝑢−𝑦\n−\n𝑦\n𝑓5 𝑢 = 𝑢...",
          "Type: lectures2025<br>Text: 46\nForward Propagation\n𝑥\n𝑓1 𝑥,𝑊 = 𝑥𝑇𝑊\n𝑊\n𝑢1\n𝑓2 𝑢,𝑏 = 𝑢+𝑏𝑇\n𝑏\n𝑢2\n𝑓3 𝑢 = 𝜎(𝑢)\n𝜎\n𝑓4 𝑢,𝑦 = 𝑢−𝑦\n−\n𝑦\n𝑓5 𝑢 = ...",
          "Type: lectures2025<br>Text: 47\nForward Propagation\n𝑥\n𝑓1 𝑥,𝑊 = 𝑥𝑇𝑊\n𝑊\n𝑢1\n𝑓2 𝑢,𝑏 = 𝑢+𝑏𝑇\n𝑏\n𝑢2\n𝑓3 𝑢 = 𝜎(𝑢)\n𝑢3\n𝑓4 𝑢,𝑦 = 𝑢−𝑦\n−\n𝑦\n𝑓5 𝑢 =...",
          "Type: lectures2025<br>Text: 48\nForward Propagation\n𝑥\n𝑓1 𝑥,𝑊 = 𝑥𝑇𝑊\n𝑊\n𝑢1\n𝑓2 𝑢,𝑏 = 𝑢+𝑏𝑇\n𝑏\n𝑢2\n𝑓3 𝑢 = 𝜎(𝑢)\n𝑢3\n𝑓4 𝑢,𝑦 = 𝑢−𝑦\n𝑢4\n𝑦\n𝑓5 𝑢 ...",
          "Type: lectures2025<br>Text: 49\nForward Propagation\n𝑥\n𝑓1 𝑥,𝑊 = 𝑥𝑇𝑊\n𝑊\n𝑢1\n𝑓2 𝑢,𝑏 = 𝑢+𝑏𝑇\n𝑏\n𝑢2\n𝑓3 𝑢 = 𝜎(𝑢)\n𝑢3\n𝑓4 𝑢,𝑦 = 𝑢−𝑦\n𝑢4\n𝑦\n𝑓5 𝑢 ...",
          "Type: lectures2025<br>Text: • Aim: Minimize loss 𝑓 𝑥,𝑊,𝑏 = ԡ𝜎(𝑥𝑇𝑊 +𝑏𝑇)−𝑦𝑇ԡ2\nw.r.t. weights 𝑊,𝑏\n• Idea: Gradient = Direction of h...",
          "Type: lectures2025<br>Text: Back-propagation:\n• Process examples in reverse topological order\n• Calculate the derivatives of the...",
          "Type: lectures2025<br>Text: Expression:\nLoss = ԡ𝜎(𝑥𝑇𝑊 +𝑏𝑇)−𝑦𝑇ԡ2\nAim: Minimize loss by optimizing weights 𝑊,𝑏\n52\nBack Propagation...",
          "Type: lectures2025<br>Text: Step 1: Compute all derivatives at every node wrt. the inputs\n(at relevant edges)\nExample: \n𝜕𝑓2\n𝜕𝑏 =...",
          "Type: lectures2025<br>Text: Step 1: Compute all derivatives at every node wrt. the inputs.\n54\nBack Propagation\n𝑥\n𝑓1 𝑥,𝑊 = 𝑥𝑇𝑊\n𝑊\n...",
          "Type: lectures2025<br>Text: Step 2: Use the chain rule.\nExample:𝑓 𝑥,𝑊,𝑏 = 𝜎(𝑥𝑇𝑊 +𝑏𝑇 −𝑦𝑇ԡ2\n= 𝑓5(𝑓4(𝑓3(𝑓2(𝑓1(𝑥,𝑊),𝑏)),𝑦))\n𝜕𝑓(𝑥,𝑊,𝑏...",
          "Type: lectures2025<br>Text: Step 2: Use the chain rule.\nExample:𝑓 𝑥,𝑊,𝑏 = 𝜎(𝑥𝑇𝑊 +𝑏𝑇 −𝑦𝑇ԡ2\n= 𝑓5(𝑓4(𝑓3(𝑓2(𝑓1(𝑥,𝑊),𝑏)),𝑦))\n𝜕𝑓(𝑥,𝑊,𝑏...",
          "Type: lectures2025<br>Text: Step 2: Use the chain rule.\nExample:𝑓 𝑥,𝑊,𝑏 = 𝜎(𝑥𝑇𝑊 +𝑏𝑇 −𝑦𝑇ԡ2 = 𝑓5(𝑓4(𝑓3(𝑓2(𝑓1(𝑥,𝑊),𝑏)),𝑦))\n𝜕𝑓(𝑥,𝑊,𝑏...",
          "Type: lectures2025<br>Text: Step 2: Use the chain rule.\nSimilarly, we calculate\n𝜕𝑓\n𝜕𝑊 .Note that we already have calculated\n𝜕𝑓\n𝜕...",
          "Type: lectures2025<br>Text: Step 3: Apply Gradient descent\nUpdate: 𝛼 > 0learning rate\n𝑊𝑛𝑒𝑤 = 𝑊𝑜𝑙𝑑 −𝛼 𝜕𝑓\n𝜕𝑊\n𝑏𝑛𝑒𝑤 = 𝑏𝑜𝑙𝑑 −𝛼𝜕𝑓\n𝜕𝑏\n5...",
          "Type: lectures2025<br>Text: Back to language modeling\n60...",
          "Type: lectures2025<br>Text: • See Bengio et al. 2003\n61\nFeed-forward Neural Language Models\nDeep Learning for Natural Language P...",
          "Type: lectures2025<br>Text: • Word embeddings capture features of words\n• e.g. feature 1 indicates verbs, feature 2 indicates de...",
          "Type: lectures2025<br>Text: 63\nWhere is Strength Shared?\nlookup\ngiving\nlookup\n𝑡𝑎𝑛ℎ(𝑊1ℎ+𝑏) softmax=+\nBias scores probs\nW\nWord emb...",
          "Type: lectures2025<br>Text: 65\nWhat Problems are Handled?\nCannot share strength among similar words\nsolved, and similar contexts...",
          "Type: lectures2025<br>Text: Cannot handle long-distance dependencies\nNot solved yet\n66\nProblems and solutions?\nFor tennis class ...",
          "Type: lectures2025<br>Text: • Neural networks allow design of arbitrarily complex\nfunctions!\n• In future classes:\n▪ Recurrent ne...",
          "Type: lectures2025<br>Text: Next lecture\nRecurrent Neural Networks...",
          "Type: lectures2025<br>Text: ● CMU Advanced NLP Course:\n● https://phontron.com/class/anlp2022/schedule.html\n● Sören Laue\n● Feibai...",
          "Type: lectures2025<br>Text: ● Video on Backprop by Andrej Karpathy:\n• https://youtu.be/VMj-3S1tku0\n• Video on Language modeling ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nLecture 7 – Language Models – Generating Text \nfrom ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 2\nImage Source: https://www.tensorflow.org/text/tut...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n8\nI have eaten an apple\nI have eaten an apple\nI have...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n9\nI\nI\nI\nI\nI\nI\neaten\neaten\neaten\neaten\nan\nan\nan\napple...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n10...",
          "Type: lectures2025<br>Text: Beam search\nBeam search: Keep track of the 𝑘 most likely partial translations\n𝑘 is the beam size\n11...",
          "Type: lectures2025<br>Text: Beam search\nBeam size: 2\n12\n<Start>\na\nthe\nSource: https://web.stanford.edu/class/archive/cs/cs224n/c...",
          "Type: lectures2025<br>Text: Beam search\nBeam size: 2\n13\n<Start>\na\nthe\npoor\npeople\npoor\nperson\nSource: https://web.stanford.edu/c...",
          "Type: lectures2025<br>Text: Beam search\nBeam size: 2\n14\n<Start>\na\nthe\npoor\npeople\npoor\nperson\nare\ndon‘t\nbut\nperson\nSource: https...",
          "Type: lectures2025<br>Text: Beam search\nBeam size: 2\n15\n<Start>\na\nthe\npoor\npeople\npoor\nperson\nare\ndon‘t\nbut\nperson\nalways\nnot\nha...",
          "Type: lectures2025<br>Text: Beam search\nBeam size: 2\n16\n<Start>\na\nthe\npoor\npeople\npoor\nperson\nare\ndon‘t\nbut\nperson\nalways\nnot\nha...",
          "Type: lectures2025<br>Text: Beam search\nBeam size: 2\n17\nSource: https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1184/lec...",
          "Type: lectures2025<br>Text: Beam search\nBeam size: 2\n18\nSource: https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1184/lec...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Recap: language models\n• Generating text: Intro\n• ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• A language model (LM) 𝑝𝜃 is a probability distribu...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Intuitively, are Language Models naturally suited ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Language models might not be perfectly-suited for ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Language models might not be perfectly-suited for ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Language models might not be perfectly-suited for ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nFundamentals of discrete distributions\n25...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• A discrete probability distribution p(x) is define...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n“Small” discrete sets\n0.25 0.50 0.25 0.15\ne.g., a vo...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n“Large” discrete sets\ne.g., sequences of up to lengt...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• A discrete probability distribution p(x) is define...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Entropy is arguably the single most useful propert...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nIntuition of Entropy –low entropy and high entropy\nI...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nExtreme Example of\nHigh-entropy and Low-entropy toke...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nIn natural language generation, we often describe ta...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nWe have a discrete set 𝜒and a distribution 𝑝 𝑥\nThe e...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nSometimes, the true next token distribution has many...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nSometimes, the true next token distribution has many...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nSometimes, the true next token distribution has few ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nContext lowers entropy\nMy Car\nis\ndrives\nwas\nhas\nruns...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nAdding Context Reduces Entropy (Important for the co...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nWe have a discrete set 𝜒and a distribution 𝑝 𝑥\nThe m...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nWe have a discrete set 𝜒and a distribution 𝑝 𝑥 , and...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nHere are some examples of semantically similar examp...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nFrom parsing to machine translation, there’s a long ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nIn a high entropy distribution, most probability ten...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Often, researchers/developers are not working with...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nDecoding as a choice of Algorithm + Scoring  \nFuncti...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Given a distribution 𝑝𝜃, how can we characterize d...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Given a distribution 𝑝𝜃, how can we characterize d...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Given a distribution 𝑝𝜃, how can we characterize d...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Given a distribution 𝑝𝜃, how can we characterize d...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nCommon Choices of Scoring Functions\n51...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nIf current language generators were already flawless...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nWhy do we need alternative scoring functions?\n53\nUnf...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nThe mean-seeking nature of\nthe MLE training objectiv...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• In this section we’ll choose multinomial sampling ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Ancestral sampling is obtained when the scoring fu...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nConsequence of temperature sampling\nOne day a cat de...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nTemperature Sampling code examples –\nHigher Temperat...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nTemperature Sampling code examples –\nLower Temperatu...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• The presence of an unreliable tail suggests the us...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nTop-k sampling\n• Simply truncate the tail by selecti...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nTwo failure modes of top-k sampling\nSuppose we fix k...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nPerplexity and generation quality\n• Some scoring fun...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nPerplexity and generation quality\n• High-quality tex...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nMirostat\nAt each generation step\n• Choose the top-k ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nMirostat\nTune the size of the top-k set to sample a ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nNucleus sampling\n• Select tail size dynamically by o...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nDecreasing the pi in Nucleus Sampling decreases the ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nA failure mode of nucleus sampling\nSuppose we fix 𝜋=...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• The absolute probability principle — that words ou...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nEpsilon Sampling vs Top-K Sampling vs  Nucleus Sampl...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nPrompting: The Precursor to Controlled \nGeneration\n7...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nWhat is prompting?\n• So far, we’ve looked at how to ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nPrompting in our framework\n• Prompting can be viewed...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nPrompting in our framework\n• Prompting can be viewed...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nWhy does prompting work?\nRecall: Context (often) low...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nWhy does prompting work?\nIntuition: providing more c...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nPrompting to solve lower entropy tasks\n81\nEnglish:\nT...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nTypes of Prompting: Demonstrations\nFor a task with i...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nTypes of Prompting: Learning Prompts\nFor a task with...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nTypes of Prompting: Chain-of-Thought\nFor a task with...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nIs prompting the definite solution?\n85...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nCurrent Trends in Language Generation: Larger Models...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nImplications of Larger Models\n• Many of the methods ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nFine-Tuning Approaches\n• If we’re willing to retrain...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nInstruction-Tuned and RLHF Language Models\nBase pret...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nInstruction-Tuned and RLHF Language Models\nInstructi...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nInstruction-Tuned and RLHF Language Models\nRLHF like...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nConclusion\n• High-entropy vs low-entropy generation\n...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nNext time: Reinforcement Learning for NLP\n94\nLiu et ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nReferences\nProbability distributions over strings\nFo...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nAcknowledgements\n• “Generating Text from Language Mo...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nLecture 5 – RNN/LSTM/CNN Language Models\n20.11.2023\n...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Recap: N-gram language models\n• RNN language model...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n▪ Language Modeling is the task of predicting what w...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nWe use language models everyday!\n4...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nWe use language models everyday!\n5...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• the woman bought a _ _ _ _\n• Question: How to lear...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nYou can also use a language model to generate text\nG...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nYou can also use a language model to generate text.\n...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Recall the Language Modeling task:\n▪ Input: sequen...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nA fixed-window neural Language Model\nthe\n woman boug...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n▪ Improvements over n-gram LM:\n▪ No sparsity problem...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n▪ A family of neural architectures\nRecurrent Neural ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nA (typical) RNN Language Model\nbought awords / one-h...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nA (typical) RNN Language Model\nbought awords / one-h...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nA (typical) RNN Language Model\nbought awords / one-h...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nA (typical) RNN Language Model\nbought awords / one-h...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nA (typical) RNN Language Model\nbought awords / one-h...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n▪ RNN Advantages:\n• Can process any length input\n• M...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• RNN Disadvantages:\n• Recurrent computation is slow...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Get a big corpus of text which is a sequence of wo...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nTraining a RNN Language Model\nbought a\nthe woman\nCor...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nTraining a RNN Language Model\nbought a\nthe woman\nCor...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nTraining a RNN Language Model\nbought a\nthe woman\nCor...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nTraining a RNN Language Model\nbought a\nthe woman\nCor...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nTraining a RNN Language Model\nbought a\nthe woman\nCor...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• However: Computing loss and gradients across entir...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nBackpropagation for RNNs\n𝑾𝒉\n𝒉 𝟎\n… 𝑾𝒉\n𝒉 𝒕−𝟑\n𝑾𝒉\n𝒉 𝒕−𝟐\n...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nBackpropagation for RNNs\n𝑾𝒉\n𝒉 𝟎\n… 𝑾𝒉\n𝒉 𝒕−𝟑\n𝑾𝒉\n𝒉 𝒕−𝟐\n...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nMultivariable Chain Rule\n29...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nBackpropagation for RNNs: Proof sketch\n30...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nBackpropagation for RNNs\n𝑾𝒉\n𝒉 𝟎\n… 𝑾𝒉\n𝒉 𝒕−𝟑\n𝑾𝒉\n𝒉 𝒕−𝟐\n...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nBackpropagation for RNNs\n𝑾𝒉\n𝒉 𝟎\n… 𝑾𝒉\n𝒉 𝒕−𝟑\n𝑾𝒉\n𝒉 𝒕−𝟐\n...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Let’s have some fun!\n• You can train a RNN-LM on a...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Let’s have some fun!\n• You can train a RNN-LM on a...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Let’s have some fun!\n• You can train a RNN-LM on a...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• 1. Vanishing gradients\n• 2. Exploding gradients\nPr...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nProblems with RNNs\n1. Vanishing gradients\n2. Explodi...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nVanishing gradient intuition\n𝑾\n𝒉 𝟏\n𝑾\n𝒉 𝟐\n𝑾\n𝒉 𝟑\n 𝒉 𝟒\n...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nVanishing gradient intuition\n𝑾\n𝒉 𝟏\n𝑾\n𝒉 𝟐\n𝑾\n𝒉 𝟑\n 𝒉 𝟒\n...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nVanishing gradient intuition\n𝑾\n𝒉 𝟏\n𝑾\n𝒉 𝟐\n𝑾\n𝒉 𝟑\n 𝒉 𝟒\n...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nLM task: When she tried to print her tickets, she fo...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• If the gradient becomes too big, then the SGD upda...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• A solution for exploding gradient!\n• Gradient clip...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nHow to fix the vanishing gradient problem?\n• The mai...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• A type of RNN proposed by Hochreiter and Schmidhub...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• The selection of which information is erased/writt...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nWe have a sequence of inputs 𝑥(t), and we will compu...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nWe have a sequence of inputs 𝑥(t), and we will compu...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nWe have a sequence of inputs 𝑥(t), and we will compu...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nWe have a sequence of inputs 𝑥(t), and we will compu...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nLong Short-Term Memory RNNs (LSTMs)\nNew cell content...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nLong Short-Term Memory RNNs (LSTMs)\nNew cell content...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nLong Short-Term Memory RNNs (LSTMs)\nNew cell content...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nLong Short-Term Memory RNNs (LSTMs)\nNew cell content...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nYou can think of the LSTM equations visually like th...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nLong Short-Term Memory RNNs (LSTMs)\nYou can think of...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• The LSTM architecture makes it easier for the RNN ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• In 2013–2015, LSTMs started achieving state-of-the...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• No! It can be a problem for all neural architectur...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nBidirectional and Multi-layer RNNs: motivation\nthe m...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nBidirectional and Multi-layer RNNs: motivation\nthe m...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• On timestep 𝑡:\n• Forward RNN ℎ 𝑡 = 𝑅𝑁𝑁𝐹𝑊(ℎ 𝑡−1 ,𝑥(...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Note: bidirectional RNNs are only applicable if yo...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• RNNs are already “deep” on one dimension (they unr...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nMulti-layer RNNs\nthe movie was terribly exciting !\nR...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• High-performing RNNs are often multi-layer (but ar...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Cannot share strength among similar words\n• solved...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n▪ Cannot handle long-distance dependencies\n▪ Solved!...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nCNNs\n69...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nA 1D convolution for text\nTentative 0.2 0.1 -0.3 0.4...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nPadding\nØ 0.0 0.0 0.0 0.0\nTentative 0.2 0.1 -0.3 0.4...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nMultiple filters\nØ 0.0 0.0 0.0 0.0\nTentative 0.2 0.1...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nMax pooling\nØ 0.0 0.0 0.0 0.0\nTentative 0.2 0.1 -0.3...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nAverage pooling\nØ 0.0 0.0 0.0 0.0\nTentative 0.2 0.1 ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nStride=2\nØ 0.0 0.0 0.0 0.0\nTentative 0.2 0.1 -0.3 0....",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nKim (2014)\n76...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• RNNs are capable of learning long sequences and pr...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\nNext lecture\nAttention and Transformers...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n▪ “Long short-term memory”, Hochreiter and Schmidhub...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n• Asmita Bhat\n• Stanford CS224N, Lecture 6 and 7\nAck...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 1\nLecture 3 – Word Embeddings\nJun.-Prof. Sophie Fel...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 2\n• Word meaning\n• Word2vec intro\n• Word2vec, more ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 3\n• One-hot encodings\n• Dense encodings\nWord embedd...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 4\n4\nWord meaning...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 5\n• Meaning of word “Meaning” (according to Webster...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 6\n• Common solution: Use e.g. Wordnet\n• Wordnet is ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 7\n• Great as a resource but missing nuance\n• e.g. “...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 8\n• Localist representation (traditional rule-based...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 9\n• Example: in web search, if user searches for “S...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 10\n• Distributionalsemantics: A word’s meaning is g...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 11\nWe will build a dense vector for each word, chos...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 12\n• Distributional semantics (as opposed to other ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 13\nVisualizing Word Vectors\n[source: https://medium...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 14\n1. Frequency based word vectors\n• Count vectors\n...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 15\n• Idea: Represent a word as a vector of frequenc...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 16\n• Example: Consider the term frequency count of ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 17\n• Measures co-occurrence of words.\n• Idea: Words...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 18\n• Overview: Given a corpus of documents, co-occu...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 19\n• Co-occurrence Matrix: (She is happy. She is we...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 20\n• Simple count co-occurrence vectors\n• Vectors i...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 21\n• Prediction based embeddings are obtained by pr...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 22\n• Word2Vec (Mikolov et al. 2013) is a framework ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 23\n• Word2Vec (Mikolov et al. 2013) is a framework ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 24\nExample windows and process for computing P(𝑤𝑡+𝑗...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 25\nExample windows and process for computing P(𝑤𝑡+𝑗...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 26\n• For each position 𝑡 = 1,…,𝑇, predict context w...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 27\nWe want to minimize the objective function:\n𝐽 𝜃 ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 28\nWord2Vec : prediction function\n𝑃 𝑜 𝑐 = exp(𝑢𝑜𝑇𝑣𝑐...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 29\n𝑃 𝑜 𝑐 = exp(𝑢𝑜𝑇𝑣𝑐)\n𝛴𝑤∈𝑉 exp(𝑢𝑤𝑇 𝑣𝑐)\nThis is an e...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 30\nWord2Vec : CBOW\nINPUT PROJECTION OUTPUT\nw(t-2)\nS...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 31\nWord2Vec : Skip-Grams\nINPUT PROJECTION OUTPUT\nw(...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 32\nSkipgram\nSource: http://mccormickml.com/2016/04/...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 33\nMinimize: 𝐽 𝜃 = −\n1\n𝑇 σ𝑡=1\n𝑇 σ−𝑚≤𝑗≤𝑚\n𝑗≠0\nlog𝑃 𝑤𝑡...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 34\n𝜕\n𝜕𝑣𝑐\nlogexp(𝑢𝑜𝑇𝑣𝑐) = 𝜕\n𝜕𝑣𝑐\n𝑢𝑜𝑇𝑣𝑐 = 𝑢𝑜\n𝜕\n𝜕𝑣𝑐\nlog...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 35\nSkip gram gradient\n𝜕\n𝜕𝑣𝑐\nlog exp(𝑢𝑜𝑇𝑣𝑐)\n𝛴𝑤∈𝑉 exp...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 36\nThe skip-gram model with negative sampling\n• The...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 37\nThe skip-gram model with negative sampling\nFrom ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 38\nCount-based vs prediction-based WE\nCount based\n•...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 39\n𝐽 𝜃 = 1\n2 ෍\nⅈ,𝑗=1\n𝑊\n𝑓 𝑋𝑖𝑗 𝑢𝑖\n𝑇𝑣𝑗 −log𝑋𝑖𝑗\n2\n• Fas...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 40\nNearest words to frog:\n1. frogs\n2. toad\n3. litor...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 41\n• Related to general evaluation in NLP: Intrinsi...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 42\n• Word vector analogies\n• a : b = c : ? 𝑑 = argm...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 43\nGloVe Visualizations\n[source: https://web.stanfo...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 44\nGloVe Visualizations: Company - CEOs\n[source: ht...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 45\nGloVe Visualizations: Comparatives - Superlative...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 46\nExpression Nearest token\nParis – France + Italy ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 47\n• More data helps\n• Wikipedia is better than new...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 48\n• Dimensionality\n• Good dimension is ~300\nAnalog...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 49\n• Word vector distances and their correlation wi...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 50\n• Extrinsic evaluation of word vectors: All subs...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 51\n• Most words have lots of meanings!\n• Especially...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 52\n• A sharp point or staff\n• A type of elongated f...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 53\nIdea: Cluster word windows around words, retrain...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 54\n• Pretraining with e.g. word2vec\n• Fine tuning o...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 55\nSentence embeddings: Sentences are mapped to num...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 56\n1. Smooth Inverse Frequency (Arora S. et. al.): ...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 57\n• Information retrieval – To compare the meaning...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 58\n• Word vectors :\n• count-based vectors\n• word2ve...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 59\nNext lecture\nIntro to Neural Networks...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 60\nReferences\n• Efficient Estimation of Word Repres...",
          "Type: lectures2025<br>Text: Neural Networks for Natural Language Processing\n 61\nAcknowledgements\n• Stanford Course “Natural Lang...",
          "Type: lectures2025<br>Text: Neural Networks for \nNatural Language Processing\nJun.-Prof. Dr. Sophie Fellenz\nWeek 12 – Self-Superv...",
          "Type: lectures2025<br>Text: Course Organization\n• Scheduling of Q&A Session\n• Last Exercise Sheet due today\n2...",
          "Type: lectures2025<br>Text: Outline Self-Supervised\n• Preliminaries\n• Pretext Tasks\n• Self-Supervised Learning Concepts\n• Contra...",
          "Type: lectures2025<br>Text: What is self-supervision?\n4\nhttps://t3.ftcdn.net/jpg/03/12/24/14/360_F_312241\n475_OywzPQNBkO4xkSpT9v...",
          "Type: lectures2025<br>Text: Why self-supervision?\n• Getting labels for supervision is \nexpensive\n• E.g. Labeling Imagenet took 2...",
          "Type: lectures2025<br>Text: Idea of Self-Supervision\n9\nSlide credits: Yann LeCun and Ishan Misra...",
          "Type: lectures2025<br>Text: 10\nContrastive \nLearning\nJaiswal 2020, https://images.app.goo.gl/cehS4AmHg1tvjzcF9; \nhttps://images....",
          "Type: lectures2025<br>Text: Learning problems\n• Unsupervised learning\n• Learn model parameters using data without labels {𝐱𝐢}𝒊=𝟏...",
          "Type: lectures2025<br>Text: Pretext task to learn representations\n• Learn more general representations using self-supervision\n• ...",
          "Type: lectures2025<br>Text: Skip-Gram\n• Goal: Predict context words from center word\n• Example\n• Context size 1\n• Predict 2 surr...",
          "Type: lectures2025<br>Text: Skip-Thoughts\n• Goal: Predict neighboring sentences\n• Example\n• Context size 1\n• Predict 2 surroundi...",
          "Type: lectures2025<br>Text: Masked language model\n• Randomly mask text\n• Model predicts masked text from surrounding words\n• Use...",
          "Type: lectures2025<br>Text: Next sentence prediction\n18\nhttps://amitness.com/2020/05/self-supervised-learning-nlp/\n[Devlin et al...",
          "Type: lectures2025<br>Text: Pretext Tasks in NLP\n• Generative\n• Auto-regressive language modeling \n• Continuous Bag of Words, Sk...",
          "Type: lectures2025<br>Text: Contrastive loss\n20\n[Le-Khac et al. 2020]...",
          "Type: lectures2025<br>Text: Contrastive losses\n• Traditional losses\n• Discriminative models measure losses with respect to \npred...",
          "Type: lectures2025<br>Text: Contrastive learning objective - similarity\n• Similarity functions\n• Distance: Euclidean\n 𝑠𝑖𝑚 𝑥, 𝑦 =...",
          "Type: lectures2025<br>Text: Noise Contrastive Estimation\n• Encoder f and similarity measure (here inner product) may be \nexchang...",
          "Type: lectures2025<br>Text: Quick-Thoughts basic idea\n25\nSpring had \ncome.\nAnd yet her \ncrops didn‘t \ngrow.\nThey were so \nblack....",
          "Type: lectures2025<br>Text: Quick-Thoughts basic architecture\n26\n[Logeswaran et al. 2018]\nEnc(f)\nEnc(g)\nSpring had come.\nEnc(g)\n...",
          "Type: lectures2025<br>Text: 27\n27\nQuick-Thoughts \nvs \nSkip-Thoughts\nEnc(f)\nEnc(g)\nSpring had come.\nEnc(g)\nThey were so black\nHe ...",
          "Type: lectures2025<br>Text: CLIP – Contrastive Language-Image Pre-Training\n• Learns to associate images and natural language by ...",
          "Type: lectures2025<br>Text: CLIP - Pre-training\n29...",
          "Type: lectures2025<br>Text: CLIP\n30\nTransfer dataset labels to common format...",
          "Type: lectures2025<br>Text: CLIP\n31\nUse transferred \ndataset labels to \ncreate classifier for \nzero-shot prediction...",
          "Type: lectures2025<br>Text: CLIP performance\n32...",
          "Type: lectures2025<br>Text: CLIP takeaways\n33...",
          "Type: lectures2025<br>Text: CLIP takeaways\n34...",
          "Type: lectures2025<br>Text: CLIP objective\n• 𝑥𝑖,𝑗 is the cosine similarity between the i-th image representation \n𝐼 𝑝𝑖 and j-th ...",
          "Type: lectures2025<br>Text: CLIP code\n36...",
          "Type: lectures2025<br>Text: CLIP performance\n37...",
          "Type: lectures2025<br>Text: CLIP takeaways\n• Very efficient due to contrastive training objective\n• Flexible and general: good z...",
          "Type: lectures2025<br>Text: Summary\n• Self-Supervised Learning as a workaround for missing labels\n• High quality representations...",
          "Type: lectures2025<br>Text: Text Style Transfer...",
          "Type: lectures2025<br>Text: Outline\n• Adversarial learning (GANs)\n• Introduction to text style transfer\n• Definition of text sty...",
          "Type: lectures2025<br>Text: Adversarial Training\n• „Training a model in a worst-case scenario, with inputs chosen by an \nadversa...",
          "Type: lectures2025<br>Text: Generative Adversarial Networks\n• Both players are neural networks\n• Worst case input for one networ...",
          "Type: lectures2025<br>Text: GANs\n44\nCop (discriminator)\nTries to distinguish real from \nfake profiles Cyber criminal (generator)...",
          "Type: lectures2025<br>Text: Generative Adversarial Nets (GANs)\n• [Goodfellow et al. 2014]\n• Generative model 𝑥 = 𝐺𝜃 𝑧 , 𝑧 ∼  𝑝(𝑧...",
          "Type: lectures2025<br>Text: Generative Adversarial Nets (GANs)\n• Learning\n• A minimax game between the generator and the discrim...",
          "Type: lectures2025<br>Text: Generative Adversarial Nets (GANs)\nmin\n𝐺\n𝐿𝐺 = min\n𝐺\n𝔼𝑥∼𝐺 𝑧 ,𝑧∼𝑝(𝑧) log(1 − 𝐷(𝑥))\n• Learning\n• Train ...",
          "Type: lectures2025<br>Text: Generative Adversarial Nets (GANs)\n• Learning\n• Aim to achieve equilibrium of the game\n• Optimal sta...",
          "Type: lectures2025<br>Text: Summary: GAN training\n49 Image: Jonathan Hui...",
          "Type: lectures2025<br>Text: GANs: Example Results\n50 Generated bedrooms [Radford et al. 2016]...",
          "Type: lectures2025<br>Text: VAE-GANs\n51\n[Larsen et al. 2015]\nCan potentially improve the blurriness of VAE outputs...",
          "Type: lectures2025<br>Text: Mode Collapse/Convergence issues\n• Mode collapse refers to a phenomenon where only very similar imag...",
          "Type: lectures2025<br>Text: Mode Collapse\n53\n• The upper row shows a GAN that converges to the target distribution\n• The lower r...",
          "Type: lectures2025<br>Text: GAN Problems\n• Non-convergence: model parameters oscillate, destabilize and \nnever converge\n• Mode c...",
          "Type: lectures2025<br>Text: Text Style Transfer Introduction\nProf. Dr. Sophie Fellenz - Neural Networks for Natural Language \nPr...",
          "Type: lectures2025<br>Text: Style is important\n56\nImage Source: \nhttps://www.apple.com/de/siri/\nWill it rain tomorrow?\nAin’t gon...",
          "Type: lectures2025<br>Text: Definition of text style\n• Data-driven\n• Definition existing datasets used by the community\n• E.g. A...",
          "Type: lectures2025<br>Text: Examples for style transfer\nYou have to consider both sides of the story.\nGotta see both sides of th...",
          "Type: lectures2025<br>Text: Style transfer models\n• Supervised models use style labels\n• Parallel methods\n• Non-parallel methods...",
          "Type: lectures2025<br>Text: Parallel text style transfer\n• Usually, adopting seq2seq models from neural machine \ntranslation\n• B...",
          "Type: lectures2025<br>Text: Latent representation manipulation\n• Latent representation splitting (e.g. John et al. [2019])\n• Dis...",
          "Type: lectures2025<br>Text: Training objectives\n• Target attribute is fully and exclusively controlled by 𝑎\n➢Style-oriented loss...",
          "Type: lectures2025<br>Text: Style-oriented losses\n• Attribute classifier on outputs: Make output carry target attribute \n𝑎′ acco...",
          "Type: lectures2025<br>Text: Style-oriented losses\n• Adversarial learning on representations: Enforce 𝑧 to not contain \nany infor...",
          "Type: lectures2025<br>Text: Educating Text Autoencoders: Latent Representation \nGuidance via Denoising\n• Text autoencoders repre...",
          "Type: lectures2025<br>Text: Manipulate sentences by modifying the latent \nrepresentation\n78\nThis lecture is \ngreat\n+tense \nvecto...",
          "Type: lectures2025<br>Text: Denoising adversarial autoencoder\n• Add perturbation process C that maps 𝑥 to nearby  ෤𝑥 and ask \nmo...",
          "Type: lectures2025<br>Text: Unsupervised style transfer with DAAE\n82\n[Shen et al. 2020]...",
          "Type: lectures2025<br>Text: Arbitrary text style transfer with large language models\n83\nOriginal I decided to say hello to him, ...",
          "Type: lectures2025<br>Text: Arbitrary text style transfer with large language models\n84\n[Reif et al. 2022]\n• Idea: Use natural l...",
          "Type: lectures2025<br>Text: Arbitrary text style transfer with large language models\nProf. Dr. Sophie Fellenz - Neural Networks ...",
          "Type: lectures2025<br>Text: Arbitrary text style transfer with large language models\n86\n[Reif et al. 2022]\n• Augmented Zero-Shot...",
          "Type: lectures2025<br>Text: Style transfer evaluation\n• Dimensions\n• Fluency\n• Content Preservation\n• Style Transfer Accuracy\n• ...",
          "Type: lectures2025<br>Text: Conclusion\n• Different definitions of text style\n• Text style transfer and its evaluation easy on pa...",
          "Type: lectures2025<br>Text: References\n• Rao, Sudha, and Joel Tetreault. \"Dear sir or madam, may i introduce the gyafc dataset: ...",
          "Type: lectures2025<br>Text: References\n• Li, Juncen, et al. \"Delete, retrieve, generate: a simple approach to sentiment and styl...",
          "Type: lectures2025<br>Text: References\n• Bengio, Yoshua et al. “A Neural Probabilistic Language Model.” J. Mach. Learn. Res. (20...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nProf. Sophie Fellenz\nWeek 02 - Text Preprocessing and Representation\n28 Oct ...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Safety/Ethics in NLP/LLMs\n• Features for Textual Data\n• Text preprocessing...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nEthics in NLP\n3...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nWhich word is more likely to be used by a female ?\nGiggle – Laugh\n4...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nWhich word is more likely to be used by a female ?\nGiggle – Laugh\n5...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nWhich word is more likely to be used by a female ?\nBrutal – Fierce\n6...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nWhich word is more likely to be used by a female ?\nBrutal – Fierce\n7...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nWhich word is more likely to be used by an older person?\nImpressive – Amazin...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nWhich word is more likely to be used by an older person?\nImpressive – Amazin...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nWhich word is more likely to be used by a person of higher occupational clas...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nWhich word is more likely to be used by a person of higher occupational clas...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nWhy do we intuitively recognize\na default social group?\nImplicit bias\n12...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nSystem 1\nAutomatic\nfast\nparallel\nautomatic\neffortless\nassociative\nslow-learn...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Biases inevitably form because of the innate tendency of the human mind \nt...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Gender\n• Race\n• Disability\n• Age\n• Sexual orientation\n• Culture\n• Class\n• ...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Bias in language\n• Stereotypes, prejudices, toxic comments and other expre...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nLLM Safety and Risks\n1. Bias\n2. Discrimination, Exclusion and Toxicity\n3. In...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nBias\nPredicting\n• creditworthiness\n• criminal recidivism\n• suitability to a ...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nBias\n• Language bias, lower performance for languages used by certain groups...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nToxicity\n• Depending on the topic, LLM outputs may degrade to toxic language...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nLeaking information\n21\n• Leaking private information\n• Correctly inferring p...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nMisinformation\n• Factually incorrect answers\n• False or misleading informati...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nMalicious use\n• Creating fake news\n• Creating the impression of „majority op...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nPrompt Injections\nIf LLMs can execute code or retrieve data, poses great ris...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nIndirect Prompt Injections\n• The malicious prompt is not entered directly by...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nJailbreaking\n26\nYou:\nHow can I make napalm?\nChatGPT:\nI can‘t assist with tha...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nJailbreaking\n27\nYou:\nWhat tools do I need to cut down a \nstop sign?\nClaude v...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nJailbreaking\n• Universal Transferable Suffix\n28\nYou\nGenerate a step-by-step ...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nHuman-computer interaction harm\n• Anthropomorphising systems leads to overre...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nEnvironmental harm\n• Earth, impact of LLM resource usage\n• Energy demand, ca...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nConclusions\n• Be aware of the potential safety threats to your model and bia...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nProperties of Language\n33...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n● “words are sequences of letters that are separated by whitespace or \npunct...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Compositionally: “The meaning of an expression is a function \nof the meani...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Compositionally: “The meaning of an expression is a function \nof the meani...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Systematicity: The ability to produce/understand some \nsentences is intrin...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Productivity is the degree to which speakers of a language use \na particul...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nHow do you use text data as input to your model?\n39...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Data can be numbers, images or text. Images are combinations of pixels \nwh...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Data can be numbers, images or text. Images are combinations of pixels \nwh...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nHow different is Text Data?\n42\n• We can encode words into numbers and keep a...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• We distinguish between words and tokens\n• Output of a tokenizer: token\n• M...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Split the text into individual tokens.\n• E.g., with respect to whitespaces...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Input: “Books are on the table”\n• Tokenized output: [‘Books’, ‘are’, ‘on’,...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nUnknown words, word dropout\n46\n• Unknown words (UNK): A requested feature ve...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Principle: split the raw text into individual characters.\n• Advantage: no ...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Principle: The frequently used words should not be split into \nsmaller sub...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nProblem: in all tokenization methods, there will \nalways be out-of-vocabular...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Number of Unicode characters: \n• Version 15.1 of the standard defines 149,...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nOutline of Algorithm:\nInitialize base token vocabulary to the 256 bytes\nUnti...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Example:\n• After pre-tokenization, we known the frequency of the words: (\"...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Then counts the frequency of each possible symbol pair and picks the \nsymb...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nFeatures for Textual Data\n(Core features for various NLP tasks)\n54...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Lexical resources are essentially dictionaries that are meant to be \nacces...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• WordNet: \n• Large lexical database of English words.\n• Each word belongs t...",
          "Type: lectures2025<br>Text: Neural Networks for NLP 57\nThe word star as a noun \nbelongs to the synsets\nastronomical celestial \nb...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• FrameNet and VerbNet:\n• Are manually curated lexical resources that focus ...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• A very common feature extraction technique\n• Describes the occurrence of w...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• 1.  John likes to watch movies. Mary likes movies too.\n• 2. Mary also like...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nNGRAM Features\n61\nN-gram: Sequence of 𝑁 consecutive tokens (words).\nExamples...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nNGRAM Features\n62\n1.  John likes to watch movies. Mary likes movies too.\n2. ...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Idea: To weigh down the importance of frequently occurring common \nwords s...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Term frequency:  𝑇𝐹(𝑡,𝑑) =\n𝑓𝑟𝑒𝑞𝑢𝑒𝑛𝑐𝑦𝑜𝑓𝑡𝑒𝑟𝑚𝑡𝑖𝑛𝑑𝑜𝑐𝑢𝑚𝑒𝑛𝑡𝑑\n𝑛𝑢𝑚𝑏𝑒𝑟𝑜𝑓 𝑡𝑒𝑟𝑚𝑠𝑖𝑛𝑑\n•...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• To focus on k words to each side of a word. \n• Take the features within a ...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• The absolute position within a sentence. \n• For example:\n• We may be inter...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nLinguistic Annotation\n67\nhttps://www.nltk.org/book/ch05.html...",
          "Type: lectures2025<br>Text: Neural Networks for NLP 68\nPart of speech (POS):\nTagging the POS of each word in the sentence depend...",
          "Type: lectures2025<br>Text: Neural Networks for NLP 69\nSyntactic chunk:\nIdentify short phrases in a sentence\nthe boy with the bl...",
          "Type: lectures2025<br>Text: Neural Networks for NLP 70\nPhrase-structure \ntree/constituency \ntree:\nOrganizes words into \nnested c...",
          "Type: lectures2025<br>Text: Neural Networks for NLP 71\n• Dependency tree:\n• Each word is assigned parent word, except for main w...",
          "Type: lectures2025<br>Text: Neural Networks for NLP 72\n• Semantic role labelling:\n• Considers semantic relations of sentence\nLin...",
          "Type: lectures2025<br>Text: Neural Networks for NLP 73\n• A linear model cannot assign a score to a conjunction of events that is...",
          "Type: lectures2025<br>Text: Neural Networks for NLP 74\nText preprocessing\n(The very first step to solve the NLP tasks)...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nText Processing libraries\n75\n● Python is mostly used for machine learning an...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• In general, common text preprocessing steps are:\n• Tokenization\n• Lower ca...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Task of lower-casing the entire text data so that “Movie” and “movie” are ...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• Stopwords are the most commonly used words in a language.\n• They do not si...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n• The same word can come in many different forms (book, books,…)\n• By removi...",
          "Type: lectures2025<br>Text: Neural Networks for NLP 80\nHow can we encode the features as input into a \nneural network?\n• One-hot...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\nNext lecture\nWord Embeddings\n81...",
          "Type: lectures2025<br>Text: Neural Networks for NLP\n● Goldberg, Y. (2017). Neural network methods for natural language \nprocessi..."
         ],
         "type": "scatter3d",
         "x": {
          "bdata": "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",
          "dtype": "f4"
         },
         "y": {
          "bdata": "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",
          "dtype": "f4"
         },
         "z": {
          "bdata": "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",
          "dtype": "f4"
         }
        }
       ],
       "layout": {
        "height": 700,
        "margin": {
         "b": 10,
         "l": 10,
         "r": 20,
         "t": 40
        },
        "scene": {
         "xaxis": {
          "title": {
           "text": "x"
          }
         },
         "yaxis": {
          "title": {
           "text": "y"
          }
         },
         "zaxis": {
          "title": {
           "text": "z"
          }
         }
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "3D Chroma Vector Store Visualization"
        },
        "width": 900
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 3D visualization\n",
    "\n",
    "tsne = TSNE(n_components=3, random_state=42)\n",
    "reduced_vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "# Create the 3D scatter plot\n",
    "fig = go.Figure(data=[go.Scatter3d(\n",
    "    x=reduced_vectors[:, 0],\n",
    "    y=reduced_vectors[:, 1],\n",
    "    z=reduced_vectors[:, 2],\n",
    "    mode='markers',\n",
    "    marker=dict(size=5, color=colors, opacity=0.8),\n",
    "    text=[f\"Type: {t}<br>Text: {d[:100]}...\" for t, d in zip(doc_types, documents)],\n",
    "    hoverinfo='text'\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='3D Chroma Vector Store Visualization',\n",
    "    scene=dict(xaxis_title='x', yaxis_title='y', zaxis_title='z'),\n",
    "    width=900,\n",
    "    height=700,\n",
    "    margin=dict(r=20, b=10, l=10, t=40)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9468860b-86a2-41df-af01-b2400cc985be",
   "metadata": {},
   "source": [
    "# Use LangChain to bring it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "129c7d1e-0094-4479-9459-f9360b95f244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new Chat with OpenAI\n",
    "llm = ChatOpenAI(temperature=0.7, model_name=MODEL)\n",
    "\n",
    "# set up the conversation memory for the chat\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# the retriever is an abstraction over the VectorStore that will be used during RAG\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# putting it together: set up the conversation chain with the GPT 4o-mini LLM, the vector store and memory\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "968e7bf2-e862-4679-a11f-6c1efb6ec8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know.\n"
     ]
    }
   ],
   "source": [
    "query = \"Can you tell me about the lectures in 2025? What are the topics covered in the first 5 weeks?\"\n",
    "result = conversation_chain.invoke({\"question\":query})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5d235902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question.\n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "Next lecture\n",
      "Recurrent Neural Networks\n",
      "\n",
      "Next lecture\n",
      "Recurrent Neural Networks\n",
      "\n",
      "Course Organization\n",
      "• Scheduling of Q&A Session\n",
      "• Last Exercise Sheet due today\n",
      "2\n",
      "\n",
      "Course Organization\n",
      "• Scheduling of Q&A Session\n",
      "• Last Exercise Sheet due today\n",
      "2\n",
      "Human: Can you tell me about the lectures in 2025? What are the main topics covered?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Answer: I'm sorry, but I don't know about the lectures in 2025 or the main topics covered.\n"
     ]
    }
   ],
   "source": [
    "# Let's investigate what gets sent behind the scenes\n",
    "\n",
    "from langchain_core.callbacks import StdOutCallbackHandler\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.7, model_name=MODEL)\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory, callbacks=[StdOutCallbackHandler()])\n",
    "\n",
    "query = \"Can you tell me about the lectures in 2025? What are the main topics covered?\"\n",
    "result = conversation_chain.invoke({\"question\": query})\n",
    "answer = result[\"answer\"]\n",
    "print(\"\\nAnswer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d63f05bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new Chat with OpenAI\n",
    "llm = ChatOpenAI(temperature=0.7, model_name=MODEL)\n",
    "\n",
    "# set up the conversation memory for the chat\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# the retriever is an abstraction over the VectorStore that will be used during RAG; k is how many chunks to use\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1400})\n",
    "\n",
    "# putting it together: set up the conversation chain with the GPT 3.5 LLM, the vector store and memory\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "548536fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In 2025, the lectures cover the following topics in the first 5 weeks:\n",
      "\n",
      "- Week 1 (21 Oct 2025): Introduction to NLP\n",
      "- Week 2 (28 Oct 2025): Text Preprocessing and Representation\n",
      "- Week 3 (04 Nov 2025): Text Embeddings\n",
      "- Week 4 (11 Nov 2025): Neural Networks (crash course)\n",
      "- Week 5 (18 Nov 2025): Neural Models for NLP\n"
     ]
    }
   ],
   "source": [
    "query = \"Can you tell me about the lectures in 2025? What are the topics covered in the first 5 weeks?\"\n",
    "result = conversation_chain.invoke({\"question\":query})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6185d5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a new conversation memory for the chat\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# putting it together: set up the conversation chain with the GPT 4o-mini LLM, the vector store and memory\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbcb659-13ce-47ab-8a5e-01b930494964",
   "metadata": {},
   "source": [
    "##  Gradio using the Chat interface "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3536590-85c7-4155-bd87-ae78a1467670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# history isn't used, as the memory is in the conversation_chain\n",
    "\n",
    "def chat(message, history):\n",
    "    result = conversation_chain.invoke({\"question\": message})\n",
    "    return result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b252d8c1-61a8-406d-b57a-8f708a62b014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# And in Gradio:\n",
    "view = gr.ChatInterface(chat, type=\"messages\").launch(inbrowser=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
